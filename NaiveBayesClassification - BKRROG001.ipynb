{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CSC5035Z Natural Language Processing\n",
    "# Assignment 1: NaÃ¯ve Bayes for Text Classification"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e57af82a0338e60"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Author: Roger Bukuru**\n",
    "\n",
    "## How to guide\n",
    "- First Ensure the following packages are installed on your system\n",
    "  - **numpy** \n",
    "    - installation command: **pip install numpy**\n",
    "  - **pandas** \n",
    "    - installation command: **pip install pandas**\n",
    "  - **sklearn** \n",
    "    - installation command: **pip install sklearn**\n",
    "- Then sequentially run each code block, the code blocks perform the following tasks:\n",
    "   - Downloads the Afrisenti dataset\n",
    "   - Selects the language of choice (Kinyarwanda)\n",
    "   - Cleans the data\n",
    "   - Creates a vocabulary\n",
    "   - Applies word-based and BPE tokenization\n",
    "   - Implements a simple binary text vectorization\n",
    "   - Implements the NaÃ¯ve Bayes classifier\n",
    "   - Trains classifier based on word-based and BPE features obtained from simple binary text vectorization\n",
    "   - Predict on both word-based and BPE features\n",
    "   - Evaluate model on a development data set\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5022761ebfccfa0b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Installations, Imports and Downloads\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd9f322a3cc17080"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import re\n",
    "import numpy as np\n",
    "warnings.filterwarnings(\"ignore\", category = UserWarning)\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "FS = (8,4) # figure size\n",
    "RS = 264\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-09T08:53:40.531394Z",
     "start_time": "2024-08-09T08:53:36.657267Z"
    }
   },
   "id": "b77082fbc75833ba",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directiory:  /Users/rogerbukuru/Documents/UCT Masters/MSc Statistics and Data Science/NLP-CSC5035Z/NLPTutsAssignments/Assignment-I/afrisent-semeval-2023\n",
      "/Users/rogerbukuru/Documents/UCT Masters/MSc Statistics and Data Science/NLP-CSC5035Z/NLPTutsAssignments/Assignment-I/afrisent-semeval-2023\n",
      "From https://github.com/afrisenti-semeval/afrisent-semeval-2023\r\n",
      " * branch            HEAD       -> FETCH_HEAD\r\n",
      "Already up to date.\r\n"
     ]
    }
   ],
   "source": [
    "PROJECT_DIR = os.getcwd() + '/afrisent-semeval-2023'\n",
    "print('Current directiory: ', PROJECT_DIR)\n",
    "PROJECT_GITHUB_URL = 'https://github.com/afrisenti-semeval/afrisent-semeval-2023.git'\n",
    "\n",
    "if not os.path.isdir(PROJECT_DIR):\n",
    "  !git clone {PROJECT_GITHUB_URL}\n",
    "else:\n",
    "  %cd {PROJECT_DIR}\n",
    "  !git pull {PROJECT_GITHUB_URL}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-09T08:53:41.485376Z",
     "start_time": "2024-08-09T08:53:40.533024Z"
    }
   },
   "id": "91fdcbd58f0388f6",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Loading\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be842af4aa9e3ccd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Choose language \n",
    "language = 'kin'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-09T08:53:41.488087Z",
     "start_time": "2024-08-09T08:53:41.486074Z"
    }
   },
   "id": "945c286f95628713",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory:  /Users/rogerbukuru/Documents/UCT Masters/MSc Statistics and Data Science/NLP-CSC5035Z/NLPTutsAssignments/Assignment-I/afrisent-semeval-2023/data/kin\n",
      "Train shape:  (3302, 2)\n",
      "Dev shape:  (827, 2)\n",
      "Test shape:  (1026, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                                   text     label\n50        @user Wa nkozi y ibibi we Umwanzi w ImanaðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚  negative\n2892  @user Ni byagaciri Kuri nyirabyo ni igihugu mu...  positive\n3050  Isabukuru nziza databuja wanjye ! My blood,Uwi...  positive\n1372  @user @user Umuntu ufite @user ikaba yizimya i...   neutral\n2071  @user Barayampompa cyane. Uzambwire aho wumvis...   neutral\n...                                                 ...       ...\n3291  @user Congratulations... Amatora y'inzego z'ib...  positive\n3063  Uri ikimenyetso cy'uko ubuzima bwatsinze urupf...  positive\n1364  Turi kuganira ku Iterambere ry'umujyi wa Musan...   neutral\n2636  Umunsi mwiza w'ubwigenge (independance day). K...  positive\n165   Ni ukubera iki ibikorerwa muri Afrika byamamaz...  negative\n\n[100 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>50</th>\n      <td>@user Wa nkozi y ibibi we Umwanzi w ImanaðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>2892</th>\n      <td>@user Ni byagaciri Kuri nyirabyo ni igihugu mu...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3050</th>\n      <td>Isabukuru nziza databuja wanjye ! My blood,Uwi...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>1372</th>\n      <td>@user @user Umuntu ufite @user ikaba yizimya i...</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>2071</th>\n      <td>@user Barayampompa cyane. Uzambwire aho wumvis...</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3291</th>\n      <td>@user Congratulations... Amatora y'inzego z'ib...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3063</th>\n      <td>Uri ikimenyetso cy'uko ubuzima bwatsinze urupf...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>1364</th>\n      <td>Turi kuganira ku Iterambere ry'umujyi wa Musan...</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>2636</th>\n      <td>Umunsi mwiza w'ubwigenge (independance day). K...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>165</th>\n      <td>Ni ukubera iki ibikorerwa muri Afrika byamamaz...</td>\n      <td>negative</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows Ã— 2 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "DATA_DIR = f'{PROJECT_DIR}/data/{language}'\n",
    "print('Data directory: ', DATA_DIR)\n",
    "\n",
    "train_df = pd.read_csv(f'{DATA_DIR}/train.tsv', sep='\\t', names=['text', 'label'], header=0)\n",
    "dev_df = pd.read_csv(f'{DATA_DIR}/dev.tsv', sep='\\t', names=['text', 'label'], header=0)\n",
    "test_df = pd.read_csv(f'{DATA_DIR}/test.tsv', sep='\\t', names=['text', 'label'], header=0)\n",
    "\n",
    "print('Train shape: ', train_df.shape)\n",
    "print('Dev shape: ', dev_df.shape)\n",
    "print('Test shape: ', test_df.shape)\n",
    "\n",
    "# Display data\n",
    "train_df.sample(n=100)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-09T08:53:41.509555Z",
     "start_time": "2024-08-09T08:53:41.490187Z"
    }
   },
   "id": "b1b9cbe14aa9740f",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Cleaning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "19067257a5279928"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Discard neutral examples\n",
    "train_df = train_df[train_df['label'] != 'neutral']\n",
    "dev_df = dev_df[dev_df['label'] != 'neutral']\n",
    "test_df = test_df[test_df['label'] != 'neutral']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-09T08:53:41.526101Z",
     "start_time": "2024-08-09T08:53:41.509803Z"
    }
   },
   "id": "aca2cf3c14aafd31",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    # Replace URLS with [URL]\n",
    "    text = re.sub(r'http\\S+', '[URL]', text)\n",
    "\n",
    "    # Replace numbers with [NUM]\n",
    "    text = re.sub(r'\\d+', '[NUM]', text)\n",
    "    \n",
    "    # Replace @user with \"\"\n",
    "    text = re.sub(r'@user\\b', '[USR]', text)\n",
    "\n",
    "    # Remove trailing spaces\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "train_df['text'] = train_df['text'].apply(clean)\n",
    "dev_df['text'] = dev_df['text'].apply(clean)\n",
    "test_df['text'] = test_df['text'].apply(clean)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-09T08:53:41.538733Z",
     "start_time": "2024-08-09T08:53:41.522509Z"
    }
   },
   "id": "b44a453899f24356",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Construct Vocabulary"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4534f864e77f22f8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Count number of tokens in corpus\n",
    "def count_tokens(sentences):\n",
    "    \"\"\"\n",
    "    Count number of tokens in corpus\n",
    "\n",
    "    param: sentences: list of list of tokens e.g. [['This', 'is', 'a', 'sentence'], ['This', 'is', 'another', 'sentence'], ...]\n",
    "    return:\n",
    "        count: number of tokens in corpus\n",
    "    \"\"\"\n",
    "    total_tokens = 0\n",
    "    for sentence in sentences:\n",
    "        total_tokens += len(sentence)\n",
    "    return total_tokens"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-09T08:53:41.539393Z",
     "start_time": "2024-08-09T08:53:41.524757Z"
    }
   },
   "id": "aa2c3ccefea2eec0",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Collect type counts in corpus\n",
    "def create_type_counts(sentences):\n",
    "    \"\"\"\n",
    "    Count number of types in corpus\n",
    "\n",
    "    param: sentences: list of list of tokens e.g. [['This', 'is', 'a', 'sentence'], ['This', 'is', 'another', 'sentence'], ...]\n",
    "    return:\n",
    "        type2count: dictionary of type counts in corpus e.g. {'This': 2, 'sentence': 2, ...}\n",
    "    \"\"\"\n",
    "    type2count = {}\n",
    "    for sentence in sentences:\n",
    "        for type_ in sentence:\n",
    "            if type_ not in type2count:\n",
    "                type2count[type_] = 1\n",
    "            else:\n",
    "                current_count = type2count[type_]\n",
    "                type2count[type_] = current_count +1\n",
    "    return type2count"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-09T08:53:41.540811Z",
     "start_time": "2024-08-09T08:53:41.527298Z"
    }
   },
   "id": "170232a2a3218e1e",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Create vocabulary\n",
    "def create_vocabulary(type2count, min_count):\n",
    "    \"\"\"\n",
    "    This function creates an indexed vocabulary from vocabulary counts and returns it as a list and a dictionary.\n",
    "\n",
    "    param:\n",
    "        type2count: dictionary of type counts in corpus e.g. {'This': 2, 'sentence': 2, ...}\n",
    "        min_count: minimum count of a word to be included in the vocabulary\n",
    "    return:\n",
    "        index2type: list of words in the vocabulary e.g. ['word1', 'word2', 'word3', ...]\n",
    "        type2index: dictionary mapping words to their index in the index2type vocabulary e.g. {'word1': 0, 'word2': 1, 'word3': 2, ...}\n",
    "    \"\"\"\n",
    "    index2type = []\n",
    "    type2index = {}\n",
    "    for type_, count in type2count.items():\n",
    "        if(count >= min_count):\n",
    "            index2type.append(type_)\n",
    "            type2index[type_] = len(index2type) - 1\n",
    "    return index2type, type2index"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-09T08:53:41.571096Z",
     "start_time": "2024-08-09T08:53:41.528570Z"
    }
   },
   "id": "a55b473b29ace5e4",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "\n",
    "# Tokenization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d3285ba6754d912"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Word Based Tokenization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26626c598cae9936"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def whitespace_tokenize(sentences):\n",
    "    return [sentence.split() for sentence in sentences]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-09T08:53:41.571801Z",
     "start_time": "2024-08-09T08:53:41.533645Z"
    }
   },
   "id": "64f366be8fdce023",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Byte-Pair Encoding (BPE) Tokenization\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d72e944dadb4c3f0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class BPETokenizer():\n",
    "\n",
    "    def __init__(self, sentences, vocab_size):\n",
    "        \"\"\"\n",
    "        Initialize the BPE tokenizer.\n",
    "\n",
    "        param:\n",
    "            sentences (list[str]): list of list of tokens e.g. [['This', 'is', 'a', 'sentence'], ['This', 'is', 'another', 'sentence'], ...]\n",
    "            vocab_size (int): The desired vocabulary size after training.\n",
    "        \"\"\"\n",
    "        self.sentences = sentences\n",
    "        self.vocab_size = vocab_size\n",
    "        self.word_freqs = defaultdict(int)\n",
    "        self.splits = {}\n",
    "        self.merges = {}\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train the BPE tokenizer by iteratively merging the most frequent pairs of symbols.\n",
    "\n",
    "        return:\n",
    "            dict: A dictionary of merges in the format {(a, b): 'ab'}, where 'a' and 'b' are symbols merged into 'ab'.\n",
    "        \"\"\"\n",
    "        # Split corpus\n",
    "        for sentence in self.sentences:\n",
    "            for word in sentence:\n",
    "                self.splits[word] = [char for char in word]\n",
    "                    \n",
    "        for i in range(self.vocab_size):\n",
    "            self.compute_pair_freqs() # compute adjacent pair frequencies\n",
    "            pair, _ = list(self.word_freqs.items())[0] # most frequent pair\n",
    "            self.merge_pair(pair[0], pair[1])\n",
    "            self.merges[pair] = pair[0] + pair[1]\n",
    "        return self.merges\n",
    "\n",
    "\n",
    "    def compute_pair_freqs(self):\n",
    "        \"\"\"\n",
    "        Compute the frequency of each pair of symbols in the corpus.\n",
    "\n",
    "        return:\n",
    "            dict: A dictionary of pairs and their frequencies in the format {(a, b): frequency}.\n",
    "        \"\"\"\n",
    "        pair_freqs = defaultdict(int)\n",
    "        for _, split in self.splits.items():\n",
    "            for i in range(len(split)-1):\n",
    "                pair = (split[i], split[i+1])\n",
    "                if pair not in pair_freqs:\n",
    "                    pair_freqs[pair] = 1\n",
    "                else:\n",
    "                    pair_freqs[pair] += 1\n",
    "        self.word_freqs = pair_freqs\n",
    "        self.word_freqs = dict(sorted(self.word_freqs.items(), key=lambda x: x[1], reverse=True)) # sort from max to min count\n",
    "        return self.word_freqs\n",
    "        \n",
    "    def merge_pair(self, a, b):\n",
    "        \"\"\"\n",
    "        Merge the given pair of symbols in all words where they appear adjacent.\n",
    "\n",
    "        param:\n",
    "            a (str): The first symbol in the pair.\n",
    "            b (str): The second symbol in the pair.\n",
    "\n",
    "        return:\n",
    "            dict: The updated splits dictionary after merging.\n",
    "        \"\"\"\n",
    "        pair = (a,b)\n",
    "        # Check if valid pair\n",
    "        if pair in self.word_freqs:\n",
    "            new_token = a+b\n",
    "            for word, split in self.splits.items():\n",
    "                for i in range(len(split)-1):\n",
    "                    if split[i] == a and split[i+1] == b:\n",
    "                       split[i] = new_token\n",
    "                       new_split = list(filter(lambda x: x not in [b], split))\n",
    "                       self.splits[word] = new_split\n",
    "        return self.splits\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"\n",
    "        Tokenize a given text using the trained BPE tokenizer.\n",
    "\n",
    "        param:\n",
    "            text (str): The text to be tokenized.\n",
    "\n",
    "        return:\n",
    "            list[str]: A list of tokens obtained after applying BPE tokenization.\n",
    "        \"\"\"\n",
    "\n",
    "        pre_tokenized_text = text.split()\n",
    "        splits_text = [[l for l in word] for word in pre_tokenized_text]\n",
    "\n",
    "        for pair, merge in self.merges.items():\n",
    "            for idx, split in enumerate(splits_text):\n",
    "                i = 0\n",
    "                while i < len(split) - 1:\n",
    "                    if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
    "                        split = split[:i] + [merge] + split[i + 2 :]\n",
    "                    else:\n",
    "                        i += 1\n",
    "                splits_text[idx] = split\n",
    "        result = sum(splits_text, [])\n",
    "        return result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-09T08:53:41.590309Z",
     "start_time": "2024-08-09T08:53:41.539854Z"
    }
   },
   "id": "185fdac1da4c7403",
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature Extraction"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "47386243702bc12f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Simple Binary Text Vectorization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6aadfd89ead00bf5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def simple_binary_vectorization(sentences, type2index):\n",
    "    \"\"\"\n",
    "    Binary text-vectorization of a list of sentences.\n",
    "\n",
    "    param: \n",
    "     sentences:  list of list of tokens e.g. [['This', 'is', 'a', 'sentence'], ['This', 'is', 'another', 'sentence'], ...]\n",
    "     type2index: dictionary mapping words to their index in the vocabulary e.g. {'word1': 0, 'word2': 1, 'word3': 2, ...}\n",
    "    return: 2D NumPy array of a simple-binary encoded sentences.\n",
    "    \"\"\"\n",
    "    vectors = []\n",
    "    unk_index = type2index['<UNK>']\n",
    "    for sentence in sentences:\n",
    "        vector = np.zeros(len(type2index.items()))\n",
    "        for word in sentence:\n",
    "            if word in type2index: \n",
    "             word_index = type2index[word]\n",
    "             vector[word_index] = 1\n",
    "            else: # If word does not exist we use the index of the UNK token.\n",
    "                vector[unk_index] = 1\n",
    "        vectors.append(np.array(vector))\n",
    "    return np.array(vectors)\n",
    "\n",
    "     "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-09T08:53:41.611304Z",
     "start_time": "2024-08-09T08:53:41.541430Z"
    }
   },
   "id": "7298bb785e0a0987",
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Word-Based Tokenization Feature Extraction\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37333bc85957788f"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in corpus:  30894\n",
      "Vocab size: 13674\n"
     ]
    }
   ],
   "source": [
    "# Store training data text as list of tweets\n",
    "train_corpus = train_df['text'].tolist()\n",
    "tokenized_train_corpus = whitespace_tokenize(train_corpus)\n",
    "num_tokens = count_tokens(tokenized_train_corpus)\n",
    "print('Number of tokens in corpus: ', num_tokens)\n",
    "type2count = create_type_counts(tokenized_train_corpus)\n",
    "\n",
    "# Sort types by counts\n",
    "type2count = dict(sorted(type2count.items(), key=lambda x: x[1], reverse=True))\n",
    "index2type, type2index = create_vocabulary(type2count, min_count=1)\n",
    "print(\"Vocab size:\", len(index2type))\n",
    "\n",
    "# We add a special token for unknown words and padding (to make all sentences in the training batch the same length)\n",
    "type2index['<UNK>'] = len(index2type)\n",
    "index2type.append('<UNK>')\n",
    "type2index['<PAD>'] = len(index2type)\n",
    "index2type.append('<PAD>')\n",
    "x_train_word_based_feature_matrix = simple_binary_vectorization(tokenized_train_corpus, type2index)\n",
    "\n",
    "dev_corpus = dev_df['text'].tolist()\n",
    "tokenized_dev_corpus = whitespace_tokenize(dev_corpus)\n",
    "x_dev_word_based_feature_matrix = simple_binary_vectorization(tokenized_dev_corpus, type2index)\n",
    "\n",
    "test_corpus = test_df['text'].tolist()\n",
    "tokenized_test_corpus = whitespace_tokenize(test_corpus)\n",
    "x_test_word_based_feature_matrix = simple_binary_vectorization(tokenized_test_corpus, type2index)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-09T08:53:41.685498Z",
     "start_time": "2024-08-09T08:53:41.555221Z"
    }
   },
   "id": "c5f9f9e91d866b89",
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "### BPE Tokenization Feature Extraction"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1183ca42d8e9f287"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in corpus:  80362\n",
      "Vocab size: 1171\n"
     ]
    }
   ],
   "source": [
    "# Train BPE\n",
    "bpe = BPETokenizer(tokenized_train_corpus, vocab_size=1000 )\n",
    "merges = bpe.train()\n",
    "\n",
    "\n",
    "# Apply to our dataset\n",
    "train_df['bpe_text'] = train_df['text'].apply(lambda x: ' '.join(bpe.tokenize(x)))\n",
    "dev_df['bpe_text'] = dev_df['text'].apply(lambda x: ' '.join(bpe.tokenize(x)))\n",
    "test_df['bpe_text'] = test_df['text'].apply(lambda x: ' '.join(bpe.tokenize(x)))\n",
    "train_df.head()\n",
    "\n",
    "bpe_train_corpus = train_df['bpe_text'].tolist()\n",
    "tokenized_bpe_train_corpus = whitespace_tokenize(bpe_train_corpus)\n",
    "num_tokens = count_tokens(tokenized_bpe_train_corpus)\n",
    "print('Number of tokens in corpus: ', num_tokens)\n",
    "bpe_type2count = create_type_counts(tokenized_bpe_train_corpus)\n",
    "\n",
    "# Sort types by counts\n",
    "bpe_type2count = dict(sorted(bpe_type2count.items(), key=lambda x: x[1], reverse=True))\n",
    "bpe_index2type, bpe_type2index = create_vocabulary(bpe_type2count, min_count=1)\n",
    "print(\"Vocab size:\", len(bpe_index2type))\n",
    "\n",
    "bpe_type2index['<UNK>'] = len(bpe_index2type)\n",
    "bpe_index2type.append('<UNK>')\n",
    "bpe_type2index['<PAD>'] = len(bpe_index2type)\n",
    "bpe_index2type.append('<PAD>')\n",
    "\n",
    "x_train_bpe_feature_matrix = simple_binary_vectorization(tokenized_bpe_train_corpus, bpe_type2index)\n",
    "\n",
    "bpe_dev_corpus = dev_df['bpe_text'].tolist()\n",
    "tokenized_bpe_dev_corpus = whitespace_tokenize(bpe_dev_corpus)\n",
    "x_dev_bpe_feature_matrix = simple_binary_vectorization(tokenized_bpe_dev_corpus, bpe_type2index)\n",
    "\n",
    "bpe_test_corpus = test_df['bpe_text'].tolist()\n",
    "tokenized_bpe_test_corpus = whitespace_tokenize(bpe_test_corpus)\n",
    "x_test_bpe_feature_matrix = simple_binary_vectorization(bpe_test_corpus, bpe_type2index)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-09T08:54:04.787573Z",
     "start_time": "2024-08-09T08:53:41.697905Z"
    }
   },
   "id": "8e66e51edab48fb3",
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Training\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "995a5523bec920cd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier():\n",
    "    \n",
    "    def __init__(self, x_train, y_train, type2index):\n",
    "        self.class_priors = {}\n",
    "        self.likelihoods_probs = {}\n",
    "        self.word_counts = {}\n",
    "        self.vocabulary = type2index\n",
    "        self.Y_train = y_train\n",
    "        self.X_train = x_train\n",
    "        self.vocab_size = len(type2index.items())\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "         Trains the Naive Bayes model by computing class priors and likelihood probabilities.\n",
    "        \"\"\"\n",
    "        # Calculate class prior probabilities\n",
    "        self.compute_prior_probs()\n",
    "        # Calculate likelihood probabilities\n",
    "        self.compute_likelihood_probs() \n",
    "    \n",
    "    def compute_prior_probs(self):\n",
    "        \"\"\"\n",
    "        Calculates the prior probability for each class in the training data.\n",
    "        \"\"\"\n",
    "        positive_sentences = 0\n",
    "        for label in self.Y_train:\n",
    "            if label == 1:\n",
    "                positive_sentences += 1\n",
    "        self.class_priors[1] = positive_sentences/len(self.Y_train)\n",
    "        self.class_priors[0] = 1 - self.class_priors[1]\n",
    "        \n",
    "    def compute_likelihood_probs(self):\n",
    "        \"\"\"\n",
    "        Calculates the likelihood probabilities for each word in the vocabulary, given each class, using Laplace smoothing.\n",
    "        \"\"\"\n",
    "        self.word_counts = {0: np.zeros(self.vocab_size), 1: np.zeros(self.vocab_size)}\n",
    "        for sentence_vector, label  in zip(self.X_train, self.Y_train):\n",
    "            self.word_counts[label] += sentence_vector\n",
    "        for class_ in self.word_counts:\n",
    "            # Compute based on Laplace Transform\n",
    "            self.likelihoods_probs[class_] = (self.word_counts[class_] + 1)/ (sum(self.word_counts[class_]) + self.vocab_size)\n",
    "            \n",
    "         \n",
    "    def predict(self,x):\n",
    "        \"\"\"\n",
    "        Predicts the class labels for a set of input documents.\n",
    "\n",
    "        param:\n",
    "            x: A list or array of document feature vectors (binary vectors representing word occurrences).\n",
    "\n",
    "        return:\n",
    "            A list of predicted class labels (0 or 1).\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        for sentence_vector in x: # text vectorization of sentence\n",
    "            posteriors = {}\n",
    "            for class_  in self.class_priors:\n",
    "                log_posterior = np.log(self.class_priors[class_])\n",
    "                for i in range(len(sentence_vector)):\n",
    "                    if sentence_vector[i] == 1:\n",
    "                        log_posterior += np.log(self.likelihoods_probs[class_][i])\n",
    "                posteriors[class_] = log_posterior\n",
    "            predicted_class = max(posteriors, key=posteriors.get)\n",
    "            predictions.append(predicted_class)\n",
    "        return predictions\n",
    "    \n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-09T08:54:04.792944Z",
     "start_time": "2024-08-09T08:54:04.790856Z"
    }
   },
   "id": "30bae654ab9c2954",
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Word-Based Tokenization: Model Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f69299d7fe8ea867"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "y_train = train_df[\"label\"]\n",
    "y_train = np.array(y_train.map({\"positive\": 1, \"negative\": 0}))\n",
    "x_train = x_train_word_based_feature_matrix\n",
    "\n",
    "# Train Model\n",
    "nb_word_based_tokenization = NaiveBayesClassifier(x_train,y_train, type2index)\n",
    "nb_word_based_tokenization.train()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-09T08:54:04.886034Z",
     "start_time": "2024-08-09T08:54:04.792969Z"
    }
   },
   "id": "14175695b44455dd",
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BPE Tokenization: Model Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91a306fc1d6b617a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "x_train_bpe = x_train_bpe_feature_matrix\n",
    "\n",
    "nb_bpe = NaiveBayesClassifier(x_train_bpe,y_train, bpe_type2index)\n",
    "nb_bpe.train()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-09T08:54:04.891473Z",
     "start_time": "2024-08-09T08:54:04.886734Z"
    }
   },
   "id": "142252802807dd47",
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prediction"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0717f51cef50855"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def prediction_accuracy(predictions, y_test):\n",
    "    \"\"\"\n",
    "    Calculates the accuracy of a set of predictions against the true labels.\n",
    "    \n",
    "    param:\n",
    "        predictions: A list or array containing predicted class labels (e.g., 0 or 1).\n",
    "        y_test: A list or array containing the true class labels.\n",
    "\n",
    "    return:\n",
    "        float: The accuracy of the predictions, expressed as a percentage (0 to 100).\n",
    "    \"\"\"\n",
    "    correct_predictions = 0\n",
    "    for i in range(len(predictions)):\n",
    "        if wb_predictions[i] == y_test[i]:\n",
    "            correct_predictions += 1\n",
    "    prediction_acc = correct_predictions/len(y_test)\n",
    "    return prediction_acc*100"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-09T08:54:04.894112Z",
     "start_time": "2024-08-09T08:54:04.892510Z"
    }
   },
   "id": "12f899d5651c6633",
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b531cc6f4778b9d1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Word-Based Tokenization Prediction"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "66d984fc70c7c3fb"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "80.09478672985783"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = test_df[\"label\"]\n",
    "y_test = np.array(y_test.map({\"positive\": 1, \"negative\": 0}))\n",
    "y_test = y_test.tolist()\n",
    "x_test = x_test_word_based_feature_matrix\n",
    "wb_predictions = nb_word_based_tokenization.predict(x_test)\n",
    "prediction_accuracy(wb_predictions, y_test)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-09T08:54:05.833274Z",
     "start_time": "2024-08-09T08:54:04.894938Z"
    }
   },
   "id": "8046de9601b5fbc7",
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BPE Tokenization Prediction"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3482db2f8514066c"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "80.09478672985783"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict\n",
    "x_test_bpe = x_test_bpe_feature_matrix\n",
    "bpe_test_predictions = nb_bpe.predict(x_test_bpe)\n",
    "prediction_accuracy(bpe_test_predictions, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-09T08:54:05.931466Z",
     "start_time": "2024-08-09T08:54:05.857660Z"
    }
   },
   "id": "9fb2b25d83589242",
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Evaluation\n",
    "\n",
    "We evaluate the performance of our model on a development set."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a86f647735e60a2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Word-Based Tokenization Model Evaluation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "949907513148f963"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.81      0.80       287\n",
      "           1       0.75      0.72      0.73       225\n",
      "\n",
      "    accuracy                           0.77       512\n",
      "   macro avg       0.76      0.76      0.76       512\n",
      "weighted avg       0.77      0.77      0.77       512\n"
     ]
    }
   ],
   "source": [
    "y_dev = dev_df[\"label\"]\n",
    "y_dev = np.array(y_dev.map({\"positive\": 1, \"negative\": 0}))\n",
    "y_dev = y_dev.tolist()\n",
    "x_dev = x_dev_word_based_feature_matrix\n",
    "\n",
    "wb_eval_predictions = nb_word_based_tokenization.predict(x_dev)\n",
    "print(classification_report(y_dev, wb_eval_predictions))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-09T08:54:06.698335Z",
     "start_time": "2024-08-09T08:54:05.944383Z"
    }
   },
   "id": "67ff3d009c7333b0",
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BPE Tokenization Model Evaluation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "130a9709f6cf5cdf"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.73      0.63       287\n",
      "           1       0.41      0.24      0.30       225\n",
      "\n",
      "    accuracy                           0.52       512\n",
      "   macro avg       0.48      0.49      0.47       512\n",
      "weighted avg       0.49      0.52      0.49       512\n"
     ]
    }
   ],
   "source": [
    "x_dev_bpe = x_dev_bpe_feature_matrix\n",
    "\n",
    "bpe_eval_predictions = nb_word_based_tokenization.predict(x_dev_bpe)\n",
    "print(classification_report(y_dev, bpe_eval_predictions))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-09T08:54:06.792549Z",
     "start_time": "2024-08-09T08:54:06.760385Z"
    }
   },
   "id": "8006238d110c4a22",
   "execution_count": 22
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
