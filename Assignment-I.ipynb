{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Installations, Imports and Downloads\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e57af82a0338e60"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import re\n",
    "import numpy as np\n",
    "warnings.filterwarnings(\"ignore\", category = UserWarning)\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "FS = (8,4) # figure size\n",
    "RS = 264\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T17:39:12.747511Z",
     "start_time": "2024-08-07T17:39:12.730788Z"
    }
   },
   "id": "b77082fbc75833ba",
   "execution_count": 207
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directiory:  /Users/rogerbukuru/Documents/UCT Masters/MSc Statistics and Data Science/NLP-CSC5035Z/NLPTutsAssignments/Assignment-I/afrisent-semeval-2023/afrisent-semeval-2023\n",
      "/Users/rogerbukuru/Documents/UCT Masters/MSc Statistics and Data Science/NLP-CSC5035Z/NLPTutsAssignments/Assignment-I/afrisent-semeval-2023/afrisent-semeval-2023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/pty.py:85: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From https://github.com/afrisenti-semeval/afrisent-semeval-2023\r\n",
      " * branch            HEAD       -> FETCH_HEAD\r\n",
      "Already up to date.\r\n"
     ]
    }
   ],
   "source": [
    "PROJECT_DIR = os.getcwd() + '/afrisent-semeval-2023'\n",
    "print('Current directiory: ', PROJECT_DIR)\n",
    "PROJECT_GITHUB_URL = 'https://github.com/afrisenti-semeval/afrisent-semeval-2023.git'\n",
    "\n",
    "if not os.path.isdir(PROJECT_DIR):\n",
    "  !git clone {PROJECT_GITHUB_URL}\n",
    "else:\n",
    "  %cd {PROJECT_DIR}\n",
    "  !git pull {PROJECT_GITHUB_URL}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T17:39:16.161774Z",
     "start_time": "2024-08-07T17:39:14.892149Z"
    }
   },
   "id": "91fdcbd58f0388f6",
   "execution_count": 208
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Loading\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be842af4aa9e3ccd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Choose language \n",
    "language = 'kin'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T17:39:19.224228Z",
     "start_time": "2024-08-07T17:39:19.216878Z"
    }
   },
   "id": "945c286f95628713",
   "execution_count": 209
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory:  /Users/rogerbukuru/Documents/UCT Masters/MSc Statistics and Data Science/NLP-CSC5035Z/NLPTutsAssignments/Assignment-I/afrisent-semeval-2023/afrisent-semeval-2023/data/kin\n",
      "Train shape:  (3302, 2)\n",
      "Dev shape:  (827, 2)\n",
      "Test shape:  (1026, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                                   text     label\n1126  @user @user @user @user Kuki utegereza c kdi b...  negative\n1922  @user @user @user Si ngombwa iyo yose itegeko ...   neutral\n1626  @user Murakoze namwe, burya turagakorana. Muta...   neutral\n93    Ese kubera iki wumva wakwishimira kurya ibitar...  negative\n2884     @user Nimwihangane kandi Imana yakire Roho ye.  positive\n185   Nonese Twitter niyo ihemba anomangafari ngo @u...  negative\n2905  Tayali ubu Messi akaba yatangiye kwambara vist...  positive\n438        @user @user Ko byaba ari ikibazo,@user @user  negative\n1753  Uyu munsi gahunda ni ukwimana Misiri. Let’s go...   neutral\n1450  @user @user @user @user Ni ukuri peeee, hagati...   neutral",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1126</th>\n      <td>@user @user @user @user Kuki utegereza c kdi b...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>1922</th>\n      <td>@user @user @user Si ngombwa iyo yose itegeko ...</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>1626</th>\n      <td>@user Murakoze namwe, burya turagakorana. Muta...</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>93</th>\n      <td>Ese kubera iki wumva wakwishimira kurya ibitar...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>2884</th>\n      <td>@user Nimwihangane kandi Imana yakire Roho ye.</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>185</th>\n      <td>Nonese Twitter niyo ihemba anomangafari ngo @u...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>2905</th>\n      <td>Tayali ubu Messi akaba yatangiye kwambara vist...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>438</th>\n      <td>@user @user Ko byaba ari ikibazo,@user @user</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>1753</th>\n      <td>Uyu munsi gahunda ni ukwimana Misiri. Let’s go...</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>1450</th>\n      <td>@user @user @user @user Ni ukuri peeee, hagati...</td>\n      <td>neutral</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "DATA_DIR = f'{PROJECT_DIR}/data/{language}'\n",
    "print('Data directory: ', DATA_DIR)\n",
    "\n",
    "train_df = pd.read_csv(f'{DATA_DIR}/train.tsv', sep='\\t', names=['text', 'label'], header=0)\n",
    "dev_df = pd.read_csv(f'{DATA_DIR}/dev.tsv', sep='\\t', names=['text', 'label'], header=0)\n",
    "test_df = pd.read_csv(f'{DATA_DIR}/test.tsv', sep='\\t', names=['text', 'label'], header=0)\n",
    "\n",
    "print('Train shape: ', train_df.shape)\n",
    "print('Dev shape: ', dev_df.shape)\n",
    "print('Test shape: ', test_df.shape)\n",
    "\n",
    "# Display data\n",
    "train_df.sample(n=10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T17:39:20.810180Z",
     "start_time": "2024-08-07T17:39:20.786234Z"
    }
   },
   "id": "b1b9cbe14aa9740f",
   "execution_count": 210
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Cleaning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "19067257a5279928"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Discard neutral examples\n",
    "train_df = train_df[train_df['label'] != 'neutral']\n",
    "dev_df = dev_df[dev_df['label'] != 'neutral']\n",
    "test_df = test_df[test_df['label'] != 'neutral']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T17:39:24.509092Z",
     "start_time": "2024-08-07T17:39:24.503467Z"
    }
   },
   "id": "aca2cf3c14aafd31",
   "execution_count": 211
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    # Replace URLS with [URL]\n",
    "    text = re.sub(r'http\\S+', '[URL]', text)\n",
    "\n",
    "    # Replace numbers with [NUM]\n",
    "    text = re.sub(r'\\d+', '[NUM]', text)\n",
    "\n",
    "    # Remove trailing spaces\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "train_df['text'] = train_df['text'].apply(clean)\n",
    "dev_df['text'] = dev_df['text'].apply(clean)\n",
    "test_df['text'] = test_df['text'].apply(clean)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T17:39:26.781345Z",
     "start_time": "2024-08-07T17:39:26.768157Z"
    }
   },
   "id": "b44a453899f24356",
   "execution_count": 212
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Construct Vocabulary"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4534f864e77f22f8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Count number of tokens in corpus\n",
    "def count_tokens(sentences):\n",
    "    \"\"\"\n",
    "    Count number of tokens in corpus\n",
    "\n",
    "    param: sentences: list of list of tokens e.g. [['This', 'is', 'a', 'sentence'], ['This', 'is', 'another', 'sentence'], ...]\n",
    "    return:\n",
    "        count: number of tokens in corpus\n",
    "    \"\"\"\n",
    "    total_tokens = 0\n",
    "    for sentence in sentences:\n",
    "        total_tokens += len(sentence)\n",
    "    return total_tokens"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T17:39:29.836676Z",
     "start_time": "2024-08-07T17:39:29.833522Z"
    }
   },
   "id": "aa2c3ccefea2eec0",
   "execution_count": 213
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Collect type counts in corpus\n",
    "def create_type_counts(sentences):\n",
    "    \"\"\"\n",
    "    Count number of types in corpus\n",
    "\n",
    "    param: sentences: list of list of tokens e.g. [['This', 'is', 'a', 'sentence'], ['This', 'is', 'another', 'sentence'], ...]\n",
    "    return:\n",
    "        type2count: dictionary of type counts in corpus e.g. {'This': 2, 'sentence': 2, ...}\n",
    "    \"\"\"\n",
    "    type2count = {}\n",
    "    for sentence in sentences:\n",
    "        for type_ in sentence:\n",
    "            if type_ not in type2count:\n",
    "                type2count[type_] = 1\n",
    "            else:\n",
    "                current_count = type2count[type_]\n",
    "                type2count[type_] = current_count +1\n",
    "    return type2count"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T17:39:32.329633Z",
     "start_time": "2024-08-07T17:39:32.324154Z"
    }
   },
   "id": "170232a2a3218e1e",
   "execution_count": 214
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Create vocabulary\n",
    "def create_vocabulary(type2count, min_count):\n",
    "    \"\"\"\n",
    "    This function creates an indexed vocabulary from vocabulary counts and returns it as a list and a dictionary.\n",
    "\n",
    "    param:\n",
    "        type2count: dictionary of type counts in corpus e.g. {'This': 2, 'sentence': 2, ...}\n",
    "        min_count: minimum count of a word to be included in the vocabulary\n",
    "    return:\n",
    "        index2type: list of words in the vocabulary e.g. ['word1', 'word2', 'word3', ...]\n",
    "        type2index: dictionary mapping words to their index in the index2type vocabulary e.g. {'word1': 0, 'word2': 1, 'word3': 2, ...}\n",
    "    \"\"\"\n",
    "    index2type = []\n",
    "    type2index = {}\n",
    "    for type_, count in type2count.items():\n",
    "        if(count >= min_count):\n",
    "            index2type.append(type_)\n",
    "            type2index[type_] = len(index2type) - 1\n",
    "    return index2type, type2index"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T17:39:34.153306Z",
     "start_time": "2024-08-07T17:39:34.149338Z"
    }
   },
   "id": "a55b473b29ace5e4",
   "execution_count": 215
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "\n",
    "# Tokenization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d3285ba6754d912"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Word Based Tokenization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26626c598cae9936"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def whitespace_tokenize(sentences):\n",
    "    return [sentence.split() for sentence in sentences]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T17:39:36.916570Z",
     "start_time": "2024-08-07T17:39:36.909757Z"
    }
   },
   "id": "64f366be8fdce023",
   "execution_count": 216
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Byte-Pair Encoding (BPE) Tokenization\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d72e944dadb4c3f0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class BPETokenizer():\n",
    "\n",
    "    def __init__(self, sentences, vocab_size):\n",
    "        \"\"\"\n",
    "        Initialize the BPE tokenizer.\n",
    "\n",
    "        Args:\n",
    "            sentences (list[str]): list of list of tokens e.g. [['This', 'is', 'a', 'sentence'], ['This', 'is', 'another', 'sentence'], ...]\n",
    "            vocab_size (int): The desired vocabulary size after training.\n",
    "        \"\"\"\n",
    "        self.sentences = sentences\n",
    "        self.vocab_size = vocab_size\n",
    "        self.word_freqs = defaultdict(int)\n",
    "        self.splits = {}\n",
    "        self.merges = {}\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train the BPE tokenizer by iteratively merging the most frequent pairs of symbols.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary of merges in the format {(a, b): 'ab'}, where 'a' and 'b' are symbols merged into 'ab'.\n",
    "        \"\"\"\n",
    "        # Split corpus\n",
    "        for sentence in self.sentences:\n",
    "            for word in sentence:\n",
    "                self.splits[word] = [char for char in word]\n",
    "                    \n",
    "        for i in range(self.vocab_size):\n",
    "            self.compute_pair_freqs() # compute adjacent pair frequencies\n",
    "            pair, _ = list(self.word_freqs.items())[0] # most frequent pair\n",
    "            self.merge_pair(pair[0], pair[1])\n",
    "            self.merges[pair] = pair[0] + pair[1]\n",
    "        return self.merges\n",
    "\n",
    "\n",
    "    def compute_pair_freqs(self):\n",
    "        \"\"\"\n",
    "        Compute the frequency of each pair of symbols in the corpus.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary of pairs and their frequencies in the format {(a, b): frequency}.\n",
    "        \"\"\"\n",
    "        pair_freqs = defaultdict(int)\n",
    "        for _, split in self.splits.items():\n",
    "            for i in range(len(split)-1):\n",
    "                pair = (split[i], split[i+1])\n",
    "                if pair not in pair_freqs:\n",
    "                    pair_freqs[pair] = 1\n",
    "                else:\n",
    "                    pair_freqs[pair] += 1\n",
    "        self.word_freqs = pair_freqs\n",
    "        self.word_freqs = dict(sorted(self.word_freqs.items(), key=lambda x: x[1], reverse=True)) # sort from max to min count\n",
    "        return self.word_freqs\n",
    "        \n",
    "    def merge_pair(self, a, b):\n",
    "        \"\"\"\n",
    "        Merge the given pair of symbols in all words where they appear adjacent.\n",
    "\n",
    "        Args:\n",
    "            a (str): The first symbol in the pair.\n",
    "            b (str): The second symbol in the pair.\n",
    "\n",
    "        Returns:\n",
    "            dict: The updated splits dictionary after merging.\n",
    "        \"\"\"\n",
    "        pair = (a,b)\n",
    "        # Check if valid pair\n",
    "        if pair in self.word_freqs:\n",
    "            new_token = a+b\n",
    "            for word, split in self.splits.items():\n",
    "                for i in range(len(split)-1):\n",
    "                    if split[i] == a and split[i+1] == b:\n",
    "                       split[i] = new_token\n",
    "                       new_split = list(filter(lambda x: x not in [b], split))\n",
    "                       self.splits[word] = new_split\n",
    "        return self.splits\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"\n",
    "        Tokenize a given text using the trained BPE tokenizer.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to be tokenized.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: A list of tokens obtained after applying BPE tokenization.\n",
    "        \"\"\"\n",
    "\n",
    "        pre_tokenized_text = text.split()\n",
    "        splits_text = [[l for l in word] for word in pre_tokenized_text]\n",
    "\n",
    "        for pair, merge in self.merges.items():\n",
    "            for idx, split in enumerate(splits_text):\n",
    "                i = 0\n",
    "                while i < len(split) - 1:\n",
    "                    if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
    "                        split = split[:i] + [merge] + split[i + 2 :]\n",
    "                    else:\n",
    "                        i += 1\n",
    "                splits_text[idx] = split\n",
    "        result = sum(splits_text, [])\n",
    "        return result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T17:39:47.945027Z",
     "start_time": "2024-08-07T17:39:47.937979Z"
    }
   },
   "id": "185fdac1da4c7403",
   "execution_count": 218
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature Extraction"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "47386243702bc12f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Simple Binary Text Vectorization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6aadfd89ead00bf5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def simple_binary_vectorization(sentences, type2index):\n",
    "    \"\"\"\n",
    "    Binary text-vectorization of a list of sentences.\n",
    "\n",
    "    param:\n",
    "        list of list of tokens e.g. [['This', 'is', 'a', 'sentence'], ['This', 'is', 'another', 'sentence'], ...]\n",
    "        type2index: dictionary mapping words to their index in the vocabulary e.g. {'word1': 0, 'word2': 1, 'word3': 2, ...}\n",
    "    return:\n",
    "        one_hot_sentences: 2d numpy array of one-hot encoded sentences e.g. [[1, 0, 0, 1, ...], [0, 1, 1, 0, ...], ...]\n",
    "    \"\"\"\n",
    "    vectors = []\n",
    "    for sentence in sentences:\n",
    "        vector = np.zeros(len(type2index.items()))\n",
    "        for word in sentence:\n",
    "            if word in type2index: # we ignore the word if it does not exist in our vocab\n",
    "             word_index = type2index[word]\n",
    "             vector[word_index] = 1\n",
    "        vectors.append(np.array(vector))\n",
    "    return np.array(vectors)\n",
    "\n",
    "     "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T17:39:55.408491Z",
     "start_time": "2024-08-07T17:39:55.403453Z"
    }
   },
   "id": "7298bb785e0a0987",
   "execution_count": 219
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Word-Based Tokenization Features\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37333bc85957788f"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in corpus:  30894\n",
      "Vocab size: 13674\n"
     ]
    }
   ],
   "source": [
    "# Store training data text as list of tweets\n",
    "train_corpus = train_df['text'].tolist()\n",
    "tokenized_train_corpus = whitespace_tokenize(train_corpus)\n",
    "num_tokens = count_tokens(tokenized_train_corpus)\n",
    "print('Number of tokens in corpus: ', num_tokens)\n",
    "type2count = create_type_counts(tokenized_train_corpus)\n",
    "# Sort types by counts\n",
    "type2count = dict(sorted(type2count.items(), key=lambda x: x[1], reverse=True))\n",
    "index2type, type2index = create_vocabulary(type2count, min_count=1)\n",
    "print(\"Vocab size:\", len(index2type))\n",
    "\n",
    "# It's good practice to add a special token for unknown words and padding (to make all sentences in training batches the same length)\n",
    "type2index['<UNK>'] = len(index2type)\n",
    "index2type.append('<UNK>')\n",
    "type2index['<PAD>'] = len(index2type)\n",
    "index2type.append('<PAD>')\n",
    "x_train_word_based_feature_matrix = simple_binary_vectorization(tokenized_train_corpus, type2index)\n",
    "\n",
    "dev_corpus = dev_df['text'].tolist()\n",
    "tokenized_dev_corpus = whitespace_tokenize(dev_corpus)\n",
    "x_dev_word_based_feature_matrix = simple_binary_vectorization(tokenized_dev_corpus, type2index)\n",
    "\n",
    "test_corpus = test_df['text'].tolist()\n",
    "tokenized_test_corpus = whitespace_tokenize(test_corpus)\n",
    "x_test_word_based_feature_matrix = simple_binary_vectorization(tokenized_test_corpus, type2index)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T17:39:57.879700Z",
     "start_time": "2024-08-07T17:39:57.676905Z"
    }
   },
   "id": "c5f9f9e91d866b89",
   "execution_count": 220
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BPE Tokenization Features"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1183ca42d8e9f287"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Train BPE\n",
    "bpe = BPETokenizer(tokenized_train_corpus, vocab_size=1000)\n",
    "merges = bpe.train()\n",
    "\n",
    "\n",
    "# Apply to our dataset\n",
    "train_df['bpe_text'] = train_df['text'].apply(lambda x: ' '.join(bpe.tokenize(x)))\n",
    "dev_df['bpe_text'] = dev_df['text'].apply(lambda x: ' '.join(bpe.tokenize(x)))\n",
    "test_df['bpe_text'] = test_df['text'].apply(lambda x: ' '.join(bpe.tokenize(x)))\n",
    "train_df.head()\n",
    "\n",
    "bpe_train_corpus = train_df['bpe_text'].tolist()\n",
    "tokenized_bpe_train_corpus = whitespace_tokenize(bpe_train_corpus)\n",
    "bpe_type2count = create_type_counts(tokenized_bpe_train_corpus)\n",
    "\n",
    "# Sort types by counts\n",
    "bpe_type2count = dict(sorted(bpe_type2count.items(), key=lambda x: x[1], reverse=True))\n",
    "bpe_index2type, bpe_type2index = create_vocabulary(bpe_type2count, min_count=1)\n",
    "\n",
    "x_train_bpe_feature_matrix = simple_binary_vectorization(tokenized_bpe_train_corpus, bpe_type2index)\n",
    "\n",
    "bpe_dev_corpus = dev_df['bpe_text'].tolist()\n",
    "tokenized_bpe_dev_corpus = whitespace_tokenize(bpe_dev_corpus)\n",
    "x_dev_bpe_feature_matrix = simple_binary_vectorization(tokenized_bpe_dev_corpus, bpe_type2index)\n",
    "\n",
    "bpe_test_corpus = test_df['bpe_text'].tolist()\n",
    "tokenized_bpe_test_corpus = whitespace_tokenize(bpe_test_corpus)\n",
    "x_test_bpe_feature_matrix = simple_binary_vectorization(bpe_test_corpus, bpe_type2index)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T17:40:24.019473Z",
     "start_time": "2024-08-07T17:40:00.146071Z"
    }
   },
   "id": "8e66e51edab48fb3",
   "execution_count": 221
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Training\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "995a5523bec920cd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier():\n",
    "    \n",
    "    def __init__(self, x_train, y_train, type2index):\n",
    "        self.class_priors = {}\n",
    "        self.likelihoods_probs = {}\n",
    "        self.word_counts = {}\n",
    "        self.vocabulary = type2index\n",
    "        self.Y_train = y_train\n",
    "        self.X_train = x_train\n",
    "        self.vocab_size = len(type2index.items())\n",
    "    \n",
    "    def train(self):\n",
    "        # Calculate class prior probabilities\n",
    "        self.compute_prior_probs()\n",
    "        # Calculate likelihood probabilities\n",
    "        self.compute_likelihood_probs() \n",
    "    \n",
    "    def compute_prior_probs(self):\n",
    "    \n",
    "        positive_sentences = 0\n",
    "        for label in self.Y_train:\n",
    "            if label == 1:\n",
    "                positive_sentences += 1\n",
    "        self.class_priors[1] = positive_sentences/len(self.Y_train)\n",
    "        self.class_priors[0] = 1 - self.class_priors[1]\n",
    "        \n",
    "    def compute_likelihood_probs(self):\n",
    "        \n",
    "        self.word_counts = {0: np.zeros(self.vocab_size), 1: np.zeros(self.vocab_size)}\n",
    "        for sentence_vector, label  in zip(self.X_train, self.Y_train):\n",
    "            self.word_counts[label] += sentence_vector\n",
    "        for class_ in self.word_counts:\n",
    "            # Compute based on Laplace Transform\n",
    "            self.likelihoods_probs[class_] = (self.word_counts[class_] + 1)/ (sum(self.word_counts[class_]) + self.vocab_size)\n",
    "            \n",
    "         \n",
    "    def predict(self,x):\n",
    "        predictions = []\n",
    "        for sentence_vector in x: # text vectorization of sentence\n",
    "            posteriors = {}\n",
    "            for class_  in self.class_priors:\n",
    "                log_posterior = np.log(self.class_priors[class_])\n",
    "                for i in range(len(sentence_vector)):\n",
    "                    if sentence_vector[i] == 1:\n",
    "                        log_posterior += np.log(self.likelihoods_probs[class_][i])\n",
    "                posteriors[class_] = log_posterior\n",
    "            predicted_class = max(posteriors, key=posteriors.get)\n",
    "            predictions.append(predicted_class)\n",
    "        return predictions\n",
    "    \n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T17:40:26.896189Z",
     "start_time": "2024-08-07T17:40:26.893659Z"
    }
   },
   "id": "30bae654ab9c2954",
   "execution_count": 222
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Word-Based Tokenization: Model Training and Prediction"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f69299d7fe8ea867"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "y_train = train_df[\"label\"]\n",
    "y_train = np.array(y_train.map({\"positive\": 1, \"negative\": 0}))\n",
    "x_train = x_train_word_based_feature_matrix\n",
    "\n",
    "# Train Model\n",
    "nb_word_based_tokenization = NaiveBayesClassifier(x_train,y_train, type2index)\n",
    "nb_word_based_tokenization.train()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T17:40:30.240636Z",
     "start_time": "2024-08-07T17:40:30.211090Z"
    }
   },
   "id": "14175695b44455dd",
   "execution_count": 223
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BPE Tokenization: Model Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91a306fc1d6b617a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "x_train_bpe = x_train_bpe_feature_matrix\n",
    "\n",
    "nb_bpe = NaiveBayesClassifier(x_train_bpe,y_train, bpe_type2index)\n",
    "nb_bpe.train()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T17:40:33.310510Z",
     "start_time": "2024-08-07T17:40:33.304502Z"
    }
   },
   "id": "142252802807dd47",
   "execution_count": 224
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prediction"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0717f51cef50855"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def prediction_accuracy(predictions, y_test):\n",
    "    correct_predictions = 0\n",
    "    for i in range(len(predictions)):\n",
    "        if wb_predictions[i] == y_test[i]:\n",
    "            correct_predictions += 1\n",
    "    prediction_acc = correct_predictions/len(y_test)\n",
    "    return prediction_acc*100"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T17:40:36.977554Z",
     "start_time": "2024-08-07T17:40:36.972848Z"
    }
   },
   "id": "12f899d5651c6633",
   "execution_count": 225
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b531cc6f4778b9d1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Word-Based Tokenization Prediction"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "66d984fc70c7c3fb"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "80.09478672985783"
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = test_df[\"label\"]\n",
    "y_test = np.array(y_test.map({\"positive\": 1, \"negative\": 0}))\n",
    "y_test = y_test.tolist()\n",
    "x_test = x_test_word_based_feature_matrix\n",
    "wb_predictions = nb_word_based_tokenization.predict(x_test)\n",
    "prediction_accuracy(wb_predictions, y_test)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T17:40:40.445101Z",
     "start_time": "2024-08-07T17:40:39.447249Z"
    }
   },
   "id": "8046de9601b5fbc7",
   "execution_count": 226
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BPE Tokenization Prediction"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3482db2f8514066c"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "80.09478672985783"
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict\n",
    "x_test_bpe = x_test_bpe_feature_matrix\n",
    "bpe_test_predictions = nb_bpe.predict(x_test_bpe)\n",
    "prediction_accuracy(bpe_test_predictions, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T17:40:44.376855Z",
     "start_time": "2024-08-07T17:40:44.306652Z"
    }
   },
   "id": "9fb2b25d83589242",
   "execution_count": 227
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Evaluation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a86f647735e60a2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Word-Based Tokenization Model Evaluation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "949907513148f963"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.82      0.80       287\n",
      "           1       0.76      0.72      0.74       225\n",
      "\n",
      "    accuracy                           0.78       512\n",
      "   macro avg       0.77      0.77      0.77       512\n",
      "weighted avg       0.77      0.78      0.77       512\n"
     ]
    }
   ],
   "source": [
    "y_dev = dev_df[\"label\"]\n",
    "y_dev = np.array(y_dev.map({\"positive\": 1, \"negative\": 0}))\n",
    "y_dev = y_dev.tolist()\n",
    "x_dev = x_dev_word_based_feature_matrix\n",
    "\n",
    "wb_eval_predictions = nb_word_based_tokenization.predict(x_dev)\n",
    "print(classification_report(y_dev, wb_eval_predictions))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T17:40:51.311845Z",
     "start_time": "2024-08-07T17:40:50.521824Z"
    }
   },
   "id": "67ff3d009c7333b0",
   "execution_count": 228
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BPE Tokenization Model Evaluation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "130a9709f6cf5cdf"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.74      0.63       287\n",
      "           1       0.42      0.25      0.31       225\n",
      "\n",
      "    accuracy                           0.52       512\n",
      "   macro avg       0.49      0.49      0.47       512\n",
      "weighted avg       0.50      0.52      0.49       512\n"
     ]
    }
   ],
   "source": [
    "x_dev_bpe = x_dev_bpe_feature_matrix\n",
    "\n",
    "bpe_eval_predictions = nb_word_based_tokenization.predict(x_dev_bpe)\n",
    "print(classification_report(y_dev, bpe_eval_predictions))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T17:40:57.682228Z",
     "start_time": "2024-08-07T17:40:57.643385Z"
    }
   },
   "id": "8006238d110c4a22",
   "execution_count": 229
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
