{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Installations, Imports and Downloads\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e57af82a0338e60"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import re\n",
    "import numpy as np\n",
    "warnings.filterwarnings(\"ignore\", category = UserWarning)\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "FS = (8,4) # figure size\n",
    "RS = 264\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T13:05:39.795455Z",
     "start_time": "2024-08-06T13:05:38.798543Z"
    }
   },
   "id": "b77082fbc75833ba",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directiory:  /Users/rogerbukuru/Documents/UCT Masters/MSc Statistics and Data Science/NLP-CSC5035Z/NLPTutsAssignments/Assignment-I/afrisent-semeval-2023\n",
      "/Users/rogerbukuru/Documents/UCT Masters/MSc Statistics and Data Science/NLP-CSC5035Z/NLPTutsAssignments/Assignment-I/afrisent-semeval-2023\n",
      "From https://github.com/afrisenti-semeval/afrisent-semeval-2023\r\n",
      " * branch            HEAD       -> FETCH_HEAD\r\n",
      "Already up to date.\r\n"
     ]
    }
   ],
   "source": [
    "PROJECT_DIR = os.getcwd() + '/afrisent-semeval-2023'\n",
    "print('Current directiory: ', PROJECT_DIR)\n",
    "PROJECT_GITHUB_URL = 'https://github.com/afrisenti-semeval/afrisent-semeval-2023.git'\n",
    "\n",
    "if not os.path.isdir(PROJECT_DIR):\n",
    "  !git clone {PROJECT_GITHUB_URL}\n",
    "else:\n",
    "  %cd {PROJECT_DIR}\n",
    "  !git pull {PROJECT_GITHUB_URL}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T13:05:43.572767Z",
     "start_time": "2024-08-06T13:05:42.653635Z"
    }
   },
   "id": "91fdcbd58f0388f6",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Loading\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be842af4aa9e3ccd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Choose language \n",
    "language = 'swa'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T13:12:39.923474Z",
     "start_time": "2024-08-06T13:12:39.881694Z"
    }
   },
   "id": "945c286f95628713",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory:  /Users/rogerbukuru/Documents/UCT Masters/MSc Statistics and Data Science/NLP-CSC5035Z/NLPTutsAssignments/Assignment-I/afrisent-semeval-2023/data/swa\n",
      "Train shape:  (1810, 2)\n",
      "Dev shape:  (453, 2)\n",
      "Test shape:  (748, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                                   text     label\n100   Hili tatizo kubwa sana kwetumtu akijaribu kutu...  negative\n559   Wameambiwa na mwenyekiti wao kila kitu ni sawa...   neutral\n1440  Kumbe tunaifukuzia China kwa wingitushafika bi...  positive\n119   Show nyingi za kibongo Show ikiisha Shabiki un...  negative\n1473  Katika kuhakikisha tunafikia 5050 Wanahabari w...  positive\n528   Hili jambo sio dogo kama tunavyofikiriahii eli...   neutral\n1456  Hii ni Hatua Muhimu sana pongezi kwa kikao hic...  positive\n138   Kama uamini kaamuulize Agger karacai msanii wa...  negative\n605   Habari ahsante kwa kuwasiliana nasi Naomba ufa...   neutral\n276   Ujumbe huo unamaanisha haukujitoa katika hudum...   neutral",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>100</th>\n      <td>Hili tatizo kubwa sana kwetumtu akijaribu kutu...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>559</th>\n      <td>Wameambiwa na mwenyekiti wao kila kitu ni sawa...</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>1440</th>\n      <td>Kumbe tunaifukuzia China kwa wingitushafika bi...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>119</th>\n      <td>Show nyingi za kibongo Show ikiisha Shabiki un...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>1473</th>\n      <td>Katika kuhakikisha tunafikia 5050 Wanahabari w...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>528</th>\n      <td>Hili jambo sio dogo kama tunavyofikiriahii eli...</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>1456</th>\n      <td>Hii ni Hatua Muhimu sana pongezi kwa kikao hic...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>138</th>\n      <td>Kama uamini kaamuulize Agger karacai msanii wa...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>605</th>\n      <td>Habari ahsante kwa kuwasiliana nasi Naomba ufa...</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>276</th>\n      <td>Ujumbe huo unamaanisha haukujitoa katika hudum...</td>\n      <td>neutral</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "DATA_DIR = f'{PROJECT_DIR}/data/{language}'\n",
    "print('Data directory: ', DATA_DIR)\n",
    "\n",
    "train_df = pd.read_csv(f'{DATA_DIR}/train.tsv', sep='\\t', names=['text', 'label'], header=0)\n",
    "dev_df = pd.read_csv(f'{DATA_DIR}/dev.tsv', sep='\\t', names=['text', 'label'], header=0)\n",
    "test_df = pd.read_csv(f'{DATA_DIR}/test.tsv', sep='\\t', names=['text', 'label'], header=0)\n",
    "\n",
    "print('Train shape: ', train_df.shape)\n",
    "print('Dev shape: ', dev_df.shape)\n",
    "print('Test shape: ', test_df.shape)\n",
    "\n",
    "# Display data\n",
    "train_df.sample(n=10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T13:12:41.726926Z",
     "start_time": "2024-08-06T13:12:41.708003Z"
    }
   },
   "id": "b1b9cbe14aa9740f",
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Cleaning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "19067257a5279928"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Discard neutral examples\n",
    "train_df = train_df[train_df['label'] != 'neutral']\n",
    "dev_df = dev_df[dev_df['label'] != 'neutral']\n",
    "test_df = test_df[test_df['label'] != 'neutral']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T13:12:47.124533Z",
     "start_time": "2024-08-06T13:12:47.122202Z"
    }
   },
   "id": "aca2cf3c14aafd31",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    # Replace URLS with [URL]\n",
    "    text = re.sub(r'http\\S+', '[URL]', text)\n",
    "\n",
    "    # Replace numbers with [NUM]\n",
    "    text = re.sub(r'\\d+', '[NUM]', text)\n",
    "\n",
    "    # Remove trailing spaces\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "train_df['text'] = train_df['text'].apply(clean)\n",
    "dev_df['text'] = dev_df['text'].apply(clean)\n",
    "test_df['text'] = test_df['text'].apply(clean)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T13:12:48.260561Z",
     "start_time": "2024-08-06T13:12:48.253411Z"
    }
   },
   "id": "b44a453899f24356",
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Construct Vocabulary"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4534f864e77f22f8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Count number of tokens in corpus\n",
    "def count_tokens(sentences):\n",
    "    \"\"\"\n",
    "    Count number of tokens in corpus\n",
    "\n",
    "    param: sentences: list of list of tokens e.g. [['This', 'is', 'a', 'sentence'], ['This', 'is', 'another', 'sentence'], ...]\n",
    "    return:\n",
    "        count: number of tokens in corpus\n",
    "    \"\"\"\n",
    "    total_tokens = 0\n",
    "    for sentence in sentences:\n",
    "        total_tokens += len(sentence)\n",
    "    return total_tokens"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T13:06:05.526828Z",
     "start_time": "2024-08-06T13:06:05.525120Z"
    }
   },
   "id": "aa2c3ccefea2eec0",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Collect type counts in corpus\n",
    "def create_type_counts(sentences):\n",
    "    \"\"\"\n",
    "    Count number of types in corpus\n",
    "\n",
    "    param: sentences: list of list of tokens e.g. [['This', 'is', 'a', 'sentence'], ['This', 'is', 'another', 'sentence'], ...]\n",
    "    return:\n",
    "        type2count: dictionary of type counts in corpus e.g. {'This': 2, 'sentence': 2, ...}\n",
    "    \"\"\"\n",
    "    type2count = {}\n",
    "    for sentence in sentences:\n",
    "        for type_ in sentence:\n",
    "            if type_ not in type2count:\n",
    "                type2count[type_] = 1\n",
    "            else:\n",
    "                current_count = type2count[type_]\n",
    "                type2count[type_] = current_count +1\n",
    "    return type2count"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T13:06:07.519064Z",
     "start_time": "2024-08-06T13:06:07.516401Z"
    }
   },
   "id": "170232a2a3218e1e",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Create vocabulary\n",
    "def create_vocabulary(type2count, min_count):\n",
    "    \"\"\"\n",
    "    This function creates an indexed vocabulary from vocabulary counts and returns it as a list and a dictionary.\n",
    "\n",
    "    param:\n",
    "        type2count: dictionary of type counts in corpus e.g. {'This': 2, 'sentence': 2, ...}\n",
    "        min_count: minimum count of a word to be included in the vocabulary\n",
    "    return:\n",
    "        index2type: list of words in the vocabulary e.g. ['word1', 'word2', 'word3', ...]\n",
    "        type2index: dictionary mapping words to their index in the index2type vocabulary e.g. {'word1': 0, 'word2': 1, 'word3': 2, ...}\n",
    "    \"\"\"\n",
    "    index2type = []\n",
    "    type2index = {}\n",
    "    for type_, count in type2count.items():\n",
    "        if(count >= min_count):\n",
    "            index2type.append(type_)\n",
    "            type2index[type_] = len(index2type) - 1\n",
    "    return index2type, type2index"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T13:06:09.838059Z",
     "start_time": "2024-08-06T13:06:09.836787Z"
    }
   },
   "id": "a55b473b29ace5e4",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "\n",
    "# Tokenization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d3285ba6754d912"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Word Based Tokenization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26626c598cae9936"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def whitespace_tokenize(sentences):\n",
    "    return [sentence.split() for sentence in sentences]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T13:06:12.644621Z",
     "start_time": "2024-08-06T13:06:12.641146Z"
    }
   },
   "id": "64f366be8fdce023",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Byte-Pair Encoding (BPE) Tokenization\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d72e944dadb4c3f0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class BPETokenizer():\n",
    "\n",
    "    def __init__(self, sentences, vocab_size):\n",
    "        \"\"\"\n",
    "        Initialize the BPE tokenizer.\n",
    "\n",
    "        Args:\n",
    "            sentences (list[str]): list of list of tokens e.g. [['This', 'is', 'a', 'sentence'], ['This', 'is', 'another', 'sentence'], ...]\n",
    "            vocab_size (int): The desired vocabulary size after training.\n",
    "        \"\"\"\n",
    "        self.sentences = sentences\n",
    "        self.vocab_size = vocab_size\n",
    "        self.word_freqs = defaultdict(int)\n",
    "        self.splits = {}\n",
    "        self.merges = {}\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train the BPE tokenizer by iteratively merging the most frequent pairs of symbols.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary of merges in the format {(a, b): 'ab'}, where 'a' and 'b' are symbols merged into 'ab'.\n",
    "        \"\"\"\n",
    "        # Split corpus\n",
    "        for sentence in self.sentences:\n",
    "            for word in sentence:\n",
    "                self.splits[word] = [char for char in word]\n",
    "                    \n",
    "        for i in range(self.vocab_size):\n",
    "            self.compute_pair_freqs() # compute adjacent pair frequencies\n",
    "            pair, _ = list(self.word_freqs.items())[0] # most frequent pair\n",
    "            self.merge_pair(pair[0], pair[1])\n",
    "            self.merges[pair] = pair[0] + pair[1]\n",
    "        return self.merges\n",
    "\n",
    "\n",
    "    def compute_pair_freqs(self):\n",
    "        \"\"\"\n",
    "        Compute the frequency of each pair of symbols in the corpus.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary of pairs and their frequencies in the format {(a, b): frequency}.\n",
    "        \"\"\"\n",
    "        pair_freqs = defaultdict(int)\n",
    "        for _, split in self.splits.items():\n",
    "            for i in range(len(split)-1):\n",
    "                pair = (split[i], split[i+1])\n",
    "                if pair not in pair_freqs:\n",
    "                    pair_freqs[pair] = 1\n",
    "                else:\n",
    "                    pair_freqs[pair] += 1\n",
    "        self.word_freqs = pair_freqs\n",
    "        self.word_freqs = dict(sorted(self.word_freqs.items(), key=lambda x: x[1], reverse=True)) # sort from max to min count\n",
    "        return self.word_freqs\n",
    "        \n",
    "    def merge_pair(self, a, b):\n",
    "        \"\"\"\n",
    "        Merge the given pair of symbols in all words where they appear adjacent.\n",
    "\n",
    "        Args:\n",
    "            a (str): The first symbol in the pair.\n",
    "            b (str): The second symbol in the pair.\n",
    "\n",
    "        Returns:\n",
    "            dict: The updated splits dictionary after merging.\n",
    "        \"\"\"\n",
    "        pair = (a,b)\n",
    "        # Check if valid pair\n",
    "        if pair in self.word_freqs:\n",
    "            new_token = a+b\n",
    "            for word, split in self.splits.items():\n",
    "                for i in range(len(split)-1):\n",
    "                    if split[i] == a and split[i+1] == b:\n",
    "                       split[i] = new_token\n",
    "                       new_split = list(filter(lambda x: x not in [b], split))\n",
    "                       self.splits[word] = new_split\n",
    "        return self.splits\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"\n",
    "        Tokenize a given text using the trained BPE tokenizer.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to be tokenized.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: A list of tokens obtained after applying BPE tokenization.\n",
    "        \"\"\"\n",
    "\n",
    "        pre_tokenized_text = text.split()\n",
    "        splits_text = [[l for l in word] for word in pre_tokenized_text]\n",
    "\n",
    "        for pair, merge in self.merges.items():\n",
    "            for idx, split in enumerate(splits_text):\n",
    "                i = 0\n",
    "                while i < len(split) - 1:\n",
    "                    if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
    "                        split = split[:i] + [merge] + split[i + 2 :]\n",
    "                    else:\n",
    "                        i += 1\n",
    "                splits_text[idx] = split\n",
    "        result = sum(splits_text, [])\n",
    "        return result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T13:06:14.522763Z",
     "start_time": "2024-08-06T13:06:14.519378Z"
    }
   },
   "id": "185fdac1da4c7403",
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature Extraction"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "47386243702bc12f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Simple Binary Text Vectorization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6aadfd89ead00bf5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def simple_binary_vectorization(sentences, type2index):\n",
    "    \"\"\"\n",
    "    Binary text-vectorization of a list of sentences.\n",
    "\n",
    "    param:\n",
    "        list of list of tokens e.g. [['This', 'is', 'a', 'sentence'], ['This', 'is', 'another', 'sentence'], ...]\n",
    "        type2index: dictionary mapping words to their index in the vocabulary e.g. {'word1': 0, 'word2': 1, 'word3': 2, ...}\n",
    "    return:\n",
    "        one_hot_sentences: 2d numpy array of one-hot encoded sentences e.g. [[1, 0, 0, 1, ...], [0, 1, 1, 0, ...], ...]\n",
    "    \"\"\"\n",
    "    vectors = []\n",
    "    for sentence in sentences:\n",
    "        vector = np.zeros(len(type2index.items()))\n",
    "        for word in sentence:\n",
    "            if word in type2index: # we ignore the word if it does not exist in our vocab\n",
    "             word_index = type2index[word]\n",
    "             vector[word_index] = 1\n",
    "        vectors.append(np.array(vector))\n",
    "    return np.array(vectors)\n",
    "\n",
    "     "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T14:46:01.727184Z",
     "start_time": "2024-08-07T14:46:01.722945Z"
    }
   },
   "id": "7298bb785e0a0987",
   "execution_count": 173
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Word-Based Tokenization Features\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37333bc85957788f"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in corpus:  13122\n",
      "Vocab size: 5383\n"
     ]
    }
   ],
   "source": [
    "# Store training data text as list of tweets\n",
    "train_corpus = train_df['text'].tolist()\n",
    "tokenized_train_corpus = whitespace_tokenize(train_corpus)\n",
    "num_tokens = count_tokens(tokenized_train_corpus)\n",
    "print('Number of tokens in corpus: ', num_tokens)\n",
    "type2count = create_type_counts(tokenized_train_corpus)\n",
    "# Sort types by counts\n",
    "type2count = dict(sorted(type2count.items(), key=lambda x: x[1], reverse=True))\n",
    "index2type, type2index = create_vocabulary(type2count, min_count=1)\n",
    "print(\"Vocab size:\", len(index2type))\n",
    "\n",
    "# It's good practice to add a special token for unknown words and padding (to make all sentences in training batches the same length)\n",
    "type2index['<UNK>'] = len(index2type)\n",
    "index2type.append('<UNK>')\n",
    "type2index['<PAD>'] = len(index2type)\n",
    "index2type.append('<PAD>')\n",
    "x_train_word_based_feature_matrix = simple_binary_vectorization(tokenized_train_corpus, type2index)\n",
    "\n",
    "dev_corpus = dev_df['text'].tolist()\n",
    "tokenized_dev_corpus = whitespace_tokenize(dev_corpus)\n",
    "x_dev_word_based_feature_matrix = simple_binary_vectorization(tokenized_dev_corpus, type2index)\n",
    "\n",
    "test_corpus = test_df['text'].tolist()\n",
    "tokenized_test_corpus = whitespace_tokenize(test_corpus)\n",
    "x_test_word_based_feature_matrix = simple_binary_vectorization(tokenized_test_corpus, type2index)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T14:46:03.812614Z",
     "start_time": "2024-08-07T14:46:03.780210Z"
    }
   },
   "id": "c5f9f9e91d866b89",
   "execution_count": 174
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BPE Tokenization Features"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1183ca42d8e9f287"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Train BPE\n",
    "bpe = BPETokenizer(tokenized_train_corpus, vocab_size=1000)\n",
    "merges = bpe.train()\n",
    "#print(\"Merges\", merges)\n",
    "# Apply to our dataset\n",
    "train_df['bpe_text'] = train_df['text'].apply(lambda x: ' '.join(bpe.tokenize(x)))\n",
    "dev_df['bpe_text'] = dev_df['text'].apply(lambda x: ' '.join(bpe.tokenize(x)))\n",
    "test_df['bpe_text'] = test_df['text'].apply(lambda x: ' '.join(bpe.tokenize(x)))\n",
    "train_df.head()\n",
    "\n",
    "bpe_train_corpus = train_df['bpe_text'].tolist()\n",
    "tokenized_bpe_train_corpus = whitespace_tokenize(bpe_train_corpus)\n",
    "bpe_type2count = create_type_counts(tokenized_bpe_train_corpus)\n",
    "# Sort types by counts\n",
    "bpe_type2count = dict(sorted(bpe_type2count.items(), key=lambda x: x[1], reverse=True))\n",
    "bpe_index2type, bpe_type2index = create_vocabulary(bpe_type2count, min_count=1)\n",
    "\n",
    "x_train_bpe_feature_matrix = simple_binary_vectorization(tokenized_bpe_train_corpus, bpe_type2index)\n",
    "\n",
    "bpe_dev_corpus = dev_df['bpe_text'].tolist()\n",
    "tokenized_bpe_dev_corpus = whitespace_tokenize(bpe_dev_corpus)\n",
    "x_dev_bpe_feature_matrix = simple_binary_vectorization(tokenized_bpe_dev_corpus, bpe_type2index)\n",
    "\n",
    "bpe_test_corpus = test_df['bpe_text'].tolist()\n",
    "tokenized_bpe_test_corpus = whitespace_tokenize(bpe_test_corpus)\n",
    "x_test_bpe_feature_matrix = simple_binary_vectorization(bpe_test_corpus, bpe_type2index)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T14:49:36.810834Z",
     "start_time": "2024-08-07T14:49:27.499735Z"
    }
   },
   "id": "8e66e51edab48fb3",
   "execution_count": 175
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Training\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "995a5523bec920cd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier():\n",
    "    \n",
    "    def __init__(self, x_train, y_train, type2index):\n",
    "        self.class_priors = {}\n",
    "        self.likelihoods_probs = {}\n",
    "        self.word_counts = {}\n",
    "        self.vocabulary = type2index\n",
    "        self.Y_train = y_train\n",
    "        self.X_train = x_train\n",
    "        self.vocab_size = len(type2index.items())\n",
    "    \n",
    "    def train(self):\n",
    "        # Calculate class prior probabilities\n",
    "        self.compute_prior_probs()\n",
    "        # Calculate likelihood probabilities\n",
    "        self.compute_likelihood_probs() \n",
    "    \n",
    "    def compute_prior_probs(self):\n",
    "    \n",
    "        positive_sentences = 0\n",
    "        for label in self.Y_train:\n",
    "            if label == 1:\n",
    "                positive_sentences += 1\n",
    "        self.class_priors[1] = positive_sentences/len(self.Y_train)\n",
    "        self.class_priors[0] = 1 - self.class_priors[1]\n",
    "        \n",
    "    def compute_likelihood_probs(self):\n",
    "        \n",
    "        self.word_counts = {0: np.zeros(self.vocab_size), 1: np.zeros(self.vocab_size)}\n",
    "        for sentence_vector, label  in zip(self.X_train, self.Y_train):\n",
    "            self.word_counts[label] += sentence_vector\n",
    "        for class_ in self.word_counts:\n",
    "            # Compute based on Laplace Transform\n",
    "            self.likelihoods_probs[class_] = (self.word_counts[class_] + 1)/ (sum(self.word_counts[class_]) + self.vocab_size)\n",
    "            \n",
    "         \n",
    "    def predict(self,x):\n",
    "        predictions = []\n",
    "        for sentence_vector in x: # text vectorization of sentence\n",
    "            posteriors = {}\n",
    "            for class_  in self.class_priors:\n",
    "                log_posterior = np.log(self.class_priors[class_])\n",
    "                for i in range(len(sentence_vector)):\n",
    "                    if sentence_vector[i] == 1:\n",
    "                        log_posterior += np.log(self.likelihoods_probs[class_][i])\n",
    "                posteriors[class_] = log_posterior\n",
    "            predicted_class = max(posteriors, key=posteriors.get)\n",
    "            predictions.append(predicted_class)\n",
    "        return predictions\n",
    "    \n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T15:28:09.636285Z",
     "start_time": "2024-08-07T15:28:09.631475Z"
    }
   },
   "id": "30bae654ab9c2954",
   "execution_count": 180
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Word-Based Tokenization: Model Training and Prediction"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f69299d7fe8ea867"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "y_train = train_df[\"label\"]\n",
    "y_train = np.array(y_train.map({\"positive\": 1, \"negative\": 0}))\n",
    "x_train = x_train_word_based_feature_matrix\n",
    "\n",
    "# Train Model\n",
    "nb_word_based_tokenization = NaiveBayesClassifier(x_train,y_train, type2index)\n",
    "nb_word_based_tokenization.train()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T15:28:11.741452Z",
     "start_time": "2024-08-07T15:28:11.737199Z"
    }
   },
   "id": "14175695b44455dd",
   "execution_count": 181
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BPE Tokenization: Model Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91a306fc1d6b617a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "x_train_bpe = x_train_bpe_feature_matrix\n",
    "\n",
    "nb_bpe = NaiveBayesClassifier(x_train_bpe,y_train, bpe_type2index)\n",
    "nb_bpe.train()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "142252802807dd47"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prediction"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0717f51cef50855"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Word-Based Tokenization Prediction"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "66d984fc70c7c3fb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "x_test = x_test_word_based_feature_matrix\n",
    "nb_word_based_tokenization.predict(x_test)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8046de9601b5fbc7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BPE Tokenization Prediction"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3482db2f8514066c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Predict\n",
    "x_test_bpe = x_test_bpe_feature_matrix\n",
    "bpe_test_pred = nb_bpe.predict(x_test_bpe)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9fb2b25d83589242"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Evaluation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a86f647735e60a2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Word Based Tokenization Model Evaluation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "949907513148f963"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "x_dev = x_dev_word_based_feature_matrix\n",
    "nb_word_based_tokenization.predict(x_dev)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "67ff3d009c7333b0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BPE Tokenization Model Evaluation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "130a9709f6cf5cdf"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "x_dev_bpe = x_dev_bpe_feature_matrix\n",
    "bpe_dev_pred  = nb_bpe.predict(x_dev_bpe)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8006238d110c4a22"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
