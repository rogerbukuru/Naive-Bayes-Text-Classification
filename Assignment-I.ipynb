{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Installations, Imports and Downloads\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e57af82a0338e60"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import re\n",
    "import numpy as np\n",
    "warnings.filterwarnings(\"ignore\", category = UserWarning)\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "FS = (8,4) # figure size\n",
    "RS = 264\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T13:05:39.795455Z",
     "start_time": "2024-08-06T13:05:38.798543Z"
    }
   },
   "id": "b77082fbc75833ba",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directiory:  /Users/rogerbukuru/Documents/UCT Masters/MSc Statistics and Data Science/NLP-CSC5035Z/NLPTutsAssignments/Assignment-I/afrisent-semeval-2023\n",
      "/Users/rogerbukuru/Documents/UCT Masters/MSc Statistics and Data Science/NLP-CSC5035Z/NLPTutsAssignments/Assignment-I/afrisent-semeval-2023\n",
      "From https://github.com/afrisenti-semeval/afrisent-semeval-2023\r\n",
      " * branch            HEAD       -> FETCH_HEAD\r\n",
      "Already up to date.\r\n"
     ]
    }
   ],
   "source": [
    "PROJECT_DIR = os.getcwd() + '/afrisent-semeval-2023'\n",
    "print('Current directiory: ', PROJECT_DIR)\n",
    "PROJECT_GITHUB_URL = 'https://github.com/afrisenti-semeval/afrisent-semeval-2023.git'\n",
    "\n",
    "if not os.path.isdir(PROJECT_DIR):\n",
    "  !git clone {PROJECT_GITHUB_URL}\n",
    "else:\n",
    "  %cd {PROJECT_DIR}\n",
    "  !git pull {PROJECT_GITHUB_URL}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T13:05:43.572767Z",
     "start_time": "2024-08-06T13:05:42.653635Z"
    }
   },
   "id": "91fdcbd58f0388f6",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Loading\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be842af4aa9e3ccd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Choose language \n",
    "language = 'swa'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T13:12:39.923474Z",
     "start_time": "2024-08-06T13:12:39.881694Z"
    }
   },
   "id": "945c286f95628713",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory:  /Users/rogerbukuru/Documents/UCT Masters/MSc Statistics and Data Science/NLP-CSC5035Z/NLPTutsAssignments/Assignment-I/afrisent-semeval-2023/data/swa\n",
      "Train shape:  (1810, 2)\n",
      "Dev shape:  (453, 2)\n",
      "Test shape:  (748, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                                   text     label\n100   Hili tatizo kubwa sana kwetumtu akijaribu kutu...  negative\n559   Wameambiwa na mwenyekiti wao kila kitu ni sawa...   neutral\n1440  Kumbe tunaifukuzia China kwa wingitushafika bi...  positive\n119   Show nyingi za kibongo Show ikiisha Shabiki un...  negative\n1473  Katika kuhakikisha tunafikia 5050 Wanahabari w...  positive\n528   Hili jambo sio dogo kama tunavyofikiriahii eli...   neutral\n1456  Hii ni Hatua Muhimu sana pongezi kwa kikao hic...  positive\n138   Kama uamini kaamuulize Agger karacai msanii wa...  negative\n605   Habari ahsante kwa kuwasiliana nasi Naomba ufa...   neutral\n276   Ujumbe huo unamaanisha haukujitoa katika hudum...   neutral",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>100</th>\n      <td>Hili tatizo kubwa sana kwetumtu akijaribu kutu...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>559</th>\n      <td>Wameambiwa na mwenyekiti wao kila kitu ni sawa...</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>1440</th>\n      <td>Kumbe tunaifukuzia China kwa wingitushafika bi...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>119</th>\n      <td>Show nyingi za kibongo Show ikiisha Shabiki un...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>1473</th>\n      <td>Katika kuhakikisha tunafikia 5050 Wanahabari w...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>528</th>\n      <td>Hili jambo sio dogo kama tunavyofikiriahii eli...</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>1456</th>\n      <td>Hii ni Hatua Muhimu sana pongezi kwa kikao hic...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>138</th>\n      <td>Kama uamini kaamuulize Agger karacai msanii wa...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>605</th>\n      <td>Habari ahsante kwa kuwasiliana nasi Naomba ufa...</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>276</th>\n      <td>Ujumbe huo unamaanisha haukujitoa katika hudum...</td>\n      <td>neutral</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "DATA_DIR = f'{PROJECT_DIR}/data/{language}'\n",
    "print('Data directory: ', DATA_DIR)\n",
    "\n",
    "train_df = pd.read_csv(f'{DATA_DIR}/train.tsv', sep='\\t', names=['text', 'label'], header=0)\n",
    "dev_df = pd.read_csv(f'{DATA_DIR}/dev.tsv', sep='\\t', names=['text', 'label'], header=0)\n",
    "test_df = pd.read_csv(f'{DATA_DIR}/test.tsv', sep='\\t', names=['text', 'label'], header=0)\n",
    "\n",
    "print('Train shape: ', train_df.shape)\n",
    "print('Dev shape: ', dev_df.shape)\n",
    "print('Test shape: ', test_df.shape)\n",
    "\n",
    "# Display data\n",
    "train_df.sample(n=10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T13:12:41.726926Z",
     "start_time": "2024-08-06T13:12:41.708003Z"
    }
   },
   "id": "b1b9cbe14aa9740f",
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Cleaning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "19067257a5279928"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Discard neutral examples\n",
    "train_df = train_df[train_df['label'] != 'neutral']\n",
    "dev_df = dev_df[dev_df['label'] != 'neutral']\n",
    "test_df = test_df[test_df['label'] != 'neutral']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T13:12:47.124533Z",
     "start_time": "2024-08-06T13:12:47.122202Z"
    }
   },
   "id": "aca2cf3c14aafd31",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    # Replace URLS with [URL]\n",
    "    text = re.sub(r'http\\S+', '[URL]', text)\n",
    "\n",
    "    # Replace numbers with [NUM]\n",
    "    text = re.sub(r'\\d+', '[NUM]', text)\n",
    "\n",
    "    # Remove trailing spaces\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "train_df['text'] = train_df['text'].apply(clean)\n",
    "dev_df['text'] = dev_df['text'].apply(clean)\n",
    "test_df['text'] = test_df['text'].apply(clean)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T13:12:48.260561Z",
     "start_time": "2024-08-06T13:12:48.253411Z"
    }
   },
   "id": "b44a453899f24356",
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Construct Vocabulary"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4534f864e77f22f8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Count number of tokens in corpus\n",
    "def count_tokens(sentences):\n",
    "    \"\"\"\n",
    "    Count number of tokens in corpus\n",
    "\n",
    "    param: sentences: list of list of tokens e.g. [['This', 'is', 'a', 'sentence'], ['This', 'is', 'another', 'sentence'], ...]\n",
    "    return:\n",
    "        count: number of tokens in corpus\n",
    "    \"\"\"\n",
    "    total_tokens = 0\n",
    "    for sentence in sentences:\n",
    "        total_tokens += len(sentence)\n",
    "    return total_tokens"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T13:06:05.526828Z",
     "start_time": "2024-08-06T13:06:05.525120Z"
    }
   },
   "id": "aa2c3ccefea2eec0",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Collect type counts in corpus\n",
    "def create_type_counts(sentences):\n",
    "    \"\"\"\n",
    "    Count number of types in corpus\n",
    "\n",
    "    param: sentences: list of list of tokens e.g. [['This', 'is', 'a', 'sentence'], ['This', 'is', 'another', 'sentence'], ...]\n",
    "    return:\n",
    "        type2count: dictionary of type counts in corpus e.g. {'This': 2, 'sentence': 2, ...}\n",
    "    \"\"\"\n",
    "    type2count = {}\n",
    "    for sentence in sentences:\n",
    "        for type_ in sentence:\n",
    "            if type_ not in type2count:\n",
    "                type2count[type_] = 1\n",
    "            else:\n",
    "                current_count = type2count[type_]\n",
    "                type2count[type_] = current_count +1\n",
    "    return type2count"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T13:06:07.519064Z",
     "start_time": "2024-08-06T13:06:07.516401Z"
    }
   },
   "id": "170232a2a3218e1e",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Create vocabulary\n",
    "def create_vocabulary(type2count, min_count):\n",
    "    \"\"\"\n",
    "    This function creates an indexed vocabulary from vocabulary counts and returns it as a list and a dictionary.\n",
    "\n",
    "    param:\n",
    "        type2count: dictionary of type counts in corpus e.g. {'This': 2, 'sentence': 2, ...}\n",
    "        min_count: minimum count of a word to be included in the vocabulary\n",
    "    return:\n",
    "        index2type: list of words in the vocabulary e.g. ['word1', 'word2', 'word3', ...]\n",
    "        type2index: dictionary mapping words to their index in the index2type vocabulary e.g. {'word1': 0, 'word2': 1, 'word3': 2, ...}\n",
    "    \"\"\"\n",
    "    index2type = []\n",
    "    type2index = {}\n",
    "    for type_, count in type2count.items():\n",
    "        if(count >= min_count):\n",
    "            index2type.append(type_)\n",
    "            type2index[type_] = len(index2type) - 1\n",
    "    return index2type, type2index"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T13:06:09.838059Z",
     "start_time": "2024-08-06T13:06:09.836787Z"
    }
   },
   "id": "a55b473b29ace5e4",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "\n",
    "# Tokenization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d3285ba6754d912"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Word Based Tokenization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26626c598cae9936"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def whitespace_tokenize(sentences):\n",
    "    return [sentence.split() for sentence in sentences]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T13:06:12.644621Z",
     "start_time": "2024-08-06T13:06:12.641146Z"
    }
   },
   "id": "64f366be8fdce023",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Byte-Pair Encoding (BPE) Tokenization\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d72e944dadb4c3f0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class BPETokenizer():\n",
    "\n",
    "    def __init__(self, sentences, vocab_size):\n",
    "        \"\"\"\n",
    "        Initialize the BPE tokenizer.\n",
    "\n",
    "        Args:\n",
    "            sentences (list[str]): list of list of tokens e.g. [['This', 'is', 'a', 'sentence'], ['This', 'is', 'another', 'sentence'], ...]\n",
    "            vocab_size (int): The desired vocabulary size after training.\n",
    "        \"\"\"\n",
    "        self.sentences = sentences\n",
    "        self.vocab_size = vocab_size\n",
    "        self.word_freqs = defaultdict(int)\n",
    "        self.splits = {}\n",
    "        self.merges = {}\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train the BPE tokenizer by iteratively merging the most frequent pairs of symbols.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary of merges in the format {(a, b): 'ab'}, where 'a' and 'b' are symbols merged into 'ab'.\n",
    "        \"\"\"\n",
    "        # Split corpus\n",
    "        for sentence in self.sentences:\n",
    "            for word in sentence:\n",
    "                self.splits[word] = [char for char in word]\n",
    "                    \n",
    "        for i in range(self.vocab_size):\n",
    "            self.compute_pair_freqs() # compute adjacent pair frequencies\n",
    "            pair, _ = list(self.word_freqs.items())[0] # most frequent pair\n",
    "            self.merge_pair(pair[0], pair[1])\n",
    "            self.merges[pair] = pair[0] + pair[1]\n",
    "        return self.merges\n",
    "\n",
    "\n",
    "    def compute_pair_freqs(self):\n",
    "        \"\"\"\n",
    "        Compute the frequency of each pair of symbols in the corpus.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary of pairs and their frequencies in the format {(a, b): frequency}.\n",
    "        \"\"\"\n",
    "        pair_freqs = defaultdict(int)\n",
    "        for _, split in self.splits.items():\n",
    "            for i in range(len(split)-1):\n",
    "                pair = (split[i], split[i+1])\n",
    "                if pair not in pair_freqs:\n",
    "                    pair_freqs[pair] = 1\n",
    "                else:\n",
    "                    pair_freqs[pair] += 1\n",
    "        self.word_freqs = pair_freqs\n",
    "        self.word_freqs = dict(sorted(self.word_freqs.items(), key=lambda x: x[1], reverse=True)) # sort from max to min count\n",
    "        return self.word_freqs\n",
    "        \n",
    "    def merge_pair(self, a, b):\n",
    "        \"\"\"\n",
    "        Merge the given pair of symbols in all words where they appear adjacent.\n",
    "\n",
    "        Args:\n",
    "            a (str): The first symbol in the pair.\n",
    "            b (str): The second symbol in the pair.\n",
    "\n",
    "        Returns:\n",
    "            dict: The updated splits dictionary after merging.\n",
    "        \"\"\"\n",
    "        pair = (a,b)\n",
    "        # Check if valid pair\n",
    "        if pair in self.word_freqs:\n",
    "            new_token = a+b\n",
    "            for word, split in self.splits.items():\n",
    "                for i in range(len(split)-1):\n",
    "                    if split[i] == a and split[i+1] == b:\n",
    "                       split[i] = new_token\n",
    "                       new_split = list(filter(lambda x: x not in [b], split))\n",
    "                       self.splits[word] = new_split\n",
    "        return self.splits\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"\n",
    "        Tokenize a given text using the trained BPE tokenizer.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to be tokenized.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: A list of tokens obtained after applying BPE tokenization.\n",
    "        \"\"\"\n",
    "\n",
    "        pre_tokenized_text = text.split()\n",
    "        splits_text = [[l for l in word] for word in pre_tokenized_text]\n",
    "\n",
    "        for pair, merge in self.merges.items():\n",
    "            for idx, split in enumerate(splits_text):\n",
    "                i = 0\n",
    "                while i < len(split) - 1:\n",
    "                    if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
    "                        split = split[:i] + [merge] + split[i + 2 :]\n",
    "                    else:\n",
    "                        i += 1\n",
    "                splits_text[idx] = split\n",
    "        result = sum(splits_text, [])\n",
    "        return result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T13:06:14.522763Z",
     "start_time": "2024-08-06T13:06:14.519378Z"
    }
   },
   "id": "185fdac1da4c7403",
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature Extraction"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "47386243702bc12f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## One-Hot Encoding Vectorization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6aadfd89ead00bf5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def one_hot_vectorize(sentences, type2index):\n",
    "    \"\"\"\n",
    "    One-hot encode a list of sentences.\n",
    "\n",
    "    param:\n",
    "        list of list of tokens e.g. [['This', 'is', 'a', 'sentence'], ['This', 'is', 'another', 'sentence'], ...]\n",
    "        type2index: dictionary mapping words to their index in the vocabulary e.g. {'word1': 0, 'word2': 1, 'word3': 2, ...}\n",
    "    return:\n",
    "        one_hot_sentences: 2d numpy array of one-hot encoded sentences e.g. [[1, 0, 0, 1, ...], [0, 1, 1, 0, ...], ...]\n",
    "    \"\"\"\n",
    "    one_hot_encoding = np.array(np.zeros((len(type2index.items()), len(type2index.items()))))\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            word_index = type2index[word]\n",
    "            one_hot_encoding[word_index, word_index] = 1\n",
    "    return one_hot_encoding\n",
    "\n",
    "def simple_binary_vectorization(sentences, type2index):\n",
    "    \"\"\"\n",
    "    Binary text-vectorization of a list of sentences.\n",
    "\n",
    "    param:\n",
    "        list of list of tokens e.g. [['This', 'is', 'a', 'sentence'], ['This', 'is', 'another', 'sentence'], ...]\n",
    "        type2index: dictionary mapping words to their index in the vocabulary e.g. {'word1': 0, 'word2': 1, 'word3': 2, ...}\n",
    "    return:\n",
    "        one_hot_sentences: 2d numpy array of one-hot encoded sentences e.g. [[1, 0, 0, 1, ...], [0, 1, 1, 0, ...], ...]\n",
    "    \"\"\"\n",
    "    vectors = []\n",
    "    for sentence in sentences:\n",
    "        vector = np.zeros(len(type2index.items()))\n",
    "        for word in sentence:\n",
    "             word_index = type2index[word]\n",
    "             vector[word_index] = 1\n",
    "        vectors.append(np.array(vector))\n",
    "    return np.array(vectors)\n",
    "\n",
    "def bow_vectorization(sentences, type2index):\n",
    "    vectors = []\n",
    "    for sentence in sentences:\n",
    "        vector = np.zeros(len(type2index.items()))\n",
    "        for word in sentence:\n",
    "             word_index = type2index[word]\n",
    "             vector[word_index] += 1\n",
    "        vectors.append(np.array(vector))\n",
    "    return np.array(vectors)\n",
    "     "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T20:20:29.431520Z",
     "start_time": "2024-08-06T20:20:29.429692Z"
    }
   },
   "id": "7298bb785e0a0987",
   "execution_count": 84
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Word-Based Tokenization Features\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37333bc85957788f"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in corpus:  13122\n",
      "Number of tokens in corpus:  3064\n",
      "Number of tokens in corpus:  5401\n"
     ]
    }
   ],
   "source": [
    "# Store training data text as list of tweets\n",
    "train_corpus = train_df['text'].tolist()\n",
    "tokenized_train_corpus = whitespace_tokenize(train_corpus)\n",
    "num_tokens = count_tokens(tokenized_train_corpus)\n",
    "print('Number of tokens in corpus: ', num_tokens)\n",
    "type2count = create_type_counts(tokenized_train_corpus)\n",
    "# Sort types by counts\n",
    "type2count = dict(sorted(type2count.items(), key=lambda x: x[1], reverse=True))\n",
    "index2type, type2index = create_vocabulary(type2count, min_count=1)\n",
    "# It's good practice to add a special token for unknown words and padding (to make all sentences in training batches the same length)\n",
    "type2index['<UNK>'] = len(index2type)\n",
    "index2type.append('<UNK>')\n",
    "type2index['<PAD>'] = len(index2type)\n",
    "index2type.append('<PAD>')\n",
    "x_train_word_based_feature_matrix = bow_vectorization(tokenized_train_corpus, type2index)\n",
    "\n",
    "dev_corpus = dev_df['text'].tolist()\n",
    "tokenized_dev_corpus = whitespace_tokenize(dev_corpus)\n",
    "num_tokens = count_tokens(tokenized_dev_corpus)\n",
    "print('Number of tokens in corpus: ', num_tokens)\n",
    "type2count_dev = create_type_counts(tokenized_dev_corpus)\n",
    "type2count_dev = dict(sorted(type2count_dev.items(), key=lambda x: x[1], reverse=True))\n",
    "index2type_dev, type2index_dev = create_vocabulary(type2count_dev, min_count=1)\n",
    "x_dev_word_based_feature_matrix = bow_vectorization(tokenized_dev_corpus, type2index_dev)\n",
    "\n",
    "\n",
    "test_corpus = test_df['text'].tolist()\n",
    "tokenized_test_corpus = whitespace_tokenize(test_corpus)\n",
    "num_tokens = count_tokens(tokenized_test_corpus)\n",
    "print('Number of tokens in corpus: ', num_tokens)\n",
    "type2count_test = create_type_counts(tokenized_test_corpus)\n",
    "type2count_test = dict(sorted(type2count_test.items(), key=lambda x: x[1], reverse=True))\n",
    "index2type_test, type2index_test = create_vocabulary(type2count_test, min_count=1)\n",
    "x_test_word_based_feature_matrix = bow_vectorization(tokenized_test_corpus, type2index_test)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T20:21:45.348193Z",
     "start_time": "2024-08-06T20:21:45.311229Z"
    }
   },
   "id": "c5f9f9e91d866b89",
   "execution_count": 85
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BPE Tokenization Features"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1183ca42d8e9f287"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Train BPE\n",
    "bpe = BPETokenizer(tokenized_train_corpus, vocab_size=1000)\n",
    "merges = bpe.train()\n",
    "#print(\"Merges\", merges)\n",
    "# Apply to our dataset\n",
    "train_df['bpe_text'] = train_df['text'].apply(lambda x: ' '.join(bpe.tokenize(x)))\n",
    "dev_df['bpe_text'] = dev_df['text'].apply(lambda x: ' '.join(bpe.tokenize(x)))\n",
    "test_df['bpe_text'] = test_df['text'].apply(lambda x: ' '.join(bpe.tokenize(x)))\n",
    "train_df.head()\n",
    "\n",
    "bpe_train_corpus = train_df['bpe_text'].tolist()\n",
    "tokenized_bpe_train_corpus = whitespace_tokenize(bpe_train_corpus)\n",
    "bpe_type2count = create_type_counts(tokenized_bpe_train_corpus)\n",
    "# Sort types by counts\n",
    "bpe_type2count = dict(sorted(bpe_type2count.items(), key=lambda x: x[1], reverse=True))\n",
    "bpe_index2type, bpe_type2index = create_vocabulary(bpe_type2count, min_count=1)\n",
    "x_train_bpe_feature_matrix = simple_binary_vectorization(tokenized_bpe_train_corpus, bpe_type2index)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T20:01:47.855105Z",
     "start_time": "2024-08-06T20:01:39.157303Z"
    }
   },
   "id": "8e66e51edab48fb3",
   "execution_count": 79
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Training\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "995a5523bec920cd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Compute Probabilities\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9bf18ba82f50e6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier():\n",
    "    \n",
    "    def __init__(self, x_train, y_train, type2index):\n",
    "        self.class_priors = {}\n",
    "        self.likelihoods = {}\n",
    "        self.word_counts = type2index\n",
    "        self.vocabulary = set()\n",
    "        self.y = y_train\n",
    "        self.x = x_train\n",
    "    def train(self):\n",
    "        # Calculate class prior probabilities\n",
    "        print(\"Training Naive Bayes Classifier\")\n",
    "        self.prior_probs()\n",
    "        print(self.class_priors)\n",
    "        #for label in set(y_train)          \n",
    "    \n",
    "    def prior_probs(self):   \n",
    "        positive_sentences = 0\n",
    "        for label in self.y:\n",
    "            if label == \"positive\":\n",
    "                positive_sentences += 1\n",
    "        self.class_priors[\"positive\"] = positive_sentences/len(self.y)\n",
    "        self.class_priors[\"negative\"] = 1 - self.class_priors[\"positive\"]\n",
    "    def likelihood(self):\n",
    "        print(\"test\")\n",
    "    def predict(self):\n",
    "        print(\"Predicting Naive Bayes\")\n",
    "    \n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T17:51:17.095905Z",
     "start_time": "2024-08-06T17:51:17.090561Z"
    }
   },
   "id": "30bae654ab9c2954",
   "execution_count": 47
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Compute Likelihood"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f69299d7fe8ea867"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0., 1., 0., ..., 0., 0., 0.],\n       [3., 1., 1., ..., 0., 0., 0.],\n       [1., 0., 1., ..., 0., 0., 0.],\n       ...,\n       [1., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 1., 0., ..., 1., 0., 0.]])"
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "#x_train_word_based_feature_matrix = jnp.array(x_train_word_based_feature_matrix, dtype=jnp.float16)\n",
    "#x_train_bpe_feature_matrix = jnp.array(x_train_bpe_feature_matrix, dtype=jnp.float16)#\n",
    "y_train = train_df[\"label\"]\n",
    "y_train = jnp.array(y_train.map({\"positive\": 1, \"negative\": 0}), dtype=jnp.float16)\n",
    "\n",
    "x_train_word_based_feature_matrix\n",
    "\n",
    "\n",
    "#nb = NaiveBayesClassifier(labels,type2index)\n",
    "\n",
    "#nb = MultinomialNB()\n",
    "#nb.fit(x_train_word_based_feature_matrix, y_train)\n",
    "#y_pred = nb.predict(x_dev_word_based_feature_matrix)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T20:22:14.022819Z",
     "start_time": "2024-08-06T20:22:14.017882Z"
    }
   },
   "id": "14175695b44455dd",
   "execution_count": 88
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
