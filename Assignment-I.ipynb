{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CSC5035Z Natural Language Processing\n",
    "# Assignment 1: Naïve Bayes for Text Classification"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e57af82a0338e60"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Author: Roger Bukuru**\n",
    "\n",
    "## How to guide\n",
    "- First Ensure the following packages are installed on your system\n",
    "  - **numpy** \n",
    "    - installation command: **pip install numpy**\n",
    "  - **pandas** \n",
    "    - installation command: **pip install pandas**\n",
    "  - **sklearn** \n",
    "    - installation command: **pip install sklearn**\n",
    "- Then sequentially run each code block, the code blocks perform the following tasks:\n",
    "   - Downloads the Afrisenti dataset\n",
    "   - Selects the language of choice (Kinyarwanda)\n",
    "   - Cleans the data\n",
    "   - Creates a vocabulary\n",
    "   - Applies word-based and BPE tokenization\n",
    "   - Implements a simple binary text vectorization\n",
    "   - Implements the Naïve Bayes classifier\n",
    "   - Trains classifier based on word-based and BPE features obtained from simple binary text vectorization\n",
    "   - Predict on both word-based and BPE features\n",
    "   - Evaluate model on a development data set\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5022761ebfccfa0b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Installations, Imports and Downloads\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd9f322a3cc17080"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import re\n",
    "import numpy as np\n",
    "warnings.filterwarnings(\"ignore\", category = UserWarning)\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "FS = (8,4) # figure size\n",
    "RS = 264\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-08T14:14:48.685403Z",
     "start_time": "2024-08-08T14:14:48.677585Z"
    }
   },
   "id": "b77082fbc75833ba",
   "execution_count": 273
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directiory:  /Users/rogerbukuru/Documents/UCT Masters/MSc Statistics and Data Science/NLP-CSC5035Z/NLPTutsAssignments/Assignment-I/afrisent-semeval-2023/afrisent-semeval-2023/afrisent-semeval-2023\n",
      "/Users/rogerbukuru/Documents/UCT Masters/MSc Statistics and Data Science/NLP-CSC5035Z/NLPTutsAssignments/Assignment-I/afrisent-semeval-2023/afrisent-semeval-2023/afrisent-semeval-2023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/pty.py:85: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From https://github.com/afrisenti-semeval/afrisent-semeval-2023\r\n",
      " * branch            HEAD       -> FETCH_HEAD\r\n",
      "Already up to date.\r\n"
     ]
    }
   ],
   "source": [
    "PROJECT_DIR = os.getcwd() + '/afrisent-semeval-2023'\n",
    "print('Current directiory: ', PROJECT_DIR)\n",
    "PROJECT_GITHUB_URL = 'https://github.com/afrisenti-semeval/afrisent-semeval-2023.git'\n",
    "\n",
    "if not os.path.isdir(PROJECT_DIR):\n",
    "  !git clone {PROJECT_GITHUB_URL}\n",
    "else:\n",
    "  %cd {PROJECT_DIR}\n",
    "  !git pull {PROJECT_GITHUB_URL}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-08T14:14:51.015067Z",
     "start_time": "2024-08-08T14:14:49.722821Z"
    }
   },
   "id": "91fdcbd58f0388f6",
   "execution_count": 274
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Loading\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be842af4aa9e3ccd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Choose language \n",
    "language = 'kin'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-08T14:14:52.979502Z",
     "start_time": "2024-08-08T14:14:52.972881Z"
    }
   },
   "id": "945c286f95628713",
   "execution_count": 275
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory:  /Users/rogerbukuru/Documents/UCT Masters/MSc Statistics and Data Science/NLP-CSC5035Z/NLPTutsAssignments/Assignment-I/afrisent-semeval-2023/afrisent-semeval-2023/afrisent-semeval-2023/data/kin\n",
      "Train shape:  (3302, 2)\n",
      "Dev shape:  (827, 2)\n",
      "Test shape:  (1026, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                                   text     label\n2389                       @user Eka genda! Kubera iki?   neutral\n2617  @user Narumvise ngo abariye bene zino nyama zi...  positive\n1965  Ese koko ubukene buvugwa mu bitangazamakuru by...   neutral\n1112            @user uh! N'umusazi ko atinya nyina ra!  negative\n601   @user Izo ziba Ari likes za shitani n'abambari...  negative\n...                                                 ...       ...\n2900  @user @user Ifite ukuntu intebe zayo zimeze nk...  positive\n1964  @user @user Na hano ni inzira yemewe cyane yo ...   neutral\n2423  @user @user @user Ark gutekereza kugera kuri s...  positive\n2995  Inama yashojwe n'ubusabane twishimira ibyagezw...  positive\n990   @user @user @user @user @user @user @user None...  negative\n\n[100 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2389</th>\n      <td>@user Eka genda! Kubera iki?</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>2617</th>\n      <td>@user Narumvise ngo abariye bene zino nyama zi...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>1965</th>\n      <td>Ese koko ubukene buvugwa mu bitangazamakuru by...</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>1112</th>\n      <td>@user uh! N'umusazi ko atinya nyina ra!</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>601</th>\n      <td>@user Izo ziba Ari likes za shitani n'abambari...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2900</th>\n      <td>@user @user Ifite ukuntu intebe zayo zimeze nk...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>1964</th>\n      <td>@user @user Na hano ni inzira yemewe cyane yo ...</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>2423</th>\n      <td>@user @user @user Ark gutekereza kugera kuri s...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>2995</th>\n      <td>Inama yashojwe n'ubusabane twishimira ibyagezw...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>990</th>\n      <td>@user @user @user @user @user @user @user None...</td>\n      <td>negative</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "DATA_DIR = f'{PROJECT_DIR}/data/{language}'\n",
    "print('Data directory: ', DATA_DIR)\n",
    "\n",
    "train_df = pd.read_csv(f'{DATA_DIR}/train.tsv', sep='\\t', names=['text', 'label'], header=0)\n",
    "dev_df = pd.read_csv(f'{DATA_DIR}/dev.tsv', sep='\\t', names=['text', 'label'], header=0)\n",
    "test_df = pd.read_csv(f'{DATA_DIR}/test.tsv', sep='\\t', names=['text', 'label'], header=0)\n",
    "\n",
    "print('Train shape: ', train_df.shape)\n",
    "print('Dev shape: ', dev_df.shape)\n",
    "print('Test shape: ', test_df.shape)\n",
    "\n",
    "# Display data\n",
    "train_df.sample(n=100)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-08T14:14:54.039101Z",
     "start_time": "2024-08-08T14:14:54.019025Z"
    }
   },
   "id": "b1b9cbe14aa9740f",
   "execution_count": 276
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Cleaning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "19067257a5279928"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Discard neutral examples\n",
    "train_df = train_df[train_df['label'] != 'neutral']\n",
    "dev_df = dev_df[dev_df['label'] != 'neutral']\n",
    "test_df = test_df[test_df['label'] != 'neutral']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-08T14:14:57.304892Z",
     "start_time": "2024-08-08T14:14:57.301812Z"
    }
   },
   "id": "aca2cf3c14aafd31",
   "execution_count": 277
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    # Replace URLS with [URL]\n",
    "    text = re.sub(r'http\\S+', '[URL]', text)\n",
    "\n",
    "    # Replace numbers with [NUM]\n",
    "    text = re.sub(r'\\d+', '[NUM]', text)\n",
    "    \n",
    "    # Replace @user with \"\"\n",
    "    text = re.sub(r'@user\\b', '[USR]', text)\n",
    "\n",
    "    # Remove trailing spaces\n",
    "    \n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "train_df['text'] = train_df['text'].apply(clean)\n",
    "dev_df['text'] = dev_df['text'].apply(clean)\n",
    "test_df['text'] = test_df['text'].apply(clean)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-08T14:15:00.457740Z",
     "start_time": "2024-08-08T14:15:00.451098Z"
    }
   },
   "id": "b44a453899f24356",
   "execution_count": 278
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Construct Vocabulary"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4534f864e77f22f8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Count number of tokens in corpus\n",
    "def count_tokens(sentences):\n",
    "    \"\"\"\n",
    "    Count number of tokens in corpus\n",
    "\n",
    "    param: sentences: list of list of tokens e.g. [['This', 'is', 'a', 'sentence'], ['This', 'is', 'another', 'sentence'], ...]\n",
    "    return:\n",
    "        count: number of tokens in corpus\n",
    "    \"\"\"\n",
    "    total_tokens = 0\n",
    "    for sentence in sentences:\n",
    "        total_tokens += len(sentence)\n",
    "    return total_tokens"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-08T14:15:03.730514Z",
     "start_time": "2024-08-08T14:15:03.726415Z"
    }
   },
   "id": "aa2c3ccefea2eec0",
   "execution_count": 279
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Collect type counts in corpus\n",
    "def create_type_counts(sentences):\n",
    "    \"\"\"\n",
    "    Count number of types in corpus\n",
    "\n",
    "    param: sentences: list of list of tokens e.g. [['This', 'is', 'a', 'sentence'], ['This', 'is', 'another', 'sentence'], ...]\n",
    "    return:\n",
    "        type2count: dictionary of type counts in corpus e.g. {'This': 2, 'sentence': 2, ...}\n",
    "    \"\"\"\n",
    "    type2count = {}\n",
    "    for sentence in sentences:\n",
    "        for type_ in sentence:\n",
    "            if type_ not in type2count:\n",
    "                type2count[type_] = 1\n",
    "            else:\n",
    "                current_count = type2count[type_]\n",
    "                type2count[type_] = current_count +1\n",
    "    return type2count"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-08T14:15:05.905633Z",
     "start_time": "2024-08-08T14:15:05.900141Z"
    }
   },
   "id": "170232a2a3218e1e",
   "execution_count": 280
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Create vocabulary\n",
    "def create_vocabulary(type2count, min_count):\n",
    "    \"\"\"\n",
    "    This function creates an indexed vocabulary from vocabulary counts and returns it as a list and a dictionary.\n",
    "\n",
    "    param:\n",
    "        type2count: dictionary of type counts in corpus e.g. {'This': 2, 'sentence': 2, ...}\n",
    "        min_count: minimum count of a word to be included in the vocabulary\n",
    "    return:\n",
    "        index2type: list of words in the vocabulary e.g. ['word1', 'word2', 'word3', ...]\n",
    "        type2index: dictionary mapping words to their index in the index2type vocabulary e.g. {'word1': 0, 'word2': 1, 'word3': 2, ...}\n",
    "    \"\"\"\n",
    "    index2type = []\n",
    "    type2index = {}\n",
    "    for type_, count in type2count.items():\n",
    "        if(count >= min_count):\n",
    "            index2type.append(type_)\n",
    "            type2index[type_] = len(index2type) - 1\n",
    "    return index2type, type2index"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-08T14:15:07.493622Z",
     "start_time": "2024-08-08T14:15:07.488690Z"
    }
   },
   "id": "a55b473b29ace5e4",
   "execution_count": 281
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "\n",
    "# Tokenization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d3285ba6754d912"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Word Based Tokenization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26626c598cae9936"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def whitespace_tokenize(sentences):\n",
    "    return [sentence.split() for sentence in sentences]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-08T14:15:10.348985Z",
     "start_time": "2024-08-08T14:15:10.340469Z"
    }
   },
   "id": "64f366be8fdce023",
   "execution_count": 282
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Byte-Pair Encoding (BPE) Tokenization\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d72e944dadb4c3f0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class BPETokenizer():\n",
    "\n",
    "    def __init__(self, sentences, vocab_size):\n",
    "        \"\"\"\n",
    "        Initialize the BPE tokenizer.\n",
    "\n",
    "        param:\n",
    "            sentences (list[str]): list of list of tokens e.g. [['This', 'is', 'a', 'sentence'], ['This', 'is', 'another', 'sentence'], ...]\n",
    "            vocab_size (int): The desired vocabulary size after training.\n",
    "        \"\"\"\n",
    "        self.sentences = sentences\n",
    "        self.vocab_size = vocab_size\n",
    "        self.word_freqs = defaultdict(int)\n",
    "        self.splits = {}\n",
    "        self.merges = {}\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train the BPE tokenizer by iteratively merging the most frequent pairs of symbols.\n",
    "\n",
    "        return:\n",
    "            dict: A dictionary of merges in the format {(a, b): 'ab'}, where 'a' and 'b' are symbols merged into 'ab'.\n",
    "        \"\"\"\n",
    "        # Split corpus\n",
    "        for sentence in self.sentences:\n",
    "            for word in sentence:\n",
    "                self.splits[word] = [char for char in word]\n",
    "                    \n",
    "        for i in range(self.vocab_size):\n",
    "            self.compute_pair_freqs() # compute adjacent pair frequencies\n",
    "            pair, _ = list(self.word_freqs.items())[0] # most frequent pair\n",
    "            self.merge_pair(pair[0], pair[1])\n",
    "            self.merges[pair] = pair[0] + pair[1]\n",
    "        return self.merges\n",
    "\n",
    "\n",
    "    def compute_pair_freqs(self):\n",
    "        \"\"\"\n",
    "        Compute the frequency of each pair of symbols in the corpus.\n",
    "\n",
    "        return:\n",
    "            dict: A dictionary of pairs and their frequencies in the format {(a, b): frequency}.\n",
    "        \"\"\"\n",
    "        pair_freqs = defaultdict(int)\n",
    "        for _, split in self.splits.items():\n",
    "            for i in range(len(split)-1):\n",
    "                pair = (split[i], split[i+1])\n",
    "                if pair not in pair_freqs:\n",
    "                    pair_freqs[pair] = 1\n",
    "                else:\n",
    "                    pair_freqs[pair] += 1\n",
    "        self.word_freqs = pair_freqs\n",
    "        self.word_freqs = dict(sorted(self.word_freqs.items(), key=lambda x: x[1], reverse=True)) # sort from max to min count\n",
    "        return self.word_freqs\n",
    "        \n",
    "    def merge_pair(self, a, b):\n",
    "        \"\"\"\n",
    "        Merge the given pair of symbols in all words where they appear adjacent.\n",
    "\n",
    "        param:\n",
    "            a (str): The first symbol in the pair.\n",
    "            b (str): The second symbol in the pair.\n",
    "\n",
    "        return:\n",
    "            dict: The updated splits dictionary after merging.\n",
    "        \"\"\"\n",
    "        pair = (a,b)\n",
    "        # Check if valid pair\n",
    "        if pair in self.word_freqs:\n",
    "            new_token = a+b\n",
    "            for word, split in self.splits.items():\n",
    "                for i in range(len(split)-1):\n",
    "                    if split[i] == a and split[i+1] == b:\n",
    "                       split[i] = new_token\n",
    "                       new_split = list(filter(lambda x: x not in [b], split))\n",
    "                       self.splits[word] = new_split\n",
    "        return self.splits\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"\n",
    "        Tokenize a given text using the trained BPE tokenizer.\n",
    "\n",
    "        param:\n",
    "            text (str): The text to be tokenized.\n",
    "\n",
    "        return:\n",
    "            list[str]: A list of tokens obtained after applying BPE tokenization.\n",
    "        \"\"\"\n",
    "\n",
    "        pre_tokenized_text = text.split()\n",
    "        splits_text = [[l for l in word] for word in pre_tokenized_text]\n",
    "\n",
    "        for pair, merge in self.merges.items():\n",
    "            for idx, split in enumerate(splits_text):\n",
    "                i = 0\n",
    "                while i < len(split) - 1:\n",
    "                    if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
    "                        split = split[:i] + [merge] + split[i + 2 :]\n",
    "                    else:\n",
    "                        i += 1\n",
    "                splits_text[idx] = split\n",
    "        result = sum(splits_text, [])\n",
    "        return result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-08T14:15:19.422350Z",
     "start_time": "2024-08-08T14:15:19.415966Z"
    }
   },
   "id": "185fdac1da4c7403",
   "execution_count": 284
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature Extraction"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "47386243702bc12f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Simple Binary Text Vectorization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6aadfd89ead00bf5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def simple_binary_vectorization(sentences, type2index):\n",
    "    \"\"\"\n",
    "    Binary text-vectorization of a list of sentences.\n",
    "\n",
    "    param: \n",
    "     sentences:  list of list of tokens e.g. [['This', 'is', 'a', 'sentence'], ['This', 'is', 'another', 'sentence'], ...]\n",
    "     type2index: dictionary mapping words to their index in the vocabulary e.g. {'word1': 0, 'word2': 1, 'word3': 2, ...}\n",
    "    return: 2D NumPy array of a simple-binary encoded sentences.\n",
    "    \"\"\"\n",
    "    vectors = []\n",
    "    unk_index = type2index['<UNK>']\n",
    "    for sentence in sentences:\n",
    "        vector = np.zeros(len(type2index.items()))\n",
    "        for word in sentence:\n",
    "            if word in type2index: \n",
    "             word_index = type2index[word]\n",
    "             vector[word_index] = 1\n",
    "            else: # If word does not exist we use the index of the UNK token.\n",
    "                vector[unk_index] = 1\n",
    "        vectors.append(np.array(vector))\n",
    "    return np.array(vectors)\n",
    "\n",
    "     "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-08T14:15:23.885299Z",
     "start_time": "2024-08-08T14:15:23.879011Z"
    }
   },
   "id": "7298bb785e0a0987",
   "execution_count": 285
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Word-Based Tokenization Feature Extraction\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37333bc85957788f"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in corpus:  30894\n",
      "Vocab size: 13674\n"
     ]
    }
   ],
   "source": [
    "# Store training data text as list of tweets\n",
    "train_corpus = train_df['text'].tolist()\n",
    "tokenized_train_corpus = whitespace_tokenize(train_corpus)\n",
    "num_tokens = count_tokens(tokenized_train_corpus)\n",
    "print('Number of tokens in corpus: ', num_tokens)\n",
    "type2count = create_type_counts(tokenized_train_corpus)\n",
    "\n",
    "# Sort types by counts\n",
    "type2count = dict(sorted(type2count.items(), key=lambda x: x[1], reverse=True))\n",
    "index2type, type2index = create_vocabulary(type2count, min_count=1)\n",
    "print(\"Vocab size:\", len(index2type))\n",
    "\n",
    "# We add a special token for unknown words and padding (to make all sentences in the training batch the same length)\n",
    "type2index['<UNK>'] = len(index2type)\n",
    "index2type.append('<UNK>')\n",
    "type2index['<PAD>'] = len(index2type)\n",
    "index2type.append('<PAD>')\n",
    "x_train_word_based_feature_matrix = simple_binary_vectorization(tokenized_train_corpus, type2index)\n",
    "\n",
    "dev_corpus = dev_df['text'].tolist()\n",
    "tokenized_dev_corpus = whitespace_tokenize(dev_corpus)\n",
    "x_dev_word_based_feature_matrix = simple_binary_vectorization(tokenized_dev_corpus, type2index)\n",
    "\n",
    "test_corpus = test_df['text'].tolist()\n",
    "tokenized_test_corpus = whitespace_tokenize(test_corpus)\n",
    "x_test_word_based_feature_matrix = simple_binary_vectorization(tokenized_test_corpus, type2index)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-08T14:15:28.624344Z",
     "start_time": "2024-08-08T14:15:28.418948Z"
    }
   },
   "id": "c5f9f9e91d866b89",
   "execution_count": 286
  },
  {
   "cell_type": "markdown",
   "source": [
    "### BPE Tokenization Feature Extraction"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1183ca42d8e9f287"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in corpus:  80362\n",
      "Vocab size: 1171\n"
     ]
    }
   ],
   "source": [
    "# Train BPE\n",
    "bpe = BPETokenizer(tokenized_train_corpus, vocab_size=1000 )\n",
    "merges = bpe.train()\n",
    "\n",
    "\n",
    "# Apply to our dataset\n",
    "train_df['bpe_text'] = train_df['text'].apply(lambda x: ' '.join(bpe.tokenize(x)))\n",
    "dev_df['bpe_text'] = dev_df['text'].apply(lambda x: ' '.join(bpe.tokenize(x)))\n",
    "test_df['bpe_text'] = test_df['text'].apply(lambda x: ' '.join(bpe.tokenize(x)))\n",
    "train_df.head()\n",
    "\n",
    "bpe_train_corpus = train_df['bpe_text'].tolist()\n",
    "tokenized_bpe_train_corpus = whitespace_tokenize(bpe_train_corpus)\n",
    "num_tokens = count_tokens(tokenized_bpe_train_corpus)\n",
    "print('Number of tokens in corpus: ', num_tokens)\n",
    "bpe_type2count = create_type_counts(tokenized_bpe_train_corpus)\n",
    "\n",
    "# Sort types by counts\n",
    "bpe_type2count = dict(sorted(bpe_type2count.items(), key=lambda x: x[1], reverse=True))\n",
    "bpe_index2type, bpe_type2index = create_vocabulary(bpe_type2count, min_count=1)\n",
    "print(\"Vocab size:\", len(bpe_index2type))\n",
    "\n",
    "bpe_type2index['<UNK>'] = len(bpe_index2type)\n",
    "bpe_index2type.append('<UNK>')\n",
    "bpe_type2index['<PAD>'] = len(bpe_index2type)\n",
    "bpe_index2type.append('<PAD>')\n",
    "\n",
    "x_train_bpe_feature_matrix = simple_binary_vectorization(tokenized_bpe_train_corpus, bpe_type2index)\n",
    "\n",
    "bpe_dev_corpus = dev_df['bpe_text'].tolist()\n",
    "tokenized_bpe_dev_corpus = whitespace_tokenize(bpe_dev_corpus)\n",
    "x_dev_bpe_feature_matrix = simple_binary_vectorization(tokenized_bpe_dev_corpus, bpe_type2index)\n",
    "\n",
    "bpe_test_corpus = test_df['bpe_text'].tolist()\n",
    "tokenized_bpe_test_corpus = whitespace_tokenize(bpe_test_corpus)\n",
    "x_test_bpe_feature_matrix = simple_binary_vectorization(bpe_test_corpus, bpe_type2index)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-08T14:22:19.132187Z",
     "start_time": "2024-08-08T14:21:56.402432Z"
    }
   },
   "id": "8e66e51edab48fb3",
   "execution_count": 297
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Training\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "995a5523bec920cd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier():\n",
    "    \n",
    "    def __init__(self, x_train, y_train, type2index):\n",
    "        self.class_priors = {}\n",
    "        self.likelihoods_probs = {}\n",
    "        self.word_counts = {}\n",
    "        self.vocabulary = type2index\n",
    "        self.Y_train = y_train\n",
    "        self.X_train = x_train\n",
    "        self.vocab_size = len(type2index.items())\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "         Trains the Naive Bayes model by computing class priors and likelihood probabilities.\n",
    "        \"\"\"\n",
    "        # Calculate class prior probabilities\n",
    "        self.compute_prior_probs()\n",
    "        # Calculate likelihood probabilities\n",
    "        self.compute_likelihood_probs() \n",
    "    \n",
    "    def compute_prior_probs(self):\n",
    "        \"\"\"\n",
    "        Calculates the prior probability for each class in the training data.\n",
    "        \"\"\"\n",
    "        positive_sentences = 0\n",
    "        for label in self.Y_train:\n",
    "            if label == 1:\n",
    "                positive_sentences += 1\n",
    "        self.class_priors[1] = positive_sentences/len(self.Y_train)\n",
    "        self.class_priors[0] = 1 - self.class_priors[1]\n",
    "        \n",
    "    def compute_likelihood_probs(self):\n",
    "        \"\"\"\n",
    "        Calculates the likelihood probabilities for each word in the vocabulary, given each class, using Laplace smoothing.\n",
    "        \"\"\"\n",
    "        self.word_counts = {0: np.zeros(self.vocab_size), 1: np.zeros(self.vocab_size)}\n",
    "        for sentence_vector, label  in zip(self.X_train, self.Y_train):\n",
    "            self.word_counts[label] += sentence_vector\n",
    "        for class_ in self.word_counts:\n",
    "            # Compute based on Laplace Transform\n",
    "            self.likelihoods_probs[class_] = (self.word_counts[class_] + 1)/ (sum(self.word_counts[class_]) + self.vocab_size)\n",
    "            \n",
    "         \n",
    "    def predict(self,x):\n",
    "        \"\"\"\n",
    "        Predicts the class labels for a set of input documents.\n",
    "\n",
    "        param:\n",
    "            x: A list or array of document feature vectors (binary vectors representing word occurrences).\n",
    "\n",
    "        return:\n",
    "            A list of predicted class labels (0 or 1).\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        for sentence_vector in x: # text vectorization of sentence\n",
    "            posteriors = {}\n",
    "            for class_  in self.class_priors:\n",
    "                log_posterior = np.log(self.class_priors[class_])\n",
    "                for i in range(len(sentence_vector)):\n",
    "                    if sentence_vector[i] == 1:\n",
    "                        log_posterior += np.log(self.likelihoods_probs[class_][i])\n",
    "                posteriors[class_] = log_posterior\n",
    "            predicted_class = max(posteriors, key=posteriors.get)\n",
    "            predictions.append(predicted_class)\n",
    "        return predictions\n",
    "    \n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-08T14:20:57.766450Z",
     "start_time": "2024-08-08T14:20:57.762750Z"
    }
   },
   "id": "30bae654ab9c2954",
   "execution_count": 289
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Word-Based Tokenization: Model Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f69299d7fe8ea867"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "y_train = train_df[\"label\"]\n",
    "y_train = np.array(y_train.map({\"positive\": 1, \"negative\": 0}))\n",
    "x_train = x_train_word_based_feature_matrix\n",
    "\n",
    "# Train Model\n",
    "nb_word_based_tokenization = NaiveBayesClassifier(x_train,y_train, type2index)\n",
    "nb_word_based_tokenization.train()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-08T14:26:52.209555Z",
     "start_time": "2024-08-08T14:26:52.120901Z"
    }
   },
   "id": "14175695b44455dd",
   "execution_count": 298
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BPE Tokenization: Model Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91a306fc1d6b617a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "x_train_bpe = x_train_bpe_feature_matrix\n",
    "\n",
    "nb_bpe = NaiveBayesClassifier(x_train_bpe,y_train, bpe_type2index)\n",
    "nb_bpe.train()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-08T14:26:54.518886Z",
     "start_time": "2024-08-08T14:26:54.505861Z"
    }
   },
   "id": "142252802807dd47",
   "execution_count": 299
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prediction"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0717f51cef50855"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def prediction_accuracy(predictions, y_test):\n",
    "    \"\"\"\n",
    "    Calculates the accuracy of a set of predictions against the true labels.\n",
    "    \n",
    "    param:\n",
    "        predictions: A list or array containing predicted class labels (e.g., 0 or 1).\n",
    "        y_test: A list or array containing the true class labels.\n",
    "\n",
    "    return:\n",
    "        float: The accuracy of the predictions, expressed as a percentage (0 to 100).\n",
    "    \"\"\"\n",
    "    correct_predictions = 0\n",
    "    for i in range(len(predictions)):\n",
    "        if wb_predictions[i] == y_test[i]:\n",
    "            correct_predictions += 1\n",
    "    prediction_acc = correct_predictions/len(y_test)\n",
    "    return prediction_acc*100"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-08T14:26:56.478308Z",
     "start_time": "2024-08-08T14:26:56.473269Z"
    }
   },
   "id": "12f899d5651c6633",
   "execution_count": 300
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b531cc6f4778b9d1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Word-Based Tokenization Prediction"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "66d984fc70c7c3fb"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "80.09478672985783"
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = test_df[\"label\"]\n",
    "y_test = np.array(y_test.map({\"positive\": 1, \"negative\": 0}))\n",
    "y_test = y_test.tolist()\n",
    "x_test = x_test_word_based_feature_matrix\n",
    "wb_predictions = nb_word_based_tokenization.predict(x_test)\n",
    "prediction_accuracy(wb_predictions, y_test)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-08T14:21:18.562900Z",
     "start_time": "2024-08-08T14:21:17.559371Z"
    }
   },
   "id": "8046de9601b5fbc7",
   "execution_count": 293
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BPE Tokenization Prediction"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3482db2f8514066c"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "80.09478672985783"
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict\n",
    "x_test_bpe = x_test_bpe_feature_matrix\n",
    "bpe_test_predictions = nb_bpe.predict(x_test_bpe)\n",
    "prediction_accuracy(bpe_test_predictions, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-08T14:21:21.607602Z",
     "start_time": "2024-08-08T14:21:21.172088Z"
    }
   },
   "id": "9fb2b25d83589242",
   "execution_count": 294
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Evaluation\n",
    "\n",
    "We evaluate the performance of our model on a development set."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a86f647735e60a2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Word-Based Tokenization Model Evaluation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "949907513148f963"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.81      0.80       287\n",
      "           1       0.75      0.72      0.73       225\n",
      "\n",
      "    accuracy                           0.77       512\n",
      "   macro avg       0.76      0.76      0.76       512\n",
      "weighted avg       0.77      0.77      0.77       512\n"
     ]
    }
   ],
   "source": [
    "y_dev = dev_df[\"label\"]\n",
    "y_dev = np.array(y_dev.map({\"positive\": 1, \"negative\": 0}))\n",
    "y_dev = y_dev.tolist()\n",
    "x_dev = x_dev_word_based_feature_matrix\n",
    "\n",
    "wb_eval_predictions = nb_word_based_tokenization.predict(x_dev)\n",
    "print(classification_report(y_dev, wb_eval_predictions))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-08T14:21:26.164859Z",
     "start_time": "2024-08-08T14:21:25.332176Z"
    }
   },
   "id": "67ff3d009c7333b0",
   "execution_count": 295
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BPE Tokenization Model Evaluation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "130a9709f6cf5cdf"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.76      0.64       287\n",
      "           1       0.42      0.23      0.29       225\n",
      "\n",
      "    accuracy                           0.52       512\n",
      "   macro avg       0.49      0.49      0.47       512\n",
      "weighted avg       0.50      0.52      0.49       512\n"
     ]
    }
   ],
   "source": [
    "x_dev_bpe = x_dev_bpe_feature_matrix\n",
    "\n",
    "bpe_eval_predictions = nb_word_based_tokenization.predict(x_dev_bpe)\n",
    "print(classification_report(y_dev, bpe_eval_predictions))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-08T14:21:30.346238Z",
     "start_time": "2024-08-08T14:21:30.066662Z"
    }
   },
   "id": "8006238d110c4a22",
   "execution_count": 296
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
