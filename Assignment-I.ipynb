{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Installations, Imports and Downloads\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e57af82a0338e60"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import re\n",
    "import numpy as np\n",
    "warnings.filterwarnings(\"ignore\", category = UserWarning)\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "FS = (8,4) # figure size\n",
    "RS = 264\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T11:33:38.749812Z",
     "start_time": "2024-08-06T11:33:37.239621Z"
    }
   },
   "id": "b77082fbc75833ba",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directiory:  /Users/rogerbukuru/Documents/UCT Masters/MSc Statistics and Data Science/NLP-CSC5035Z/NLPTutsAssignments/Assignment-I/afrisent-semeval-2023\n",
      "/Users/rogerbukuru/Documents/UCT Masters/MSc Statistics and Data Science/NLP-CSC5035Z/NLPTutsAssignments/Assignment-I/afrisent-semeval-2023\n",
      "From https://github.com/afrisenti-semeval/afrisent-semeval-2023\r\n",
      " * branch            HEAD       -> FETCH_HEAD\r\n",
      "Already up to date.\r\n"
     ]
    }
   ],
   "source": [
    "PROJECT_DIR = os.getcwd() + '/afrisent-semeval-2023'\n",
    "print('Current directiory: ', PROJECT_DIR)\n",
    "PROJECT_GITHUB_URL = 'https://github.com/afrisenti-semeval/afrisent-semeval-2023.git'\n",
    "\n",
    "if not os.path.isdir(PROJECT_DIR):\n",
    "  !git clone {PROJECT_GITHUB_URL}\n",
    "else:\n",
    "  %cd {PROJECT_DIR}\n",
    "  !git pull {PROJECT_GITHUB_URL}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T11:33:41.800339Z",
     "start_time": "2024-08-06T11:33:40.732404Z"
    }
   },
   "id": "91fdcbd58f0388f6",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Loading\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be842af4aa9e3ccd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Choose language \n",
    "language = 'swa'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T11:33:45.567985Z",
     "start_time": "2024-08-06T11:33:45.565929Z"
    }
   },
   "id": "945c286f95628713",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory:  /Users/rogerbukuru/Documents/UCT Masters/MSc Statistics and Data Science/NLP-CSC5035Z/NLPTutsAssignments/Assignment-I/afrisent-semeval-2023/data/swa\n",
      "Train shape:  (1810, 2)\n",
      "Dev shape:  (453, 2)\n",
      "Test shape:  (748, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                                   text     label\n742   Mkurugenzi Wa Bodi ya Mikopo ya Elimu ya Juu A...   neutral\n1177              Ahsante kwa kutembelea ukurasa wetunk   neutral\n144   Mkurugenzi Mtendaji wa Halmashauri ya wilaya y...  negative\n1288  Usawa wa kijinsia bado ni changamoto kubwa dun...  positive\n638   Emmanuel Msanga Tafadhali tunaomba ututumie sc...   neutral\n1377  Habari Juma Usijali tutakufahamisha mshindi hu...  positive\n1764  Ndoto zinatufanya tubaki kuwa vijanaa maono ya...  positive\n803   Sasa hapo ule vyote vya baridi au kimoja wapo ...   neutral\n461   Leo saa 8 mchana Live On Live 1077 Upendo fm R...   neutral\n1278  Napenda kuishukuru Serikali ya Jamhuri ya Muun...  positive",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>742</th>\n      <td>Mkurugenzi Wa Bodi ya Mikopo ya Elimu ya Juu A...</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>1177</th>\n      <td>Ahsante kwa kutembelea ukurasa wetunk</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>144</th>\n      <td>Mkurugenzi Mtendaji wa Halmashauri ya wilaya y...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>1288</th>\n      <td>Usawa wa kijinsia bado ni changamoto kubwa dun...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>638</th>\n      <td>Emmanuel Msanga Tafadhali tunaomba ututumie sc...</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>1377</th>\n      <td>Habari Juma Usijali tutakufahamisha mshindi hu...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>1764</th>\n      <td>Ndoto zinatufanya tubaki kuwa vijanaa maono ya...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>803</th>\n      <td>Sasa hapo ule vyote vya baridi au kimoja wapo ...</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>461</th>\n      <td>Leo saa 8 mchana Live On Live 1077 Upendo fm R...</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>1278</th>\n      <td>Napenda kuishukuru Serikali ya Jamhuri ya Muun...</td>\n      <td>positive</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "DATA_DIR = f'{PROJECT_DIR}/data/{language}'\n",
    "print('Data directory: ', DATA_DIR)\n",
    "\n",
    "train_df = pd.read_csv(f'{DATA_DIR}/train.tsv', sep='\\t', names=['text', 'label'], header=0)\n",
    "dev_df = pd.read_csv(f'{DATA_DIR}/dev.tsv', sep='\\t', names=['text', 'label'], header=0)\n",
    "test_df = pd.read_csv(f'{DATA_DIR}/test.tsv', sep='\\t', names=['text', 'label'], header=0)\n",
    "\n",
    "print('Train shape: ', train_df.shape)\n",
    "print('Dev shape: ', dev_df.shape)\n",
    "print('Test shape: ', test_df.shape)\n",
    "\n",
    "# Display data\n",
    "train_df.sample(n=10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T11:33:47.571654Z",
     "start_time": "2024-08-06T11:33:47.552001Z"
    }
   },
   "id": "b1b9cbe14aa9740f",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Cleaning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "19067257a5279928"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Discard neutral examples\n",
    "train_df = train_df[train_df['label'] != 'neutral']\n",
    "dev_df = dev_df[dev_df['label'] != 'neutral']\n",
    "test_df = test_df[test_df['label'] != 'neutral']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T11:35:32.250405Z",
     "start_time": "2024-08-06T11:35:32.248487Z"
    }
   },
   "id": "aca2cf3c14aafd31",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    # Replace URLS with [URL]\n",
    "    text = re.sub(r'http\\S+', '[URL]', text)\n",
    "\n",
    "    # Replace numbers with [NUM]\n",
    "    text = re.sub(r'\\d+', '[NUM]', text)\n",
    "\n",
    "    # Remove trailing spaces\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "train_df['text'] = train_df['text'].apply(clean)\n",
    "dev_df['text'] = dev_df['text'].apply(clean)\n",
    "test_df['text'] = test_df['text'].apply(clean)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b44a453899f24356"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Construct Vocabulary"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4534f864e77f22f8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Count number of tokens in corpus\n",
    "def count_tokens(sentences):\n",
    "    \"\"\"\n",
    "    Count number of tokens in corpus\n",
    "\n",
    "    param: sentences: list of list of tokens e.g. [['This', 'is', 'a', 'sentence'], ['This', 'is', 'another', 'sentence'], ...]\n",
    "    return:\n",
    "        count: number of tokens in corpus\n",
    "    \"\"\"\n",
    "    total_tokens = 0\n",
    "    for sentence in sentences:\n",
    "        total_tokens += len(sentence)\n",
    "    return total_tokens"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa2c3ccefea2eec0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Collect type counts in corpus\n",
    "def create_type_counts(sentences):\n",
    "    \"\"\"\n",
    "    Count number of types in corpus\n",
    "\n",
    "    param: sentences: list of list of tokens e.g. [['This', 'is', 'a', 'sentence'], ['This', 'is', 'another', 'sentence'], ...]\n",
    "    return:\n",
    "        type2count: dictionary of type counts in corpus e.g. {'This': 2, 'sentence': 2, ...}\n",
    "    \"\"\"\n",
    "    type2count = {}\n",
    "    for sentence in sentences:\n",
    "        for type_ in sentence:\n",
    "            if type_ not in type2count:\n",
    "                type2count[type_] = 1\n",
    "            else:\n",
    "                current_count = type2count[type_]\n",
    "                type2count[type_] = current_count +1\n",
    "    return type2count"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "170232a2a3218e1e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Create vocabulary\n",
    "def create_vocabulary(type2count, min_count):\n",
    "    \"\"\"\n",
    "    This function creates an indexed vocabulary from vocabulary counts and returns it as a list and a dictionary.\n",
    "\n",
    "    param:\n",
    "        type2count: dictionary of type counts in corpus e.g. {'This': 2, 'sentence': 2, ...}\n",
    "        min_count: minimum count of a word to be included in the vocabulary\n",
    "    return:\n",
    "        index2type: list of words in the vocabulary e.g. ['word1', 'word2', 'word3', ...]\n",
    "        type2index: dictionary mapping words to their index in the index2type vocabulary e.g. {'word1': 0, 'word2': 1, 'word3': 2, ...}\n",
    "    \"\"\"\n",
    "    index2type = []\n",
    "    type2index = {}\n",
    "    for type_, count in type2count.items():\n",
    "        if(count >= min_count):\n",
    "            index2type.append(type_)\n",
    "            type2index[type_] = len(index2type) - 1\n",
    "    return index2type, type2index"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a55b473b29ace5e4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "\n",
    "# Tokenization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d3285ba6754d912"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Word Based Tokenization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26626c598cae9936"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def whitespace_tokenize(sentences):\n",
    "    return [sentence.split() for sentence in sentences]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "64f366be8fdce023"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Byte-Pair Encoding (BPE) Tokenization\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d72e944dadb4c3f0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class BPETokenizer():\n",
    "\n",
    "    def __init__(self, sentences, vocab_size):\n",
    "        \"\"\"\n",
    "        Initialize the BPE tokenizer.\n",
    "\n",
    "        Args:\n",
    "            sentences (list[str]): list of list of tokens e.g. [['This', 'is', 'a', 'sentence'], ['This', 'is', 'another', 'sentence'], ...]\n",
    "            vocab_size (int): The desired vocabulary size after training.\n",
    "        \"\"\"\n",
    "        self.sentences = sentences\n",
    "        self.vocab_size = vocab_size\n",
    "        self.word_freqs = defaultdict(int)\n",
    "        self.splits = {}\n",
    "        self.merges = {}\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train the BPE tokenizer by iteratively merging the most frequent pairs of symbols.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary of merges in the format {(a, b): 'ab'}, where 'a' and 'b' are symbols merged into 'ab'.\n",
    "        \"\"\"\n",
    "        # Split corpus\n",
    "        for sentence in self.sentences:\n",
    "            for word in sentence:\n",
    "                self.splits[word] = [char for char in word]\n",
    "                    \n",
    "        for i in range(self.vocab_size):\n",
    "            self.compute_pair_freqs() # compute adjacent pair frequencies\n",
    "            pair, _ = list(self.word_freqs.items())[0] # most frequent pair\n",
    "            self.merge_pair(pair[0], pair[1])\n",
    "            self.merges[pair] = pair[0] + pair[1]\n",
    "        return self.merges\n",
    "\n",
    "\n",
    "    def compute_pair_freqs(self):\n",
    "        \"\"\"\n",
    "        Compute the frequency of each pair of symbols in the corpus.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary of pairs and their frequencies in the format {(a, b): frequency}.\n",
    "        \"\"\"\n",
    "        pair_freqs = defaultdict(int)\n",
    "        for _, split in self.splits.items():\n",
    "            for i in range(len(split)-1):\n",
    "                pair = (split[i], split[i+1])\n",
    "                if pair not in pair_freqs:\n",
    "                    pair_freqs[pair] = 1\n",
    "                else:\n",
    "                    pair_freqs[pair] += 1\n",
    "        self.word_freqs = pair_freqs\n",
    "        self.word_freqs = dict(sorted(self.word_freqs.items(), key=lambda x: x[1], reverse=True)) # sort from max to min count\n",
    "        return self.word_freqs\n",
    "        \n",
    "    def merge_pair(self, a, b):\n",
    "        \"\"\"\n",
    "        Merge the given pair of symbols in all words where they appear adjacent.\n",
    "\n",
    "        Args:\n",
    "            a (str): The first symbol in the pair.\n",
    "            b (str): The second symbol in the pair.\n",
    "\n",
    "        Returns:\n",
    "            dict: The updated splits dictionary after merging.\n",
    "        \"\"\"\n",
    "        pair = (a,b)\n",
    "        # Check if valid pair\n",
    "        if pair in self.word_freqs:\n",
    "            new_token = a+b\n",
    "            for word, split in self.splits.items():\n",
    "                for i in range(len(split)-1):\n",
    "                    if split[i] == a and split[i+1] == b:\n",
    "                       split[i] = new_token\n",
    "                       new_split = list(filter(lambda x: x not in [b], split))\n",
    "                       self.splits[word] = new_split\n",
    "        return self.splits\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"\n",
    "        Tokenize a given text using the trained BPE tokenizer.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to be tokenized.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: A list of tokens obtained after applying BPE tokenization.\n",
    "        \"\"\"\n",
    "\n",
    "        pre_tokenized_text = text.split()\n",
    "        splits_text = [[l for l in word] for word in pre_tokenized_text]\n",
    "\n",
    "        for pair, merge in self.merges.items():\n",
    "            for idx, split in enumerate(splits_text):\n",
    "                i = 0\n",
    "                while i < len(split) - 1:\n",
    "                    if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
    "                        split = split[:i] + [merge] + split[i + 2 :]\n",
    "                    else:\n",
    "                        i += 1\n",
    "                splits_text[idx] = split\n",
    "        result = sum(splits_text, [])\n",
    "        return result"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "185fdac1da4c7403"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature Extraction"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "47386243702bc12f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## One-Hot Encoding Vectorization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6aadfd89ead00bf5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def one_hot_vectorize(sentences, type2index):\n",
    "    \"\"\"\n",
    "    One-hot encode a list of sentences.\n",
    "\n",
    "    param:\n",
    "        list of list of tokens e.g. [['This', 'is', 'a', 'sentence'], ['This', 'is', 'another', 'sentence'], ...]\n",
    "        type2index: dictionary mapping words to their index in the vocabulary e.g. {'word1': 0, 'word2': 1, 'word3': 2, ...}\n",
    "    return:\n",
    "        one_hot_sentences: 2d numpy array of one-hot encoded sentences e.g. [[1, 0, 0, 1, ...], [0, 1, 1, 0, ...], ...]\n",
    "    \"\"\"\n",
    "    one_hot_encoding = np.array(np.zeros((len(type2index.items()), len(type2index.items()))))\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            word_index = type2index[word]\n",
    "            one_hot_encoding[word_index, word_index] = 1\n",
    "    return one_hot_encoding\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7298bb785e0a0987"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Word-Based Tokenization Features\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37333bc85957788f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Store training data text as list of tweets\n",
    "train_corpus = train_df['text'].tolist()\n",
    "tokenized_train_corpus = whitespace_tokenize(train_corpus)\n",
    "type2count = create_type_counts(tokenized_train_corpus)\n",
    "# Sort types by counts\n",
    "type2count = dict(sorted(type2count.items(), key=lambda x: x[1], reverse=True))\n",
    "index2type, type2index = create_vocabulary(type2count, min_count=1)\n",
    "# It's good practice to add a special token for unknown words and padding (to make all sentences in training batches the same length)\n",
    "type2index['<UNK>'] = len(index2type)\n",
    "index2type.append('<UNK>')\n",
    "type2index['<PAD>'] = len(index2type)\n",
    "index2type.append('<PAD>')\n",
    "one_hot_vectorize(tokenized_train_corpus, type2index)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5f9f9e91d866b89"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BPE Tokenization Features"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1183ca42d8e9f287"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Train BPE\n",
    "bpe = BPETokenizer(tokenized_train_corpus, vocab_size=1000)\n",
    "bpe.train()\n",
    "# Apply to our dataset\n",
    "train_df['bpe_text'] = train_df['text'].apply(lambda x: ' '.join(bpe.tokenize(x)))\n",
    "dev_df['bpe_text'] = dev_df['text'].apply(lambda x: ' '.join(bpe.tokenize(x)))\n",
    "test_df['bpe_text'] = test_df['text'].apply(lambda x: ' '.join(bpe.tokenize(x)))\n",
    "train_df.head()\n",
    "\n",
    "bpe_corpus = train_df['bpe_text'].tolist()\n",
    "tokenized_bpe_corpus = whitespace_tokenize(bpe_corpus)\n",
    "bpe_type2count = create_type_counts(tokenized_bpe_corpus)\n",
    "# Sort types by counts\n",
    "bpe_type2count = dict(sorted(bpe_type2count.items(), key=lambda x: x[1], reverse=True))\n",
    "bpe_index2type, bpe_type2index = create_vocabulary(bpe_type2count, min_count=2)\n",
    "one_hot_vectorize(tokenized_bpe_corpus, bpe_index2type)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e66e51edab48fb3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
