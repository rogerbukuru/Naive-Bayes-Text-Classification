{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JGzT6u-0Qyyc"
   },
   "source": [
    "# CSC5035Z Natural Language Processing\n",
    "# Tutorial 1: Text Processing and Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ismKibXLQyyg"
   },
   "source": [
    "**Authors: Francois Meyer, Jan Buys**\n",
    "\n",
    "**Introduction**\n",
    "\n",
    "Welcome to the first practical tutorial for the NLP course! We will implement some of the ideas covered in this week's lectures and set you up for the practical assignment. This notebook provides skeleton code that you are encouraged to complete as part of your learning process.\n",
    "\n",
    "**Topics:**\n",
    "\n",
    "Content: text processing, tokenisation, text classification, word embeddings\n",
    "\n",
    "\n",
    "**Aims/Learning Objectives:**\n",
    "\n",
    "* Acquire skills in loading annotated NLP datasets.\n",
    "* Implement basic principles of tokenisation.\n",
    "* Train a text classification model (logistic regression)\n",
    "* Explore the fundamentals of word embeddings.\n",
    "\n",
    "**Prerequisites:**\n",
    "\n",
    "* Familiarity with Python libraries for text processing (pandas, nltk, sklearn).\n",
    "* Introductory knowledge of BPE tokenisation.\n",
    "* Understanding of logistic regression.\n",
    "* Introductory knowledge of Word2Vec.\n",
    "\n",
    "**Outline:**\n",
    "\n",
    "* [1. Text processing](#section1)\n",
    "    * [1.1. Data loading](#section1_1)\n",
    "    * [1.2. Data cleaning](#section1_2)\n",
    "    * [1.3. Vocabulary construction](#section1_3)\n",
    "    * [1.4. BPE tokenisation](#section1_4)\n",
    "* [2. Text classification](#section2)\n",
    "    * [2.1. Text vectorization](#section2_1)\n",
    "    * [2.2. Logistic regression](#section2_2)\n",
    "    * [2.3. Evaluation](#section2_3)\n",
    "* [3. Word embeddings](#section3)\n",
    "    * [3.1. Context windows](#section3_1)\n",
    "    * [3.1. Word2Vec CBOW](#section3_2)\n",
    "    * [3.1. Pretrained embeddings](#section3_3)\n",
    "* [4. Conclusion](#section4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gDmJce36Qyyh"
   },
   "source": [
    "# Installations, Imports and Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "1LecSdyiQyyh",
    "ExecuteTime": {
     "end_time": "2024-08-05T21:13:02.585736Z",
     "start_time": "2024-08-05T21:13:01.180977Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import re\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "FS = (8, 4)  # figure size\n",
    "RS = 124  # random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "eM1X7TmNQyyi",
    "ExecuteTime": {
     "end_time": "2024-08-05T21:13:20.512086Z",
     "start_time": "2024-08-05T21:13:19.491903Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directiory:  /Users/rogerbukuru/Documents/UCT Masters/MSc Statistics and Data Science/NLP-CSC5035Z/NLPTutsAssignments/Assignment-I/afrisent-semeval-2023\n",
      "/Users/rogerbukuru/Documents/UCT Masters/MSc Statistics and Data Science/NLP-CSC5035Z/NLPTutsAssignments/Assignment-I/afrisent-semeval-2023\n",
      "From https://github.com/afrisenti-semeval/afrisent-semeval-2023\r\n",
      " * branch            HEAD       -> FETCH_HEAD\r\n",
      "Already up to date.\r\n"
     ]
    }
   ],
   "source": [
    "# Download dataset\n",
    "PROJECT_DIR = os.getcwd() + '/afrisent-semeval-2023'\n",
    "print('Current directiory: ', PROJECT_DIR)\n",
    "PROJECT_GITHUB_URL = 'https://github.com/afrisenti-semeval/afrisent-semeval-2023.git'\n",
    "\n",
    "if not os.path.isdir(PROJECT_DIR):\n",
    "  !git clone {PROJECT_GITHUB_URL}\n",
    "else:\n",
    "  %cd {PROJECT_DIR}\n",
    "  !git pull {PROJECT_GITHUB_URL}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "byf0nl_YQyyj"
   },
   "source": [
    "<a name=\"section1\"></a>\n",
    "#1. Text processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h2YqUXBwQyyj"
   },
   "source": [
    "Throughout this notebook we will use the AfriSenti dataset. It is a sentiment analysis dataset for 14 African languages. Sentiment analysis is the task of classifying the emotional tone of a piece of text. Sentiment analysis datasets resemble the following table - it consists of pieces of text which have been annotated as _positive_ or _negative_ (some datasets also allow _neutral_ as a label).\n",
    "\n",
    "\n",
    "| Text                     | Label |\n",
    "|-----------------------------------|--------------|\n",
    "| I haven't heard anything. I'm really worried actually             |  negative     |\n",
    "| About to go to bed. I am so glad the Tigers won tonight!                     | positive          |\n",
    "\n",
    "The AfriSenti dataset consists of tweets which have been human-labelled according to their emotional tone as either positive, negative, or neutral.\n",
    "\n",
    "\n",
    "* [Paper introducing the dataset: _AfriSenti: A Twitter Sentiment Analysis Benchmark for African Languages_, Muhammad et al., 2023.](https://aclanthology.org/2023.emnlp-main.862.pdf)\n",
    "* [Github repository containing the full dataset](https://github.com/afrisenti-semeval/afrisent-semeval-2023)\n",
    "\n",
    "### AfriSenti languages\n",
    "\n",
    "| No. | Language                     | Code | Country        |\n",
    "|-----|------------------------------|--------------|----------------|\n",
    "| 1   | Algerian Arabic              | arq          | Algeria        |\n",
    "| 2   | Amharic                      | amh          | Ethiopia       |\n",
    "| 3   | Hausa                        | hau          | Nigeria        |\n",
    "| 4   | Igbo                         | ibo          | Nigeria        |\n",
    "| 5   | Kinyarwanda                  | kin          | Rwanda         |\n",
    "| 6   | Moroccan Arabic/Darija       | ary          | Morocco        |\n",
    "| 7   | Mozambique Portuguese        | por        | Mozambique     |\n",
    "| 8   | Nigerian Pidgin              | pcm          | Nigeria        |\n",
    "| 9   | Oromo                        | orm          | Ethiopia       |\n",
    "| 10  | Swahili                      | swa          | Kenya/Tanzania |\n",
    "| 11  | Tigrinya                     | tir          | Ethiopia       |\n",
    "| 12  | Twi                          | twi          | Ghana          |\n",
    "| 13  | Xitsonga                     | tso          | Mozambique     |\n",
    "| 14  | Yoruba                       | yor          | Nigeria        |\n",
    "\n",
    "\n",
    "The dataset covers 14 languages spoken across the African continent. For each language, the dataset stores annotated tweets for training (*train.tsv*), validation (*dev.tsv*), and testing (*test.tsv*). In NLP datasets and implementations, languages are often referred to via abbreviations known as language codes. Your first task is to edit the next code cell to enter the language code of the language you want to use going forward in this notebook. The table above lists the language codes of each of the 14 languages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "pV7-4sf9Qyyj",
    "ExecuteTime": {
     "end_time": "2024-08-05T21:13:25.461046Z",
     "start_time": "2024-08-05T21:13:25.458451Z"
    }
   },
   "outputs": [],
   "source": [
    "# Choose language\n",
    "language =  'swa'  # Can be ['arq', 'amh', 'hau', 'ibo', 'kin', 'ary', 'por', 'pcm', orm', 'swa', 'tir', 'twi', 'tso', 'yor']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RbOLeaF9Qyyk"
   },
   "source": [
    "<a name=\"section1_1\"></a>\n",
    "## 1.1. Data loading\n",
    "\n",
    "Now we can load the train/dev/test datasets for our chosen language. Each dataset is stored as a .tsv file (tab-separated values) with two data columns (the tweet text and the sentiment label) separated by a tab space. Below we read these datasets into dataframes - a Python data structure for storing tabular data in the pandas library. We display a few rows of the training dataframe to show the data format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "yAADlbPPQyyk",
    "ExecuteTime": {
     "end_time": "2024-08-05T21:13:27.183271Z",
     "start_time": "2024-08-05T21:13:27.162806Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory:  /Users/rogerbukuru/Documents/UCT Masters/MSc Statistics and Data Science/NLP-CSC5035Z/NLPTutsAssignments/Assignment-I/afrisent-semeval-2023/data/swa\n",
      "Train shape:  (1810, 2)\n",
      "Dev shape:  (453, 2)\n",
      "Test shape:  (748, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                                   text     label\n1786  Vunja ukimya rushwa ya ngono inadhalilisha na ...  positive\n1127  vipi mzee wangu uko na mshua boy Shujaa huko k...   neutral\n652   Habari pole sana kwa changamoto kwa sasa laini...   neutral\n1575  Namshukuru sana baba yangu babu  bibi mama zan...  positive\n947   Lol Msukuma aliuliza kwahiyo umeme ukikatika t...   neutral\n1492  Hatutakiwa kuhukumu Mungu pekee ndo anajukumu ...  positive\n540   Taasisi zinahitaji kujiuliza zinataka page za ...   neutral\n1464  Episode 4 hii ni Tamthilia yenye kuburudisha n...  positive\n637   Viongozi mbalimbali na wanachama wa wakiwa kat...   neutral\n344   wanasemaje kuhusu Majukwaa ya kidijitali Mtoto...   neutral",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1786</th>\n      <td>Vunja ukimya rushwa ya ngono inadhalilisha na ...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>1127</th>\n      <td>vipi mzee wangu uko na mshua boy Shujaa huko k...</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>652</th>\n      <td>Habari pole sana kwa changamoto kwa sasa laini...</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>1575</th>\n      <td>Namshukuru sana baba yangu babu  bibi mama zan...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>947</th>\n      <td>Lol Msukuma aliuliza kwahiyo umeme ukikatika t...</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>1492</th>\n      <td>Hatutakiwa kuhukumu Mungu pekee ndo anajukumu ...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>540</th>\n      <td>Taasisi zinahitaji kujiuliza zinataka page za ...</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>1464</th>\n      <td>Episode 4 hii ni Tamthilia yenye kuburudisha n...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>637</th>\n      <td>Viongozi mbalimbali na wanachama wa wakiwa kat...</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>344</th>\n      <td>wanasemaje kuhusu Majukwaa ya kidijitali Mtoto...</td>\n      <td>neutral</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "DATA_DIR = f'{PROJECT_DIR}/data/{language}'\n",
    "print('Data directory: ', DATA_DIR)\n",
    "\n",
    "train_df = pd.read_csv(f'{DATA_DIR}/train.tsv', sep='\\t', names=['text', 'label'], header=0)\n",
    "dev_df = pd.read_csv(f'{DATA_DIR}/dev.tsv', sep='\\t', names=['text', 'label'], header=0)\n",
    "test_df = pd.read_csv(f'{DATA_DIR}/test.tsv', sep='\\t', names=['text', 'label'], header=0)\n",
    "\n",
    "print('Train shape: ', train_df.shape)\n",
    "print('Dev shape: ', dev_df.shape)\n",
    "print('Test shape: ', test_df.shape)\n",
    "\n",
    "# Display data\n",
    "train_df.sample(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hs3eTveIQyyl"
   },
   "source": [
    "<a name=\"section1_2\"></a>\n",
    "## 1.2. Data cleaning\n",
    "\n",
    "Before we proceed, it's important to ensure that our data is clean and ready for processing. In this section, we will perform some basic data cleaning steps to remove unwanted elements in the text and prepare our dataset for NLP modelling purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7PrIXhIQyyl"
   },
   "source": [
    "In this notebook we are interested in **binary classification** - predicting a tweet's emotional content as either **positive** or **negative**. We filter out tweets that are labelled as **neutral** before we continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "zAWRyXM3Qyyl",
    "ExecuteTime": {
     "end_time": "2024-08-05T21:13:30.766636Z",
     "start_time": "2024-08-05T21:13:30.758918Z"
    }
   },
   "outputs": [],
   "source": [
    "# Discard neutral examples\n",
    "train_df = train_df[train_df['label'] != 'neutral']\n",
    "dev_df = dev_df[dev_df['label'] != 'neutral']\n",
    "test_df = test_df[test_df['label'] != 'neutral']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5pwpqmF3Qyyl"
   },
   "source": [
    "The extent of data cleaning and preprocessing will depend on quality of the raw dataset, the NLP task we are preparing the data for, and our personal preferences as NLP practitioners. The ``nltk`` library (Natural Language Toolkit) is a popular Python library for text processing and pre-processing. It supports tokenization, stemming, tagging, and parsing for several languages. We do not need it for this this notebook, since we stick to rather basic preprocessing stategies.\n",
    "\n",
    "* Replace all urls with a special '[URL]' token.\n",
    "* Replace all numbers with a special '[NUM]' token.\n",
    "* Remove white extra whitespaces either side of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "zcmqDXmDQyyl",
    "ExecuteTime": {
     "end_time": "2024-08-05T21:13:36.317804Z",
     "start_time": "2024-08-05T21:13:36.315270Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    # Replace URLS with [URL]\n",
    "    text = re.sub(r'http\\S+', '[URL]', text)\n",
    "\n",
    "    # Replace numbers with [NUM]\n",
    "    text = re.sub(r'\\d+', '[NUM]', text)\n",
    "\n",
    "    # Remove trailing spaces\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "train_df['text'] = train_df['text'].apply(clean)\n",
    "dev_df['text'] = dev_df['text'].apply(clean)\n",
    "test_df['text'] = test_df['text'].apply(clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A8yajd7_Qyyl"
   },
   "source": [
    "<a name=\"section1_3\"></a>\n",
    "## 1.3. Vocabulary construction\n",
    "\n",
    "One of the fundamental steps in text processing for NLP is constructing a vocabulary from our dataset. A vocabulary is a set of unique words or tokens present in the text corpus. In this section, we will create a vocabulary from our training dataset and explore its characteristics.\n",
    "\n",
    "We refer to vocabulary items as **types** and to particular occurrences of these types in the dataset as **tokens**. For now we simply tokenise our text data based on the existing tokens in the raw text - we split text on white spaces.\n",
    "\n",
    "For NLP purposes, we want to map each type in our vocabulary to an **index**, a unique number identifying that type. Later we can use this index to, for example, look up vector representations for our words using a lookup table. To achieve this, our vocabulary will be represented with three variables:\n",
    "* index2type: list of unique types in the vocabulary e.g. ['word1', 'word2', 'word3', ...]\n",
    "* type2index: dictionary mapping types to their index in the index2type vocabulary e.g. {'word1': 0, 'word2': 1, 'word3': 2, ...}\n",
    "* type2count: dictionary mapping types to the number of corresponding token occurences of that type in the training data e.g. {'word1': 1012, 'word2': 510, 'word3': 45, ...}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "rnREP1IwQyyl",
    "ExecuteTime": {
     "end_time": "2024-08-05T21:13:40.651033Z",
     "start_time": "2024-08-05T21:13:40.643785Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['Kwani tanesco wanakataga umeme makusudinadhani kuna changamoto behind zinatakiwa zitatuliwe na sio kutoa matamko',\n 'cjawahi kuona content yoyote zaidi ya kuwa analalamika cjawahi kuona akitafuta solution ya tattizo zaid ya kulalamikabasi tutakuwa na Rais wa ajabu huwa mwaka [NUM]',\n 'Bomu lililokuwa limetegwa ndani ya gari likiwalenga wajenzi kutoka Uturuki limelipuka katika eneo la Afgoye kaskazini Magharibi mwa mji mkuu wa Mogadishu Somalia limeua watu wanne polisi wamesema',\n 'Kuna video inasambaa mitandaoni jamaa amemfumania mkewe akiwa na jamaa mwingine huku akimlalamikia jamaa akimwambia kw',\n 'Viwavijeshi wanapita katika hatua kuu [NUM] za ukuaji katika hatua yake ya larvae mayai [NUM] hutagwa na larvae mmoja kwa mwezi Hatua pekee inayowezesha kusambaa kwa viwa vijeshi ni kipepeo ambao huruka kwa makundi na wanaweza kuruka hadi kilometa [NUM]']"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store training data text as list of tweets\n",
    "train_corpus = train_df['text'].tolist()\n",
    "train_corpus[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next we have to decide how we are going to tokenize the tweets. Libraries like ``nltk`` provide regex-based tokenizers that are handcrafted for specific languages.  For now we will use the simple strategy of splitting text on white spaces - so our tokens will be the units of text divided by white spaces."
   ],
   "metadata": {
    "id": "VCksk5pjnfME"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def whitespace_tokenize(sentences):\n",
    "    return [sentence.split() for sentence in sentences]\n",
    "\n",
    "tokenized_train_corpus = whitespace_tokenize(train_corpus)\n",
    "tokenized_train_corpus[0:5]"
   ],
   "metadata": {
    "id": "73Qyz6CQnTgS",
    "ExecuteTime": {
     "end_time": "2024-08-05T21:13:42.950798Z",
     "start_time": "2024-08-05T21:13:42.947264Z"
    }
   },
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "[['Kwani',\n  'tanesco',\n  'wanakataga',\n  'umeme',\n  'makusudinadhani',\n  'kuna',\n  'changamoto',\n  'behind',\n  'zinatakiwa',\n  'zitatuliwe',\n  'na',\n  'sio',\n  'kutoa',\n  'matamko'],\n ['cjawahi',\n  'kuona',\n  'content',\n  'yoyote',\n  'zaidi',\n  'ya',\n  'kuwa',\n  'analalamika',\n  'cjawahi',\n  'kuona',\n  'akitafuta',\n  'solution',\n  'ya',\n  'tattizo',\n  'zaid',\n  'ya',\n  'kulalamikabasi',\n  'tutakuwa',\n  'na',\n  'Rais',\n  'wa',\n  'ajabu',\n  'huwa',\n  'mwaka',\n  '[NUM]'],\n ['Bomu',\n  'lililokuwa',\n  'limetegwa',\n  'ndani',\n  'ya',\n  'gari',\n  'likiwalenga',\n  'wajenzi',\n  'kutoka',\n  'Uturuki',\n  'limelipuka',\n  'katika',\n  'eneo',\n  'la',\n  'Afgoye',\n  'kaskazini',\n  'Magharibi',\n  'mwa',\n  'mji',\n  'mkuu',\n  'wa',\n  'Mogadishu',\n  'Somalia',\n  'limeua',\n  'watu',\n  'wanne',\n  'polisi',\n  'wamesema'],\n ['Kuna',\n  'video',\n  'inasambaa',\n  'mitandaoni',\n  'jamaa',\n  'amemfumania',\n  'mkewe',\n  'akiwa',\n  'na',\n  'jamaa',\n  'mwingine',\n  'huku',\n  'akimlalamikia',\n  'jamaa',\n  'akimwambia',\n  'kw'],\n ['Viwavijeshi',\n  'wanapita',\n  'katika',\n  'hatua',\n  'kuu',\n  '[NUM]',\n  'za',\n  'ukuaji',\n  'katika',\n  'hatua',\n  'yake',\n  'ya',\n  'larvae',\n  'mayai',\n  '[NUM]',\n  'hutagwa',\n  'na',\n  'larvae',\n  'mmoja',\n  'kwa',\n  'mwezi',\n  'Hatua',\n  'pekee',\n  'inayowezesha',\n  'kusambaa',\n  'kwa',\n  'viwa',\n  'vijeshi',\n  'ni',\n  'kipepeo',\n  'ambao',\n  'huruka',\n  'kwa',\n  'makundi',\n  'na',\n  'wanaweza',\n  'kuruka',\n  'hadi',\n  'kilometa',\n  '[NUM]']]"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQ_9skEDQyyl"
   },
   "source": [
    "Complete the following to cells of code.\n",
    "* The function `` count_tokens `` should take a list of sentences as input and count the corpus size.\n",
    "* The function `` create_vocabulary `` should take a list of sentences as input and iteratively build a vocabulary.\n",
    "\n",
    "In each case, tokenise sentences on white spaces. This can be done with the function function ``split()``, which splits a string into a list of its whitespace delimited strings.\n",
    "\n",
    "e.g. ``sentence.split()``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "42yNURWTQyym",
    "ExecuteTime": {
     "end_time": "2024-08-05T21:13:45.069759Z",
     "start_time": "2024-08-05T21:13:45.067416Z"
    }
   },
   "outputs": [],
   "source": [
    "# Count number of tokens in corpus\n",
    "def count_tokens(sentences):\n",
    "    \"\"\"\n",
    "    Count number of tokens in corpus\n",
    "\n",
    "    param: sentences: list of list of tokens e.g. [['This', 'is', 'a', 'sentence'], ['This', 'is', 'another', 'sentence'], ...]\n",
    "    return:\n",
    "        count: number of tokens in corpus\n",
    "    \"\"\"\n",
    "    total_tokens = 0\n",
    "    for sentence in sentences:\n",
    "        total_tokens += len(sentence)\n",
    "    return total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0jivEJjwQyym"
   },
   "outputs": [],
   "source": [
    "num_tokens = count_tokens(tokenized_train_corpus)\n",
    "print('Number of tokens in corpus: ', num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "d4qPHK_pQyym",
    "ExecuteTime": {
     "end_time": "2024-08-05T21:13:47.079426Z",
     "start_time": "2024-08-05T21:13:47.072464Z"
    }
   },
   "outputs": [],
   "source": [
    "# Collect type counts in corpus\n",
    "def create_type_counts(sentences):\n",
    "    \"\"\"\n",
    "    Count number of types in corpus\n",
    "\n",
    "    param: sentences: list of list of tokens e.g. [['This', 'is', 'a', 'sentence'], ['This', 'is', 'another', 'sentence'], ...]\n",
    "    return:\n",
    "        type2count: dictionary of type counts in corpus e.g. {'This': 2, 'sentence': 2, ...}\n",
    "    \"\"\"\n",
    "    type2count = {}\n",
    "    for sentence in sentences:\n",
    "        for type in sentence:\n",
    "            if type not in type2count:\n",
    "                type2count[type] = 1\n",
    "            else:\n",
    "                current_count = type2count[type]\n",
    "                type2count[type] = current_count +1\n",
    "    return type2count\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "4aTRsvy-Qyym",
    "ExecuteTime": {
     "end_time": "2024-08-05T21:13:49.875238Z",
     "start_time": "2024-08-05T21:13:49.871441Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of types in corpus:  5383\n",
      "ya: 529\n",
      "na: 505\n",
      "wa: 394\n",
      "kwa: 321\n",
      "[NUM]: 216\n",
      "ni: 136\n"
     ]
    }
   ],
   "source": [
    "type2count = create_type_counts(tokenized_train_corpus)\n",
    "print('Number of types in corpus: ', len(type2count))\n",
    "\n",
    "# Sort types by counts\n",
    "type2count = dict(sorted(type2count.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "# Print first few types and counts\n",
    "for i, (type_, count) in enumerate(type2count.items()):\n",
    "    print(f'{type_}: {count}')\n",
    "    if i == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "iwIP_prkQyyn",
    "ExecuteTime": {
     "end_time": "2024-08-05T21:13:52.337490Z",
     "start_time": "2024-08-05T21:13:52.330735Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create vocabulary\n",
    "def create_vocabulary(type2count, min_count):\n",
    "    \"\"\"\n",
    "    This function creates an indexed vocabulary from vocabulary counts and returns it as a list and a dictionary.\n",
    "\n",
    "    param:\n",
    "        type2count: dictionary of type counts in corpus e.g. {'This': 2, 'sentence': 2, ...}\n",
    "        min_count: minimum count of a word to be included in the vocabulary\n",
    "    return:\n",
    "        index2type: list of words in the vocabulary e.g. ['word1', 'word2', 'word3', ...]\n",
    "        type2index: dictionary mapping words to their index in the index2type vocabulary e.g. {'word1': 0, 'word2': 1, 'word3': 2, ...}\n",
    "    \"\"\"\n",
    "    index2type = []\n",
    "    type2index = {}\n",
    "    for type_, count in type2count.items():\n",
    "        if(count >= min_count):\n",
    "            index2type.append(type_)\n",
    "            type2index[type_] = len(index2type) - 1\n",
    "    return index2type, type2index\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qvuS2gCiQyyn"
   },
   "outputs": [],
   "source": [
    "index2type, type2index = create_vocabulary(type2count, min_count=1)\n",
    "\n",
    "# It's good practice to add a special token for unknown words and padding (to make all sentences in training batches the same length)\n",
    "type2index['<UNK>'] = len(index2type)\n",
    "index2type.append('<UNK>')\n",
    "type2index['<PAD>'] = len(index2type)\n",
    "index2type.append('<PAD>')\n",
    "\n",
    "print('Vocabulary size: ', len(index2type))\n",
    "print('First 10 words in the vocabulary: ', index2type[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NGrwhRUkQyyn"
   },
   "source": [
    "<a name=\"section1_4\"></a>\n",
    "## 1.4. BPE tokenisation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jmj6V7LBQyyn"
   },
   "source": [
    "So far we have tokenised sentences based on the white spaces separating tokens in raw text. In most modern NLP systems, sentences are tokenised into subword tokens instead of words. This approach helps in handling out-of-vocabulary words and improves the model's ability to capture morphological (subword) information.\n",
    "\n",
    "Byte Pair Encoding (BPE) is a popular subword tokenisation algorithm in NLP. In this section, we will implement the BPE algorithm and apply it to our dataset.\n",
    "\n",
    "BPE and related algorithms have two parts:\n",
    "* A type learner that takes a raw training corpus and induces a vocabulary (a set of types) of prespecified size (e.g. 1000 subwords).\n",
    "* A token segmenter that takes a raw test sentence and tokenises it according to that subword vocabulary.\n",
    "\n",
    "## BPE type learner (train on training set)\n",
    "\n",
    "1. Start with a vocabulary consisting of all individual characters e.g. {A, B, C, D,…, a, b, c, d...}.\n",
    "2. Repeat until the prespecified vocabulary size has been reached:\n",
    "    * Choose the two symbols that are most frequently adjacent in the training corpus (say 'A', 'B').\n",
    "    * Merge these symbols and add the newly merged symbol 'AB' to the vocabulary.\n",
    "    * Replace every adjacent 'A' 'B' in the corpus with 'AB'.\n",
    "\n",
    "\n",
    "## BPE token segmenter (apply to train/dev/test set)\n",
    "\n",
    "Segmenter algorithm: Run each merge learned from the training data greedily, in the order they were learned (test frequencies don't play a role).\n",
    "\n",
    "So merge every \"A\" \"B\" to \"AB\", then merge \"AB\" \"C\" to \"ABC\", etc.\n",
    "\n",
    "## Other details\n",
    "\n",
    "* Usually basic tokenization is performed first (space-based tokenization and separating punctuation). BPE is then applied to the initial tokens.\n",
    "* To enable the algorithm to learn to represent the boundary between tokens, commonly a special end-of-word symbol '_' is added before spaces in the training corpus (or alternatively between the space and the next word).\n",
    "\n",
    "## Pseudocode\n",
    "\n",
    "```\n",
    "function BYTE-PAIR ENCODING(strings C, number of merges k) returns vocab V\n",
    "    V <- all unique characters in C     # initial set of tokens is characters\n",
    "    for i = 1 to k do                   # merge tokens k times / until vocab size reached\n",
    "        t_L, t_R <- Most frequent pair of adjacent tokens in C\n",
    "        t_new <- t_L + t_R              # make new token by concatenating\n",
    "        V <- V + t_new                  # update the vocabulary\n",
    "        Replace each occurrence of t_L, t_R in C with t_new   # and update the corpus\n",
    "    return V\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "r0VmGNvLQyyn",
    "ExecuteTime": {
     "end_time": "2024-08-06T09:23:46.426431Z",
     "start_time": "2024-08-06T09:23:46.415541Z"
    }
   },
   "outputs": [],
   "source": [
    "# Implement BPE algorithm\n",
    "# The following class provides a skeleton for implementing the BPE algorithm.\n",
    "# You can use the existing code and method headings as a guideline, or structure\n",
    "# the code as you prefer.\n",
    "\n",
    "class BPETokenizer():\n",
    "\n",
    "    def __init__(self, sentences, vocab_size):\n",
    "        \"\"\"\n",
    "        Initialize the BPE tokenizer.\n",
    "\n",
    "        Args:\n",
    "            sentences (list[str]): list of list of tokens e.g. [['This', 'is', 'a', 'sentence'], ['This', 'is', 'another', 'sentence'], ...]\n",
    "            vocab_size (int): The desired vocabulary size after training.\n",
    "        \"\"\"\n",
    "        self.sentences = sentences\n",
    "        self.vocab_size = vocab_size\n",
    "        self.word_freqs = defaultdict(int)\n",
    "        self.splits = {}\n",
    "        self.merges = {}\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train the BPE tokenizer by iteratively merging the most frequent pairs of symbols.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary of merges in the format {(a, b): 'ab'}, where 'a' and 'b' are symbols merged into 'ab'.\n",
    "        \"\"\"\n",
    "        # Split corpus\n",
    "        for sentence in self.sentences:\n",
    "            for word in sentence:\n",
    "                self.splits[word] = [char for char in word]\n",
    "                    \n",
    "        for i in range(self.vocab_size):\n",
    "            self.compute_pair_freqs() # compute adjacent pair frequencies\n",
    "            pair, _ = list(self.word_freqs.items())[0] # most frequent pair\n",
    "            self.merge_pair(pair[0], pair[1])\n",
    "            self.merges[pair] = pair[0] + pair[1]\n",
    "        return self.merges\n",
    "\n",
    "\n",
    "    def compute_pair_freqs(self):\n",
    "        \"\"\"\n",
    "        Compute the frequency of each pair of symbols in the corpus.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary of pairs and their frequencies in the format {(a, b): frequency}.\n",
    "        \"\"\"\n",
    "        pair_freqs = defaultdict(int)\n",
    "        for _, split in self.splits.items():\n",
    "            for i in range(len(split)-1):\n",
    "                pair = (split[i], split[i+1])\n",
    "                if pair not in pair_freqs:\n",
    "                    pair_freqs[pair] = 1\n",
    "                else:\n",
    "                    pair_freqs[pair] += 1\n",
    "        self.word_freqs = pair_freqs\n",
    "        self.word_freqs = dict(sorted(self.word_freqs.items(), key=lambda x: x[1], reverse=True))\n",
    "        return self.word_freqs\n",
    "        \n",
    "    def merge_pair(self, a, b):\n",
    "        \"\"\"\n",
    "        Merge the given pair of symbols in all words where they appear adjacent.\n",
    "\n",
    "        Args:\n",
    "            a (str): The first symbol in the pair.\n",
    "            b (str): The second symbol in the pair.\n",
    "\n",
    "        Returns:\n",
    "            dict: The updated splits dictionary after merging.\n",
    "        \"\"\"\n",
    "        pair = (a,b)\n",
    "        # Check if valid pair\n",
    "        if pair in self.word_freqs:\n",
    "            new_token = a+b\n",
    "            for word, split in self.splits.items():\n",
    "                print(\"split\", split)\n",
    "                for i in range(len(split)-1):\n",
    "                    if split[i] == a and split[i+1] == b:\n",
    "                       split[i] = new_token\n",
    "                       new_split = list(filter(lambda x: x not in [b], split))\n",
    "                       self.splits[word] = new_split\n",
    "        return self.splits\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"\n",
    "        Tokenize a given text using the trained BPE tokenizer.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to be tokenized.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: A list of tokens obtained after applying BPE tokenization.\n",
    "        \"\"\"\n",
    "\n",
    "        pre_tokenized_text = text.split()\n",
    "        splits_text = [[l for l in word] for word in pre_tokenized_text]\n",
    "\n",
    "        for pair, merge in self.merges.items():\n",
    "            for idx, split in enumerate(splits_text):\n",
    "                i = 0\n",
    "                while i < len(split) - 1:\n",
    "                    if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
    "                        split = split[:i] + [merge] + split[i + 2 :]\n",
    "                    else:\n",
    "                        i += 1\n",
    "                splits_text[idx] = split\n",
    "        result = sum(splits_text, [])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aHk5wKzDQyyo"
   },
   "source": [
    "Now let's train a BPE tokeniser on our AfriSenti training corpus, apply it to our corpus, and see how our vocabulary changes after subword tokenisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "qUqUo_6HQyyo",
    "ExecuteTime": {
     "end_time": "2024-08-06T09:36:10.528082Z",
     "start_time": "2024-08-06T09:35:51.382536Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train BPE\n",
    "bpe = BPETokenizer(tokenized_train_corpus, vocab_size=1000)\n",
    "#testSentences = [ [\"low\", \"lower\", \"newest\", \"widest\"]]\n",
    "#bpe2 = BPETokenizer(testSentences, 10)\n",
    "t = bpe.compute_pair_freqs()\n",
    "merges = bpe.train()\n",
    "print('Merges: ', merges)\n",
    "\n",
    "# Tokenize text\n",
    "text = 'This is a test sentence.'\n",
    "tokenized_text = text.split()\n",
    "tokens = bpe.tokenize(text)\n",
    "print('BPE tokens: ', tokens)\n",
    "\n",
    "# Apply to our dataset\n",
    "#train_df['bpe_text'] = train_df['text'].apply(lambda x: ' '.join(bpe.tokenize(x)))\n",
    "#dev_df['bpe_text'] = dev_df['text'].apply(lambda x: ' '.join(bpe.tokenize(x)))\n",
    "#test_df['bpe_text'] = test_df['text'].apply(lambda x: ' '.join(bpe.tokenize(x)))\n",
    "\n",
    "#train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now create a vocabulary of BPE tokens, based on our tokenised corpus. Specifying the ``vocab_size`` parameter of our BPE training algorithm allows us to control the vocabulary size, which enables smaller vocabularies than word-based tokenisation."
   ],
   "metadata": {
    "id": "dBAtehY1kGtZ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h-rRXz_0Qyyo"
   },
   "outputs": [],
   "source": [
    "bpe_corpus = train_df['bpe_text'].tolist()\n",
    "tokenized_bpe_corpus = whitespace_tokenize(bpe_corpus)\n",
    "\n",
    "# Count number of BPE tokens in corpus\n",
    "num_tokens = count_tokens(tokenized_bpe_corpus)\n",
    "print('Number of BPE tokens in corpus: ', num_tokens)\n",
    "\n",
    "# Collect type counts in BPE corpus\n",
    "bpe_type2count = create_type_counts(tokenized_bpe_corpus)\n",
    "print('Number of BPE types in corpus: ', len(bpe_type2count))\n",
    "\n",
    "# Sort types by counts\n",
    "bpe_type2count = dict(sorted(bpe_type2count.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "# Print first few types and counts\n",
    "for i, (type_, count) in enumerate(bpe_type2count.items()):\n",
    "    print(f'{type_}: {count}')\n",
    "    if i == 5:\n",
    "        break\n",
    "\n",
    "# Create a vocabulary for BPE tokens\n",
    "bpe_index2type, bpe_type2index = create_vocabulary(bpe_type2count, min_count=2)\n",
    "print('Vocabulary size: ', len(bpe_index2type))\n",
    "print('First 10 BPE tokens in the vocabulary: ', bpe_index2type[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bxpu01RfQyyo"
   },
   "source": [
    "<a name=\"section2\"></a>\n",
    "# 2. Text Classification\n",
    "\n",
    "Now we will use AfriSenti to train a text classification model for sentiment analysis. We frame the task as a binary classification problem - the model is trained to predict whether a given piece of text is positive or negative in its emotional tone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kWCXjTgUQyyo"
   },
   "source": [
    "<a name=\"section2_1\"></a>\n",
    "## 2.1. Text vectorization\n",
    "\n",
    "In order to train a machine learning model for our binary classification task, we need to transform the textual data into a numerical format that the model can understand and process. This transformation is known as vectorization.Vectorization is the process of converting text into numerical vectors. Machine learning models operate on numerical data, so vectorization is a critical step in preparing text for modeling. There are several methods for vectorizing text, including:\n",
    "\n",
    "* One-hot encoding: Represents each word as a binary vector with a 1 in the position corresponding to the word's index in the vocabulary and 0s elsewhere.\n",
    "* Term Frequency-Inverse Document Frequency (TF-IDF): Weighs the words based on their frequency in the document and their rarity across all documents, providing a more informative representation.\n",
    "* Word Embeddings: Maps words to dense vectors in a continuous vector space, capturing semantic similarities between words.\n",
    "\n",
    "In this tutorial, we will explore different vectorization techniques and apply them to our text data to prepare it for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Io87VrzFQyyo"
   },
   "outputs": [],
   "source": [
    "def one_hot_vectorize(sentences, type2index):\n",
    "    \"\"\"\n",
    "    One-hot encode a list of sentences.\n",
    "\n",
    "    param:\n",
    "        list of list of tokens e.g. [['This', 'is', 'a', 'sentence'], ['This', 'is', 'another', 'sentence'], ...]\n",
    "        type2index: dictionary mapping words to their index in the vocabulary e.g. {'word1': 0, 'word2': 1, 'word3': 2, ...}\n",
    "    return:\n",
    "        one_hot_sentences: 2d numpy array of one-hot encoded sentences e.g. [[1, 0, 0, 1, ...], [0, 1, 1, 0, ...], ...]\n",
    "    \"\"\"\n",
    "    # TODO: COMPLETE THIS CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cngGpEQ8Qyyp"
   },
   "outputs": [],
   "source": [
    "train_sentences = train_df[\"text\"].tolist()\n",
    "dev_sentences = dev_df[\"text\"].tolist()\n",
    "test_sentences = test_df[\"text\"].tolist()\n",
    "\n",
    "tokenized_train_sentences = whitespace_tokenize(train_sentences)\n",
    "tokenized_dev_sentences = whitespace_tokenize(dev_sentences)\n",
    "tokenized_test_sentences = whitespace_tokenize(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T58rP6R8Qyyp"
   },
   "outputs": [],
   "source": [
    "X_train = one_hot_vectorize(tokenized_train_sentences, type2index)\n",
    "X_dev = one_hot_vectorize(tokenized_dev_sentences, type2index)\n",
    "X_test = one_hot_vectorize(tokenized_test_sentences, type2index)\n",
    "print('Train length: ', len(X_train))\n",
    "print('Dev length: ', len(X_dev))\n",
    "print('Test length: ', len(X_test))\n",
    "\n",
    "# Print examples\n",
    "print('Train example: ', X_train[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bfaVOrdSQyyp"
   },
   "outputs": [],
   "source": [
    "## TODO: uncomment to use TF-IDF vectorizer instead of one-hot encoding\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# vectorizer = TfidfVectorizer()\n",
    "\n",
    "# X_train = vectorizer.fit_transform(train_sentences)\n",
    "# X_dev = vectorizer.transform(dev_sentences)\n",
    "# X_test = vectorizer.transform(test_sentences)\n",
    "\n",
    "# X_train = X_train.toarray()\n",
    "# X_dev = X_dev.toarray()\n",
    "# X_test = X_test.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eEtSggW9Qyyp"
   },
   "source": [
    "Now that we have converted our text to vectors, we can store them as Jax arrays to later feed as input to our classification model. We also convert the output labels to numerical representations (positive = 1 and negative = 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ozFLk-8EQyyp"
   },
   "outputs": [],
   "source": [
    "X_train = jnp.array(X_train, dtype=jnp.float16)\n",
    "X_dev = jnp.array(X_dev, dtype=jnp.float16)\n",
    "X_test = jnp.array(X_test, dtype=jnp.float16)\n",
    "n_feat = X_train.shape[1]\n",
    "\n",
    "y_train = train_df[\"label\"]\n",
    "y_train = jnp.array(y_train.map({\"positive\": 1, \"negative\": 0}), dtype=jnp.float16)\n",
    "\n",
    "y_dev = dev_df[\"label\"]\n",
    "y_dev = jnp.array(y_dev.map({\"positive\": 1, \"negative\": 0}), dtype=jnp.float16)\n",
    "\n",
    "y_test = test_df[\"label\"]\n",
    "y_test = jnp.array(y_test.map({\"positive\": 1, \"negative\": 0}), dtype=jnp.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A-Z0gxf6Qyyt"
   },
   "source": [
    "<a name=\"section2_2\"></a>\n",
    "# 2.2. Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nCvXCXuyQyyt"
   },
   "source": [
    "In this tutorial, we are going to implement logistic regression for sentiment analysis. Logistic regression is a discriminative classifier that models the decision boundary between data from different classes, i.e., $p(y | \\mathbf{x})$. In our case, this means modeling the decision boundary between positive and negative tweets.\n",
    "\n",
    "We represent our data as pairs $(\\mathbf{x}, y)$ of (input, output) / (tweet, label). Our logistic regression implementation requires the following components:\n",
    "\n",
    "**1. A feature representation of the input.**\n",
    "   - For each tweet, extract a vector of features $\\mathbf{x} = [x_1, x_2, ... , x_n]^T$.\n",
    "   - We will use a vectorizer like one-hot encoding or TF-IDF.\n",
    "\n",
    "**2. A classification function that computes the estimated class via $p(y|\\mathbf{x})$.**\n",
    "   - In logistic regression, the classification function is the sigmoid function, defined as $\\sigma(z) = \\frac{1}{1 + e^{-z}}$, where $z = \\mathbf{w}^T\\mathbf{x} + b$, and $\\mathbf{w}$ and $b$ are the parameters to be learned.\n",
    "   - The output of the sigmoid function, $\\sigma(z)$, represents the probability that the input $(\\mathbf{x})$ belongs to the positive class i.e., $p(y=1|\\mathbf{x})$.\n",
    "\n",
    "**3. An objective function for learning, like cross-entropy loss.**\n",
    "   - The cross-entropy loss function for binary classification is given by $-\\frac{1}{N}\\sum_{i=1}^{N}[y_i\\log(\\hat{y}_i) + (1 - y_i)\\log(1 - \\hat{y}_i)]$, where $N$ is the number of samples, $y_i$ is the true label, and $\\hat{y}_i$ is the predicted probability that the $i$-th sample belongs to the positive class.\n",
    "   - The goal is to minimize this loss function with respect to the parameters $\\mathbf{w}$ and $b$.\n",
    "\n",
    "**4. An algorithm for optimizing the objective function, like stochastic gradient descent (SGD).**\n",
    "   - SGD is an iterative optimization algorithm that updates the parameters $\\mathbf{w}$ and $b$ in the direction of the negative gradient of the loss function with respect to these parameters.\n",
    "   - The updates are made for each training sample or a batch of samples, leading to the update rules: $\\mathbf{w} \\leftarrow \\mathbf{w} - \\alpha \\nabla_{\\mathbf{w}}\\mathcal{L}$ and $b \\leftarrow b - \\alpha \\nabla_{b}\\mathcal{L}$, where $\\alpha$ is the learning rate, $\\nabla_{\\mathbf{w}}\\mathcal{L}$ and $\\nabla_{b}\\mathcal{L}$ are the gradients of the loss function with respect to $\\mathbf{w}$ and $b$, respectively.\n",
    "\n",
    "In the following sections, we will implement these components step by step to build our logistic regression model for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ha7ooevQyyu"
   },
   "outputs": [],
   "source": [
    "# Let's start by defining our classification function p(y|x)\n",
    "def sigmoid(r):\n",
    "    return 1 / (1 + jnp.exp(-r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xTokt_TPQyyu"
   },
   "outputs": [],
   "source": [
    "# We can plot the sigmoid function to see its shape\n",
    "z = 10\n",
    "r = jnp.linspace(-z, z, 200)\n",
    "_, ax = plt.subplots(figsize=FS)\n",
    "plt.plot(r, sigmoid(r))\n",
    "ax.grid()\n",
    "_ = ax.set(xlabel=\"r\", ylabel=\"$logistic(r)$\", title=\"The $logistic$ curve\")\n",
    "_ = ax.set_xlim(-z, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wpKGHuxvQyyu"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bz8W4skyQyyu"
   },
   "outputs": [],
   "source": [
    "# Now let's use the sigmoid function to define our classification function p(y|x)\n",
    "# It takes as input the bias b, the weights w, and the input features X\n",
    "def predict(b, w, X):\n",
    "    return sigmoid(jnp.dot(X, w) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9YYioKU-Qyyu"
   },
   "outputs": [],
   "source": [
    "# Now we can define our cost/objective function, which is the cross-entropy loss\n",
    "# Remember to clip values to avoid log(0)\n",
    "eps=1e-14\n",
    "def cross_entropy_loss(b, w, X, y, lmbd=0.1):\n",
    "    n = y.size\n",
    "    p = predict(b, w, X)\n",
    "    p = jnp.clip(p, eps, 1 - eps)  # clip predictions to avoid log(0)\n",
    "\n",
    "    # TODO: COMPLETE THIS CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dXzaBGeSQyyu"
   },
   "outputs": [],
   "source": [
    "# Let's test these methods with randomly initialized parameters for the logistic regression model\n",
    "b_0 = 1.0\n",
    "w_0 = 1.0e-5 * jnp.ones(n_feat)\n",
    "print(cross_entropy_loss(b_0, w_0, X_train, y_train))\n",
    "\n",
    "y_pred_proba = predict(b_0, w_0, X_test)\n",
    "print(y_pred_proba[:5])\n",
    "\n",
    "y_pred = jnp.array(y_pred_proba)\n",
    "y_pred = jnp.where(y_pred < 0.5, y_pred, 1.0)\n",
    "y_pred = jnp.where(y_pred >= 0.5, y_pred, 0.0)\n",
    "print(y_pred[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oEQFN4r2Qyyv"
   },
   "outputs": [],
   "source": [
    "# We can use the grad function from JAX to compute the gradients of the cross-entropy loss function\n",
    "print(grad(cross_entropy_loss, argnums=0)(b_0, w_0, X_train, y_train))\n",
    "print(grad(cross_entropy_loss, argnums=1)(b_0, w_0, X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gkxdbb1OQyyv"
   },
   "source": [
    "In the following code cell, we will train our logistic regression model using the training dataset. The training process involves iteratively updating the model's parameters to minimize the cross-entropy loss function. We will use stochastic gradient descent (SGD) as our optimization algorithm. During training, the model learns to predict the sentiment of tweets (positive or negative) based on the features extracted from the text. It's important to monitor the loss during training to ensure that the model is learning effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hmyqLOMnQyyv"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "n_iter = 1000\n",
    "eta = 5e-2\n",
    "tol = 1e-6\n",
    "w = w_0\n",
    "b = b_0\n",
    "\n",
    "new_loss = float(cross_entropy_loss(b, w, X_train, y_train))\n",
    "loss_hist = [new_loss]\n",
    "for i in range(n_iter):\n",
    "    # TODO: COMPLETE THIS CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jVyythY1Qyyv"
   },
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=FS)\n",
    "plt.semilogy(loss_hist)\n",
    "ax.grid()\n",
    "_ = ax.set(xlabel=\"Iteration\", ylabel=\"Cost value\", title=\"Convergence history\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mJPlL-9CQyyv"
   },
   "source": [
    "<a name=\"section2_3\"></a>\n",
    "## 2.3. Evaluation\n",
    "\n",
    "After training the model, it's crucial to evaluate its performance on a separate validation or test dataset. In this code cell, we will use the `classification_report` method from `sklearn` to assess the model's performance along several metrics, including precision, recall, and F1 score.\n",
    "\n",
    "* Precision measures the proportion of correctly predicted positive instances out of all predicted positive instances.\n",
    "* Recall measures the proportion of correctly predicted positive instances out of all actual positive instances.\n",
    "* The F1 score is the harmonic mean of precision and recall, providing a single metric that balances both.\n",
    "\n",
    "**The macro-averaged F1 is often used as a single reference to compare performance, since it incorporates precision and recall across different classes.**\n",
    "\n",
    "These metrics are particularly important in the context of class imbalance, where the number of instances in different classes may vary significantly. Additionally, we will compare the model's performance against random results to ensure that our model is making meaningful predictions and not just guessing based on class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "efqZt56ZQyyv"
   },
   "outputs": [],
   "source": [
    "y_pred_proba = predict(b, w, X_test)\n",
    "y_pred = jnp.array(y_pred_proba)\n",
    "y_pred = jnp.where(y_pred < 0.5, y_pred, 1.0)\n",
    "y_pred = jnp.where(y_pred >= 0.5, y_pred, 0.0)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T5gj7SxaQyyv"
   },
   "outputs": [],
   "source": [
    "# Let's compare against a randomly initialized model\n",
    "b_0_random = 1.0\n",
    "w_0_random = 1.0e-5 * jnp.ones(n_feat)\n",
    "y_pred_proba_random = predict(b_0_random, w_0_random, X_test)\n",
    "y_pred_random = jnp.array(y_pred_proba_random)\n",
    "y_pred_random = jnp.where(y_pred_random < 0.5, y_pred_random, 1.0)\n",
    "y_pred_random = jnp.where(y_pred_random >= 0.5, y_pred_random, 0.0)\n",
    "print(y_pred_random[:5])\n",
    "print(classification_report(y_test, y_pred_random))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a name=\"section2_4\"></a>\n",
    "## 2.4. Addressing class imbalance\n",
    "\n",
    "You may have noticed that your model is not performing any better than a randomly initialised model. It might be worth experimenting with different training hyperparameters (e.g. learning rate, epochs, weight initialisations) which could be affecting perfomance. But another possibility is the issue of class imbalance. This occurs when the number of instances in one class significantly outnumbers those in another, which is a common scenario in real-world datasets. For instance, in sentiment analysis, positive sentiments may be more prevalent than negative ones, or vice versa.\n",
    "\n",
    "To confirm if class imbalance is affecting your model, you should:\n",
    "\n",
    "* Investigate the distribution of classes in your test set. A simple count of instances in each class will reveal any imbalance.\n",
    "* Analyze the predictions made by your model. If it's predominantly predicting the majority class, class imbalance is likely skewing its performance.\n",
    "\n",
    "If class imbalance is indeed present, there are several strategies you can apply to your logistic regression model to mitigate its effects:\n",
    "\n",
    "* Oversampling the minority class or undersampling the majority class can balance the class distribution.\n",
    "\n",
    "* Modify the logistic regression's loss function to incorporate weights that reflect the class distribution, as discussed earlier.\n",
    "\n",
    "* Use different regularization terms for different classes to penalize misclassification of the minority class more than the majority.\n",
    "\n",
    "If you suspect class imbalance is hurting your model's performance, experiment with one of these methods to see if it solves the issue.\n",
    "\n",
    "The following cell block contains code for a weighted loss function that introduces weights into the loss calculation to account for class imbalance. By incorporating these weights, the loss function penalizes misclassifications of the minority class more heavily, encouraging the model to pay more attention to these instances during training and helping to reduce bias towards the majority class."
   ],
   "metadata": {
    "id": "IZvxtsy9aYkh"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# # UNCOMMENT TO USE WEIGHTED LOSS FUNCTION\n",
    "# positive_class_samples = jnp.sum(y_train == 1)\n",
    "# negative_class_samples = jnp.sum(y_train == 0)\n",
    "# total_samples = positive_class_samples + negative_class_samples\n",
    "\n",
    "# weight_for_positive_class = total_samples / (2.0 * positive_class_samples)\n",
    "# weight_for_negative_class = total_samples / (2.0 * negative_class_samples)\n",
    "\n",
    "# def cross_entropy_loss(b, w, X, y, lmbd=0.1):\n",
    "#     n = y.size\n",
    "#     p = predict(b, w, X)\n",
    "#     p = jnp.clip(p, eps, 1 - eps)  # clip predictions to avoid log(0)\n",
    "\n",
    "#     # Calculate class weights\n",
    "#     class_weights = jnp.where(y == 1, weight_for_positive_class, weight_for_negative_class)\n",
    "\n",
    "#     # Weighted cross-entropy loss\n",
    "#     loss = -jnp.sum(class_weights * (y * jnp.log(p) + (1 - y) * jnp.log(1 - p))) / n\n",
    "\n",
    "#     # Regularization term\n",
    "#     reg_term = 0.5 * lmbd * (jnp.dot(w, w) + b * b)\n",
    "\n",
    "#     return loss + reg_term"
   ],
   "metadata": {
    "id": "oNYPKugEdw0j"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gHPmnEnIQyyv"
   },
   "source": [
    "<a name=\"section3\"></a>\n",
    "# 3. Word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PyFCmJyRQyyv"
   },
   "source": [
    "Word embeddings are vector representations of words/tokens that encode semantic meanings of words in relation to other words in a given corpus. In this section we will train word embeddings from our corpus of tweets using the Continuous Bag of Words (CBOW) algorithm from Word2Vec. Our goal in this tutorial is to implement parts of the CBOW algorithm and train basic word embeddings using our small corpus of tweets.\n",
    "\n",
    "The CBOW algorithm predicts a target word based on its surrounding context words. For example, given the context words \"the cat ___ on the mat,\" CBOW aims to predict the target word \"sat\". This is achieved by training a neural network to maximize the probability of the target word given its context.\n",
    "\n",
    "\n",
    "The CBOW algorithm involves the following components:\n",
    "\n",
    "**1. Context Window**\n",
    "\n",
    "A fixed-size window that slides over the text, capturing the surrounding context words for each target word.\n",
    "\n",
    "\n",
    "**2. A 1-layer neural network**\n",
    "\n",
    "* Input layer: The input to the neural network is the sum of the vector representations of the context words.\n",
    "* Hidden layer: A fully connected layer that transforms the input into a hidden representation. The dimensionality of this layer determines the size of the resulting word embeddings.\n",
    "* Output layer: A softmax layer that outputs the probability distribution over the entire vocabulary, predicting the target word.\n",
    "\n",
    "\n",
    "**3. Objective Function**\n",
    "\n",
    "The training objective is to maximize the log likelihood of the correct target word given the context words. This is typically achieved using the cross-entropy loss function.\n",
    "\n",
    "**4. Optimization Algorithm**\n",
    "\n",
    "Stochastic Gradient Descent (SGD) or other optimization algorithms are used to update the model parameters, minimizing the loss function.\n",
    "\n",
    "By training the CBOW model on a large corpus of text, the resulting word embeddings capture rich semantic and syntactic information about words. Words with similar meanings tend to have similar vector representations, enabling various NLP tasks such as word similarity, analogy solving, and text classification.\n",
    "\n",
    "In the following sections, we will implement the CBOW algorithm step by step and use it to generate word embeddings for our text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uZimDCpXQyyw"
   },
   "source": [
    "<a name=\"section3_1\"></a>\n",
    "## 3.1. Context windows\n",
    "\n",
    "We start by defining a function that transforms a text corpus to the format required by Word2Vec CBOW - as a sequence of context-target pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GozvGKJ-Qyyw"
   },
   "outputs": [],
   "source": [
    "def generate_train_data(sentences, type2index, window_size):\n",
    "    contexts = []\n",
    "    targets = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        for i in range(len(sentence)):\n",
    "            target = i\n",
    "            context = []\n",
    "            for j in range(-window_size, window_size + 1):\n",
    "                if j == 0:\n",
    "                    continue\n",
    "                try:\n",
    "                    if i + j < 0:\n",
    "                        context.append('<PAD>')\n",
    "                        continue\n",
    "                    context.append(sentence[i + j])\n",
    "                except Exception:\n",
    "                    context.append('<PAD>')\n",
    "            if sentence[target] not in type2index:\n",
    "                continue\n",
    "            contexts.append([type2index[k] if k in type2index else type2index[\"<UNK>\"] for k in context])\n",
    "            targets.append(type2index[sentence[target]])\n",
    "\n",
    "    return jnp.array(contexts), jnp.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UDCsKHDDQyyw"
   },
   "outputs": [],
   "source": [
    "contexts, targets = generate_train_data(tokenized_train_sentences, type2index, window_size=5)\n",
    "\n",
    "# Print example context-target indices\n",
    "print(contexts[:5])\n",
    "print(targets[:5])\n",
    "\n",
    "# Print example context-target tokens\n",
    "print()\n",
    "print(\"Sentence:\", train_sentences[0])\n",
    "print()\n",
    "\n",
    "for i in range(5):\n",
    "    print('Context: ', [index2type[j] for j in contexts[i]])\n",
    "    print('Target: ', index2type[targets[i]])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gh9hSjQ2Qyyw"
   },
   "source": [
    "<a name=\"section3_2\"></a>\n",
    "# 3.2. Word2Vec CBOW\n",
    "\n",
    "Now that we have prepared our training data in the form of context-target pairs, we are ready to define and train our Continuous Bag of Words (CBOW) model. In this section, we will focus on implementing the neural network architecture for CBOW and training it on our tweet corpus.\n",
    "\n",
    "The CBOW model we will implement consists of the following components:\n",
    "\n",
    "* Input Layer: This layer takes the average of the one-hot encoded vectors of the context words.\n",
    "* Hidden Layer: A fully connected layer that projects the input to a lower-dimensional space, where each dimension represents a feature of the word embedding.\n",
    "* Output Layer: A softmax layer that outputs a probability distribution over the entire vocabulary, aiming to predict the target word.\n",
    "\n",
    "Our training process will involve feeding batches of context-target pairs into the model and using stochastic gradient descent (SGD) to update the weights of the network to minimize the cross-entropy loss between the predicted and actual target words.\n",
    "\n",
    "Let's proceed to define our CBOW neural network class and train it on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t4qd-ZGmQyyw"
   },
   "outputs": [],
   "source": [
    "from jax.nn import softmax, one_hot\n",
    "from jax import random, vmap\n",
    "\n",
    "class Word2VecCBOW():\n",
    "    def __init__(self, window_size, embed_dim, vocab_size, random_state):\n",
    "        # Defines the key to be used for the random creation of the weights\n",
    "        self.key = random.PRNGKey(random_state)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.linear = self._create_random_matrix(vocab_size, embed_dim)\n",
    "        self.soft = self._create_random_matrix(embed_dim, vocab_size)\n",
    "\n",
    "        # Vectorizes the predict method\n",
    "        self.predict = vmap(self._predict, in_axes=(None, 0))\n",
    "\n",
    "    def train(self, X, y, num_epochs, batch_size):\n",
    "        # TODO: COMPLETE THIS CODE\n",
    "\n",
    "    def _predict(self, params, X):\n",
    "        activations = []\n",
    "        for x in X:\n",
    "            activations.append(jnp.dot(x, params[0]))\n",
    "        # Averages the activations\n",
    "        activation = jnp.mean(jnp.array(activations), axis=0)\n",
    "        logits = jnp.dot(activation, params[1])\n",
    "        result = softmax(logits)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def _create_random_matrix(self, window_size, embed_dim):\n",
    "        w_key = random.split(self.key, num=1)\n",
    "        return 0.2 * random.normal(self.key, (window_size, embed_dim))\n",
    "\n",
    "    def loss(self, params, X, y):\n",
    "        preds = self.predict(params, X)\n",
    "        l = -jnp.mean(preds * y)\n",
    "        self.l = l\n",
    "        return l\n",
    "    def update(self, params, X, y, step_size=0.02):\n",
    "        grads = grad(self.loss)(params, X, y)\n",
    "        return [params[0] - step_size * grads[0],\n",
    "                params[1] - step_size * grads[1]]\n",
    "\n",
    "    def get_embedding(self):\n",
    "        return self.linear\n",
    "\n",
    "\n",
    "    def generate_batches(self, X, y, batch_size):\n",
    "            for index, offset in enumerate(range(0, len(y), batch_size)):\n",
    "                yield X[offset: offset + batch_size], y[offset: offset + batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WDBY9PtpQyyw"
   },
   "outputs": [],
   "source": [
    "tokenized_corpus = tokenized_train_sentences\n",
    "contexts, targets = generate_train_data(tokenized_corpus, type2index, window_size=5)\n",
    "w2v = Word2VecCBOW(2, 32, len(type2index), 42)\n",
    "w2v.train(contexts, targets, 3, 32)\n",
    "w2v.get_embedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6EapYGNGQyyw"
   },
   "source": [
    "After training, we will extract the word embeddings from the weights of the input layer, which will contain the learned representations of words in our tweet corpus. These embeddings can then be used for various NLP tasks, such as sentiment analysis or text classification. In this notebook we will not use embeddings for such tasks. However, we can still get an idea of the representation space learned by CBOW by comparing the embeddings of different words. A common way to do this is to investigate the closest embeddings for different words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vmdQbn3CQyyx"
   },
   "outputs": [],
   "source": [
    "# Compare embeddings of words\n",
    "def cosine_similarity(a, b):\n",
    "    return jnp.dot(a, b) / (jnp.linalg.norm(a) * jnp.linalg.norm(b))\n",
    "\n",
    "def most_similar(word, embeddings, index2type, topn=5):\n",
    "    word_index = type2index[word]\n",
    "    word_embedding = embeddings[word_index]\n",
    "    similarities = []\n",
    "    for i, embedding in enumerate(embeddings):\n",
    "        similarities.append((index2type[i], cosine_similarity(word_embedding, embedding)))\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:topn]\n",
    "\n",
    "embeddings = w2v.get_embedding()\n",
    "most_similar(\"good\", embeddings, index2type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m5fV1mTwQyyx"
   },
   "source": [
    "<a name=\"section3_3\"></a>\n",
    "# 3.3. Pretrained embeddings\n",
    "\n",
    "While training word embeddings from scratch can be insightful, it often requires a large corpus and substantial computational resources to achieve high-quality embeddings. An alternative approach is to use pretrained embeddings, which have been trained on extensive text corpora such as Wikipedia or the Google News dataset. Pretrained embeddings can provide a strong starting point for various NLP tasks and can be especially useful when working with smaller datasets.\n",
    "\n",
    "In this section, we will load publicly available pretrained word embeddings.  We use the gensim library, which can be used to download several sets of pretrained embeddings. In this example we load embeddings trained with GloVe, an alternative to Word2Vec. You can check the nearest neighbours of different words, which reveals the type of semantic information encoded in word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "64S450cdQyyx"
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('glove-wiki-gigaword-50')\n",
    "# wv = api.load('word2vec-google-news-300') # larger embeddings that will take longer to download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U6kpe4q8Qyyx"
   },
   "outputs": [],
   "source": [
    "wv.most_similar('king')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yMH_yb1qQyyx"
   },
   "outputs": [],
   "source": [
    "wv.most_similar('queen')"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "wv.most_similar('eat')"
   ],
   "metadata": {
    "id": "Cr__GbvNXpyH"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "wv.get_vector('eat')"
   ],
   "metadata": {
    "id": "QJfWQwIbXz5r"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a name=\"section4\"></a>\n",
    "\n",
    "# 4. Conclusion\n",
    "\n",
    "\n",
    "In this notebook we have tackled key components of the NLP pipeline like text processing, tokenisation, classification, and word embeddings. These tools and techniques provide a solid starting point for further exploration and more complex applications in the field of NLP."
   ],
   "metadata": {
    "id": "4eeRBwfxYWBu"
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
